{
    "1x-ArtClarity": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_ArtClarity.pth\n**License:** WTFPL\n**Link:** https://1drv.ms/u/s!Aip-EMByJHY28w0Zkd9fdANYH3j6?e=9zwxII\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Texture retaining denoiser and sharpener for digital artwork\n**HR_size:** 128\n**Dataset:** ArtStation illustrations, then interpolated with some other 1x models\n**Dataset_size:** 800 images\n**OTF Training:** Yes\n**Pretrained_Model_G:** 1xESRGAN.pth\n\nhttps://cdn.discordapp.com/attachments/547949806761410560/870357763862167582/unknown.png\nhttps://cdn.discordapp.com/attachments/547949806761410560/872898863801978920/yjm_800.jpg",
        "author": "DinJerr",
        "when": "2021-08-05T18:05:59.175000+00:00",
        "name": "1x_ArtClarity.pth",
        "hasLink": true
    },
    "1x-BaldrickVHSFixV0-2": {
        "content": "<@&560103931204861954> <@&577839492199874570>\nName: 1xBaldrickVHSFix_180000_G_V0.2\nLicense: CC BY-NC-SA 4.0\nLink: https://icedrive.net/0/d3bwO9Byjo\nModel Architecture: ESRGAN\nScale: 1\nPurpose: Fixing minor VHS Chroma and Pattern Noise\n\nIterations: 180k\nbatch_size: 3\nHR_size: 64\nDataset: Old vhs film recording for LR and cropped and downscaled Blu-ray rip of same film for HR\nDataset_size: 663\nOTF Training: No\nPretrained_Model_G: 1xESRGAN\n\nDescription: Should fix up most minor to moderate chroma and pattern noise on vhs-rips NOTE: only works on deinterlaced sources",
        "author": "NimRodZorg [GTX 1650S] (UK)",
        "when": "2021-03-03T16:15:59.707000+00:00",
        "name": "1xBaldrickVHSFix_180000_G_V0.2",
        "hasLink": true
    },
    "1x-Bandage-Smooth": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n\n**Name:** 1x_Bandage-Smooth-[64]_105000_G\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/folders/1WD2RqtER_jk0QkPFUySeXhDV2aAO8_nr?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Debanding\n\n**Iterations:** 105k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** 765\n**Dataset:** Random anime images\n**Dataset_size:** 550\n**OTF Training:** No\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 37.026, SSIM: 0.97026, LPIPS: 0.015815\n\n**Description:** Attempts to remove the damages done by color banding. For this model, I downscaled all of the HR images to make their pixel size closer to 1000x1000 using the Box filter and then downscaled them again by 50% using the Point filter. Afterwards, I applied 64-bit color banding to every image in the dataset.\n\nhttps://imgsli.com/NTA1NTk/1/0",
        "author": "Sisyphean",
        "when": "2021-04-17T00:13:38.195000+00:00",
        "name": "1x_Bandage-Smooth-[64]_105000_G",
        "hasLink": true
    },
    "1x-BC1-smooth2": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Scale:** 1x\n**Purpose:** BC1/DXT color artifact cleanup\n**Description:** A model to help remove compression artifacts in BC1-BC3/DXT1-DXT5 compressed images (these all have color encoded the same way)\n**License:** CC BY-NC 4.0\n**Link:** https://drive.google.com/open?id=1LHplsPRqhmjR28jGgRlEekeP_bvx3nUC\n**Iterations:** 1000000 \n**batch_size:** 20\n**HR_size:** 32\n**Epoch:** 4184 (used automatic cropping)\n**Dataset_size:** 4767\n**Pretrained_Model_G:** none",
        "author": "BlueAmulet",
        "when": "2019-09-05T16:17:30.286000+00:00",
        "name": null,
        "hasLink": true
    },
    "1x-BCGone-Detailed": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_BCGone-DetailedV2_40-60_115000_G\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/folders/1pnyPklpLPtMAHeNC5NVU1mV_Qh61wWnx?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** BC1 Compression\n\n**Iterations:** 115k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** 341\n**Dataset:** Realistic images\n**Dataset_size:** 450 * 3\n**OTF Training:** No\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 32.498, SSIM: 0.90722, LPIPS: 0.06429\n\n**Description:** Attempts to remove the damages done by BC1 compression. For this model, I downscaled the HR images by 50% using the Box filter as it gave me the sharpest results without adding any artifacts. Afterwards, I compressed all of the images in the dataset to 40%, 50%, and 60% to make the model effective at that range.\n\nhttps://imgsli.com/NDUwMzU/4/3",
        "author": "Sisyphean",
        "when": "2021-03-19T01:21:45.303000+00:00",
        "name": "1x_BCGone-DetailedV2_40-60_115000_G",
        "hasLink": true
    },
    "1x-BCGone-DetailedV2-40-60": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_BCGone-DetailedV2_40-60_115000_G\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/folders/1pnyPklpLPtMAHeNC5NVU1mV_Qh61wWnx?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** BC1 Compression\n\n**Iterations:** 115k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** 341\n**Dataset:** Realistic images\n**Dataset_size:** 450 * 3\n**OTF Training:** No\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 32.498, SSIM: 0.90722, LPIPS: 0.06429\n\n**Description:** Attempts to remove the damages done by BC1 compression. For this model, I downscaled the HR images by 50% using the Box filter as it gave me the sharpest results without adding any artifacts. Afterwards, I compressed all of the images in the dataset to 40%, 50%, and 60% to make the model effective at that range.\n\nhttps://imgsli.com/NDUwMzU/4/3",
        "author": "Sisyphean",
        "when": "2021-03-19T01:21:45.303000+00:00",
        "name": "1x_BCGone-DetailedV2_40-60_115000_G",
        "hasLink": true
    },
    "1x-BCGone-Smooth": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_BCGone_Smooth_110000_G\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/u/1/folders/1pnyPklpLPtMAHeNC5NVU1mV_Qh61wWnx\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** BC1 Compression\n\n**Iterations:** 110k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** 311\n**Dataset:** Images from DIV2K and random anime drawings\n**Dataset_size:** 800 (+600)\n**OTF Training:** No\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 36.178, SSIM: 0.95255, LPIPS: 0.051818\n\n**Description:** Attempts to remove the damages done by BC1 compression.\n\nhttps://imgsli.com/MjU5MzQ/4/3",
        "author": "Sisyphean",
        "when": "2020-08-02T20:18:57.634000+00:00",
        "name": "1x_BCGone_Smooth_110000_G",
        "hasLink": true
    },
    "1x-BroadcastToStudioLite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_BroadcastToStudioLite_485k\n**License:** CC BY-NC-SA 4.0\n**Link:** https://mega.nz/file/TO4S3KoT#eOiYCKWp0-O0rgBC0xgUl87DrPTbZ2or-NDgf22kPGA\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Improvement of low-quality cartoons from broadcast sources.\n\n**Iterations:** 485,000\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** unknown\n**Dataset:** DVD\n**Dataset_size:** 3,418\n**OTF Training** unkown\n**Pretrained_Model_G:** none\n\n**Description:** Will greatly increase the visual quality of bad broadcast tape sources of '80s and '90s cartoons (e.g. Garfield and Friends, Heathcliff, DuckTales, etc). Directly addresses chroma blur, dot crawl, and rainbowing. You're highly advised to take care of haloing beforehand in your favorite video editor as the model will not fix it and may make existing halos more noticeable.\n\nhttps://imgsli.com/OTg0MjA/6/7\nhttps://imgsli.com/OTc1OTg/2/3\nhttps://imgsli.com/OTg0Mjc/0/1",
        "author": "SaurusX",
        "when": "2022-03-05T23:58:45.063000+00:00",
        "name": "1x_BroadcastToStudioLite_485k",
        "hasLink": true
    },
    "1x-BS-Colorizer": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** BS_Colorizer/Vapourizer\n**License:** GNU GPLv3\n**Link:** https://drive.google.com/file/d/1UgxnymTfLaVhY3aaDuBRnTZ9ehckKVKJ\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Colorize Grayscale or Desaturated images.\n\n**Iterations:** 22000\n**batch_size:** 1\n**HR_size:** 128\n**Epoch:** 0 (so 1 epoch)\n**Dataset:** Wallpapers\n**Dataset_size:** 45045 tiles from about 800 wallpapers\n**OTF Training** No\n**Pretrained_Model_G:** 1xESRGAN\n\n**Description:** This model is a partially failed attempt at restoring color from Grayscale | B/W | 100% Desaturated images. \nIt mostly results in Blue and Yellow images with slight hints of Green, Orange and Magenta.\nYou are free to use this as a pretrain to achieve better results.\n\nYou may also utilize this on colored images, it may end up with a Vaporwave effect.",
        "author": "BlackScout",
        "when": "2020-01-31T00:22:22.711000+00:00",
        "name": "BS_Colorizer/Vapourizer",
        "hasLink": true
    },
    "1x-BSChroma": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 1x\\_BSChroma\n**License:** GNU GPLv3\n**Link:** https://drive.google.com/file/d/12v_2OwrB_rr6_0tMshHaHgD_XWRNgIyF/view\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Sharpen the \"chroma\" in images. Makes colors slightly more vibrant with the side effect of slightly added Chromatic Aberration.\n\n**Iterations:** 30000\n**batch_size:** 15\n**HR_size:** 128\n**Epoch:** 3\n**Dataset:** Random Images, LRs were converted to Lab color, then had their a and b channel blurred with a gaussian function (random radius: 0.5 - 5.0).\n**Dataset_size:** 45045 tiles\n**OTF Training** No\n**Pretrained_Model_G:** No\n**Tensorboard Stats:** psnr: 37.914, ssim: 0.9511, lpips: 0.0012156\n\n**Description:** This model is essentially a \"ChromaSharpen\". It makes the colors slightly more vibrant with a sideeffect of possibly adding Chromatic Abberation to the image. \nI am not sure about the usage of this model on real case scenarios but anything blurry or with fuzzy colors could work?",
        "author": "BlackScout",
        "when": "2020-02-29T15:05:54.264000+00:00",
        "name": "1x\\_BSChroma",
        "hasLink": true
    },
    "1x-BSLuma": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 1x\\_BSLuma\n**License:** GNU GPLv3\n**Link:** https://drive.google.com/file/d/1FDb9H6GOmE3aRvZbx_-kTiCLS77WyjgL/view\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Fix Luminance issues?? LumaSharpen ESRGAN Edition?\n\n**Iterations:** 8000\n**batch_size:** 12\n**HR_size:** 128\n**Epoch:** 3\n**Dataset:** Random Images, LRs were converted to Lab color, then had their L channel blurred with a gaussian function (rad avg: 1.5).\n**Dataset_size:** 45045 tiles\n**OTF Training** No\n**Pretrained_Model_G:** No\n**Tensorboard Stats:** psnr: 29.886, ssim: 0.81003, lpips: 0.018043\n\n**Description:** This model mostly does what \"Lumasharpen\" algorithms do. It may help fixing images with Luminance ~~images~~ issues? Like old DVD rips? I am not sure, didn't test.",
        "author": "BlackScout",
        "when": "2020-02-29T00:28:31.659000+00:00",
        "name": "1x\\_BSLuma",
        "hasLink": true
    },
    "1x-cinepak": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n\n**Scale:** 1x\n**Purpose:** Removal of Compression such as Cinepak, msvideo1 and Roq\n**Description:** Removes video compression artifacts\n**Link:** <https://de-next.owncube.com/index.php/s/wKjLmYsq7M5JAmx>\n**Iterations:** 200k\n**batch_size:** 1\n**HR_size:** 128px\n**Epoch:** 21\n**Dataset_size:** 8k\n**Pretrained_Model_G:** Null",
        "author": "twittman",
        "when": "2019-07-03T15:26:36.893000+00:00",
        "name": null,
        "hasLink": true
    },
    "1x-DeBink-v4": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x-DeBink-v4/v5/v6\n**License:** CC BY-NC-SA 4.0\n**Link:** https://drive.google.com/drive/folders/1qn1LeYI93HDKHxREEK4EYsDB6LDsZ_dI?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** This model removes early 2000s Bink and other compression artifacts.\n\n**Iterations:** 4,000/16,400/800\n**batch_size:** 8\n**HR_size:** 64\n**Epoch:** 36\n**Dataset:** Lossless footage of Metal Arms: Glitch in the System from Dolphin emulator (provided by zstorm4), LR compressed with Bink and frames extracted losslessly\n**Dataset_size:** 3,620\n**OTF Training** No\n**Pretrained_Model_G:** 1x-ESRGAN.pth\n\n**Description:** This model removes early 2000s Bink compression artifacts.\nIt was trained on `Metal Arms: Glitch in the System`, but it should work for most videos compressed with Bink or similar compression methods. I've found that it works well on MPEG-2 as well.\n\nDeBink V4 is better suited toward more detailed geometry and texture detials, but has somewhat strong color shift.\n\nDeBink V5 is by far the strongest, but it can also remove details in some scenes. This model introduces a lot of noise in areas with strong DoF.\n\nDeBink V6 is a nice balance between the two.\n\nBink:\nhttps://u.cubeupload.com/Kim1100/DeBinkv4Comparison.png\nhttps://u.cubeupload.com/Kim1100/DeBinkv5Comparison.png\n\nMPEG-2:\nhttps://u.cubeupload.com/Kim1100/148comparison.png\n\nExtra comparisons:\n<https://imgsli.com/NjQxODc>\n<https://imgsli.com/NjQxODg>",
        "author": "Kim",
        "when": "2021-08-06T23:26:12.018000+00:00",
        "name": "1x-DeBink-v4/v5/v6",
        "hasLink": false
    },
    "1x-DeBink-v5": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x-DeBink-v4/v5/v6\n**License:** CC BY-NC-SA 4.0\n**Link:** https://drive.google.com/drive/folders/1qn1LeYI93HDKHxREEK4EYsDB6LDsZ_dI?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** This model removes early 2000s Bink and other compression artifacts.\n\n**Iterations:** 4,000/16,400/800\n**batch_size:** 8\n**HR_size:** 64\n**Epoch:** 36\n**Dataset:** Lossless footage of Metal Arms: Glitch in the System from Dolphin emulator (provided by zstorm4), LR compressed with Bink and frames extracted losslessly\n**Dataset_size:** 3,620\n**OTF Training** No\n**Pretrained_Model_G:** 1x-ESRGAN.pth\n\n**Description:** This model removes early 2000s Bink compression artifacts.\nIt was trained on `Metal Arms: Glitch in the System`, but it should work for most videos compressed with Bink or similar compression methods. I've found that it works well on MPEG-2 as well.\n\nDeBink V4 is better suited toward more detailed geometry and texture detials, but has somewhat strong color shift.\n\nDeBink V5 is by far the strongest, but it can also remove details in some scenes. This model introduces a lot of noise in areas with strong DoF.\n\nDeBink V6 is a nice balance between the two.\n\nBink:\nhttps://u.cubeupload.com/Kim1100/DeBinkv4Comparison.png\nhttps://u.cubeupload.com/Kim1100/DeBinkv5Comparison.png\n\nMPEG-2:\nhttps://u.cubeupload.com/Kim1100/148comparison.png\n\nExtra comparisons:\n<https://imgsli.com/NjQxODc>\n<https://imgsli.com/NjQxODg>",
        "author": "Kim",
        "when": "2021-08-06T23:26:12.018000+00:00",
        "name": "1x-DeBink-v4/v5/v6",
        "hasLink": false
    },
    "1x-DeBink-v6": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x-DeBink-v4/v5/v6\n**License:** CC BY-NC-SA 4.0\n**Link:** https://drive.google.com/drive/folders/1qn1LeYI93HDKHxREEK4EYsDB6LDsZ_dI?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** This model removes early 2000s Bink and other compression artifacts.\n\n**Iterations:** 4,000/16,400/800\n**batch_size:** 8\n**HR_size:** 64\n**Epoch:** 36\n**Dataset:** Lossless footage of Metal Arms: Glitch in the System from Dolphin emulator (provided by zstorm4), LR compressed with Bink and frames extracted losslessly\n**Dataset_size:** 3,620\n**OTF Training** No\n**Pretrained_Model_G:** 1x-ESRGAN.pth\n\n**Description:** This model removes early 2000s Bink compression artifacts.\nIt was trained on `Metal Arms: Glitch in the System`, but it should work for most videos compressed with Bink or similar compression methods. I've found that it works well on MPEG-2 as well.\n\nDeBink V4 is better suited toward more detailed geometry and texture detials, but has somewhat strong color shift.\n\nDeBink V5 is by far the strongest, but it can also remove details in some scenes. This model introduces a lot of noise in areas with strong DoF.\n\nDeBink V6 is a nice balance between the two.\n\nBink:\nhttps://u.cubeupload.com/Kim1100/DeBinkv4Comparison.png\nhttps://u.cubeupload.com/Kim1100/DeBinkv5Comparison.png\n\nMPEG-2:\nhttps://u.cubeupload.com/Kim1100/148comparison.png\n\nExtra comparisons:\n<https://imgsli.com/NjQxODc>\n<https://imgsli.com/NjQxODg>",
        "author": "Kim",
        "when": "2021-08-06T23:26:12.018000+00:00",
        "name": "1x-DeBink-v4/v5/v6",
        "hasLink": false
    },
    "1x-DEDXT": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_DEDXT\n**License:** WTFPL\n**Link:** https://mega.nz/file/TltS1TQY#CW3zaI24pxFXggeRjgnXb4bSfjnQsY_ch1bogCjMCmU\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** DXT compression\n\n**Iterations:** 24k+\n**batch_size:** 8\n**HR_size:** 64\n**Epoch:** 4k+\n**Dataset:** Textures from paladins\n**Dataset_size:** 150\n**OTF Training** No\n**Pretrained_Model_G:** BCGONE_DetailedV2\n\n**Description:** To retain details while removing artifacts caused by dxt compression on textures\nOriginal -> BCGone-DetailedV2 -> DXTDecompressor_Source_V3 -> DEDXT",
        "author": "CF2lter",
        "when": "2022-02-26T13:02:18.494000+00:00",
        "name": "1x_DEDXT",
        "hasLink": true
    },
    "1x-DeEdge": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_DeEdge_105000_G\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/u/1/folders/1pT75GdIU4JAH75eJzpxsaHfIoJ1a_gXB\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Halo Remover\n\n**Iterations:** 105k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** 299\n**Dataset:** Images from DIV2K and random anime drawings\n**Dataset_size:** 800 (+600)\n**OTF Training:** No\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 54.441, SSIM: 0.99903, LPIPS: 0.00034184\n\n**Description:** Attempts to remove the haloing around the edges. I trained this model by downscaling the images with Mitchell (HR) and Lanczos (LR).\n\nhttps://imgsli.com/MjgxMDU/3/0",
        "author": "Sisyphean",
        "when": "2020-11-03T22:27:08.686000+00:00",
        "name": "1x_DeEdge_105000_G",
        "hasLink": true
    },
    "1x-DeJpeg-Fatality-PlusULTRA": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** DeJpeg Fatality_PlusULTRA!\n**License:** MPLv2\n**Link:** <https://de-next.owncube.com/index.php/s/w82HLrLWmWi4SQ5>\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Jpeg compression removal\n\n**Iterations:** 200k\n**batch_size:** 1\n**HR_size:** 128px\n**Epoch:** 5\n**Dataset:** Real life, Manga and digital text\n**Dataset_size:** 26k\n**OTF Training** No\n**Pretrained_Model_G:** 1x_DeJpeg_Fatality_01_200000_G.pth\n\n**Description:** Anything and everything that is Jpeg, will tremble before you!",
        "author": "twittman",
        "when": "2019-10-29T17:52:17.258000+00:00",
        "name": "DeJpeg Fatality_PlusULTRA!",
        "hasLink": true
    },
    "1x-DeRoqBeta-lite": {
        "content": "Name: 1x_DeRoqBeta-lite\nLicense: CC BY-NC-SA 4.0\nLink: <https://u.pcloud.link/publink/show?code=XZa4XhVZDokh5FX7Osz08HXz9jUTjupp8UNX> <https://mega.nz/file/qAY11ZBR#zgLbwfSI2MDYHb0HxehLkq4cppEg-U9bplxHu5hVrgU>\nModel Architecture: ESRGAN-lite\nScale: 1\nPurpose: Incomplete lite model to remove ROQ compression",
        "author": "Kim",
        "when": "2022-07-27T02:04:40.969000+00:00",
        "name": "1x_DeRoqBeta-lite",
        "hasLink": true
    },
    "1x-DitherDeleterV2-Smooth": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 1x_DitherDeleter-Smooth_104000_G\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/folders/1XKREcpHnaYz_dcmkMPDbyETcXzDcQsR2?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Dither Remover\n\n**Iterations:** 104k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** 1385\n**Dataset:** Random anime images\n**Dataset_size:** 300\n**Data-Efficient GAN Training:** Yes\n**OTF Training:** No\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 37.708, SSIM: 0.96218, LPIPS: 0.02023\n\n**Description:** Attempts to remove dithering from the image. Compared to my older versions, this model removes a lot more dithering along with some color banding. I trained this model by reducing the color depth by 48 colors for every image in the dataset. Afterwards, I added 32-bit Riemersma dithering.\n\nhttps://imgsli.com/Mzg0MTk/1/2",
        "author": "Sisyphean",
        "when": "2021-01-23T22:03:55.377000+00:00",
        "name": "1x_DitherDeleter-Smooth_104000_G",
        "hasLink": true
    },
    "1x-DitherDeleterV3-Smooth": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n\n**Name:** 1x_DitherDeleterV3-Smooth-[32]_115000_G\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/folders/1XKREcpHnaYz_dcmkMPDbyETcXzDcQsR2?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Dedithering\n\n**Iterations:** 115k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** 839\n**Dataset:** Random anime images\n**Dataset_size:** 550\n**OTF Training:** No\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 38.312, SSIM: 0.96523, LPIPS: 0.013638\n\n**Description:** Attempts to remove the damages done by dithering. For this model, I downscaled all of the HR images to make their pixel size closer to 1000x1000 using the Box filter and then downscaled them again by 50% using the Point filter. Afterwards, I applied 32-bit Riemersma to every image in the dataset.\n\nhttps://imgsli.com/NTA5MjU/1/0",
        "author": "Sisyphean",
        "when": "2021-04-18T18:49:41.192000+00:00",
        "name": "1x_DitherDeleterV3-Smooth-[32]_115000_G",
        "hasLink": true
    },
    "1x-Dotzilla-Compact": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_Dotzilla_Compact\n**License:** WTFPL\n**Link:** https://mega.nz/file/VQkDmSgb#dlVuJZQ__yAKdJaWA_4Nx52YU_c2qy_a2oG9_a98dO4\n**Model Architecture:** Real-ESRGAN Compact\n**Scale:** 1\n**Purpose:** For animated content with dot crawl.\n\n**Iterations:** 80K\n**batch_size:** Variable (1-8)\n**HR_size:** 192\n**Dataset:** Blue Submarine No. 6 Toonami version aligned to the 2016 Discotek DVD release made up about half of the dataset. The remainder was hand selected frames from a variety of animated series cleaned up with Avisynth.\n**Dataset_size:** 177\n**OTF Training** No\n**Pretrained_Model_G:** 1x_Compact_Pretrain.pth\n\n**Description:** Wipes out dot crawl and rainbows in animation. It was primarily trained on 720x480 content, so will probably work best on content at that same resolution.\nhttps://imgsli.com/MTM4ODkz",
        "author": "Zarxrax",
        "when": "2022-12-12T19:30:04.411000+00:00",
        "name": "1x_Dotzilla_Compact",
        "hasLink": true
    },
    "1x-DoubleDetoon": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 1xDoubleDetoon\n**License:** CC BY-NC-SA\n**Link:** https://drive.google.com/file/d/12vkDn35cuVnq2KYpkaA3XfCmk6zY_u4_/view?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Detooning\n\n**Iterations:** 100k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** 58\n**Dataset:** Images from reddit, cartoonized using https://github.com/SystemErrorWang/White-box-Cartoonization\n**Dataset_size:** 6,604\n**OTF Training:** Yes, for a bit of added jpeg and stylization (extra black outlines) every once in a while\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 29.102, SSIM: 0.89189, LPIPS: 0.0811\n\n**Description:** An attempt to detoon images/drawings of people",
        "author": "Joey",
        "when": "2020-07-31T15:15:08.891000+00:00",
        "name": "1xDoubleDetoon",
        "hasLink": true
    },
    "1x-DXTDecompressor-Source-V3": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_DXTDecompressor_Source_V3-300000_G\n**License:** WTFPL\n**Link:** https://drive.google.com/file/d/1WXBNdlqDWV10a3L1_U7zuNkSpN9aBF0M/view?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1x\n**Purpose:** Removing compression artifacts from DXT1 compressed textures\n\n**Iterations:** 300,000\n**batch_size:** 2\n**HR_size:** 64\n**Epoch:** 50\n**Dataset:** Uncompressed textures from various Source Engine games (https://github.com/JosephtheKP/sourceengine_tga_archive) and some data provided by friends\n**Dataset_size:** 10k+ of various sizes\n**OTF Training** No\n**Pretrained_Model_G:** 1xESRGAN\n\n**Description:** This model was created to remove DXT1 compression artifacts from textures imported into the Source Engine. Compressed textures in the engine sometimes have a green-tint which this model also corrects. The data for this model contained a good mix of diffuse textures and normal maps which means this model is pretty good at removing compression from normals as well.\nCreating this model was a real learning experience for me and I hope someone finds a good use for it.\n\nhttps://imgsli.com/ODcwNDg\nhttps://imgsli.com/ODcwNDk\nhttps://imgsli.com/ODcwNTA\nhttps://imgsli.com/ODcwNTE",
        "author": "nyctomatic",
        "when": "2021-12-21T20:17:08.837000+00:00",
        "name": "1x_DXTDecompressor_Source_V3-300000_G",
        "hasLink": true
    },
    "1x-DXTless-SourceEngine": {
        "content": "<@&560103931204861954> <@&577839492199874570>\nName: 1x_DXTless_SourceEngine_170000_G\nLicense: WTFPL\nLink: https://u.pcloud.link/publink/show?code=XZTAmsXZzEO8tMJaYAF1MrKKnmOCKu7Ghedy\nModel Architecture: ESRGAN\nScale: 1\nPurpose: Source Engine DXT5 compression\n\nIterations: 170k\nDataset: Mostly TF2 textures with some misc stuff in, artificially expanded.\nDataset_size: 473\nOTF Training: No\nPretrained_Model_G: 1x_Saiyajin_DeJpeg_300000_G\n\nDescription: This model is made for Source Engine textures. It tries to remove compression artifacts such as blockyness, discoloration, green tint. It does pretty well on a lot of things, realistic stuff as well, but it was mostly made to work on TF2 textures. It's made to keep as much detail as possible, without any unnecessary denoising/sharpening. Huge thanks to <@!324585701389762561> for assisting me along this journey.\nhttps://imgsli.com/NDk1NTk\nhttps://imgsli.com/NDk1NjA\nhttps://imgsli.com/NDk1NjE\nhttps://imgsli.com/NDk1NjI",
        "author": "Xeller",
        "when": "2021-04-10T21:16:30.378000+00:00",
        "name": "1x_DXTless_SourceEngine_170000_G",
        "hasLink": true
    },
    "1x-Epsilon-one-compact": {
        "content": "<@&560103931204861954>  <@&577839492199874570>\nName: 1xEpsilon-one-compact.pth\nLicense: CC BY-NC-SA 4.0\nLink: https://drive.google.com/drive/folders/13PhLXD3Pvo1A7LwHGIeY_ekObva_333e?usp=share_link\nModel Architecture: realESRGAN compact\nScale: 1\nPurpose: give old anime DVDs a fresh paint \nIterations: 98,500\nbatch_size: 20\nHR_size: 128\nEpoch: unknown\nDataset:  frames from remastered 80s-90s anime\nDataset_size: ||320||<:psy_duck:905916792369729567> \nOTF Training: no\nPretrained_Model_G: 1x_Compact_Pretrain.pth by Zarxrax\nDescription: \nhttps://imgsli.com/MTM3Mzgz/2/3\nhttps://imgsli.com/MTM3Mzg1/2/3\nhttps://imgsli.com/MTM3Mzg2/0/1\nThis model is far from perfect, but it does a decent job on removing dot crawl and dehalo\\deblock old anime at fast rate without removing many details.\nchaining it with models like 2x-anifilm-compact or 2xLD-ANIME or 2x_AnimeClassics_UltraLite_510K will give good results i think.",
        "author": "evA-01",
        "when": "2022-12-03T07:32:12.584000+00:00",
        "name": "1xEpsilon-one-compact.pth",
        "hasLink": true
    },
    "1x-Fatality-DeBlur": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Scale:** 1x\n**Purpose:** Deblurring\n**Description:** Deblurs images. Some robustness against compression and noise.\n**License:** Your soul now belongs to me\n**Link:** <https://de-next.owncube.com/index.php/s/aAojXwLTPZto8rP>\n**Iterations:** 270k\n**batch_size:** 1\n**HR_size:** 128\n**Epoch:** 4\n**Dataset_size:** 41k tiles\n**Pretrained_Model_G:** 1x_DeJpeg_Fatality_01_175000_G.pth",
        "author": "twittman",
        "when": "2019-10-16T15:15:24.931000+00:00",
        "name": null,
        "hasLink": true
    },
    "1x-Filmify4K-v2": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_Filmify4K_v2_325000_G.pth\n**License:** Public domain\n**Link:** https://drive.google.com/drive/folders/1ZcMD3hy1YCm-hsrT6xYGvhboxSzhrJKg\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** 1080p films, upscaled to 4K/UHD using Topaz Gaia-HQ.\n**Iterations:** 325,000\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 488\n**Dataset:** Still frames from the 4K Blu-Ray of Lawrence of Arabia, downscaled to 1080p and upscaled with Gaia-HQ for the LRs\n**Dataset_size:** 8200\n**OTF Training** Yes\n**Pretrained_Model_G:** 1x_UnResize_V3_110000_G.pth\n\n**Description:** This model attempts to make films upscaled to 4K with Topaz Gaia-HQ look more natural and filmic. It sharpens, adds film grain, and smooths out small artefacts from the upscaling process. I recommend adding a tiny amount of grain to the input to seed the model (you can do this in VEAI), otherwise the film grain will remain static across frames that don't move much. Pretrain model used with permission to relicense from twittman.\n\nhttps://imgsli.com/NjE5MTE\nhttps://imgsli.com/NjE5MTI\nhttps://imgsli.com/NjE5MTM\nhttps://imgsli.com/NjE5MTQ",
        "author": "Muf",
        "when": "2021-07-19T22:56:32.475000+00:00",
        "name": "1x_Filmify4K_v2_325000_G.pth",
        "hasLink": true
    },
    "1x-Focus": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** Focus + Focus_Moderate\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZS4XhVZ1GKsTtkjdULdYn6z9pTHs7fdLvR7> <https://mega.nz/folder/OVomUZyT#vDyNntGY1MirBEACNjpqVg>\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** These models deblur images\n\n**Iterations:** 120k\n**batch_size:** 4 (8 virtual)\n**HR_size:** 64\n**Epoch:** 304\n**Dataset:** The UniScale Dataset + My Fabric dataset\n**Dataset_size:** 3067\n**OTF Training** Yes (Config here: <https://mega.nz/folder/OVomUZyT#vDyNntGY1MirBEACNjpqVg>)\n**Pretrained_Model_G:** 1xESRGAN\n\n**Description:** These models deblur most images. It was trained mostly on aniso2 and iso blurring (BSRGAN augmentation) with some gaussian mixed in. It performs well on most blurry images, but I'd recommend using something like Fatality_DeBlur for extreme gaussian blur.\n\nThe Moderate (it's labelled Soft in the comparisons) model is an interpolation.\n\nhttps://cdn.discordapp.com/attachments/893483065664483338/893483645803847690/unknown.png\nhttps://cdn.discordapp.com/attachments/893483065664483338/893497416207188028/unknown.png\nhttps://cdn.discordapp.com/attachments/893483065664483338/893483280895193118/unknown.png\nhttps://cdn.discordapp.com/attachments/893483065664483338/893496207832408135/unknown.png",
        "author": "Kim",
        "when": "2021-10-01T14:02:01.788000+00:00",
        "name": "Focus + Focus_Moderate",
        "hasLink": true
    },
    "1x-Focus-Moderate": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** Focus + Focus_Moderate\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZS4XhVZ1GKsTtkjdULdYn6z9pTHs7fdLvR7> <https://mega.nz/folder/OVomUZyT#vDyNntGY1MirBEACNjpqVg>\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** These models deblur images\n\n**Iterations:** 120k\n**batch_size:** 4 (8 virtual)\n**HR_size:** 64\n**Epoch:** 304\n**Dataset:** The UniScale Dataset + My Fabric dataset\n**Dataset_size:** 3067\n**OTF Training** Yes (Config here: <https://mega.nz/folder/OVomUZyT#vDyNntGY1MirBEACNjpqVg>)\n**Pretrained_Model_G:** 1xESRGAN\n\n**Description:** These models deblur most images. It was trained mostly on aniso2 and iso blurring (BSRGAN augmentation) with some gaussian mixed in. It performs well on most blurry images, but I'd recommend using something like Fatality_DeBlur for extreme gaussian blur.\n\nThe Moderate (it's labelled Soft in the comparisons) model is an interpolation.\n\nhttps://cdn.discordapp.com/attachments/893483065664483338/893483645803847690/unknown.png\nhttps://cdn.discordapp.com/attachments/893483065664483338/893497416207188028/unknown.png\nhttps://cdn.discordapp.com/attachments/893483065664483338/893483280895193118/unknown.png\nhttps://cdn.discordapp.com/attachments/893483065664483338/893496207832408135/unknown.png",
        "author": "Kim",
        "when": "2021-10-01T14:02:01.788000+00:00",
        "name": "Focus + Focus_Moderate",
        "hasLink": true
    },
    "1x-FrankenMapGenerator-CX-Lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_FrankenMapGenerator-CX-Lite\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZecVzXZEO6fAyO6w25YDc43BkSVHRgFdOwX\n**Model Architecture:** ESRGAN Lite [nf=32 nb=12]\n**Scale:** 1\n**Purpose:** Generating roughness and displacement maps\n\n**Iterations:** 215k\n**batch_size:** Varied\n**HR_size:** 64\n**Epoch:** 137\n**Dataset:** CC0 textures/displacement maps/roughness maps, 2K size, randomly downscaled OTF (by 1, 2, and 4) to get more variation\n**Dataset_size:** 14,784\n**OTF Training** Yes (for hr downscale)\n**Pretrained_Model_G:** 1x_DIV2K-Lite_450k.pth\n\n**Description:** This model generates \"Franken Maps\" (named after Frankenstein), which is a custom material map combination I made. Basically, the Red channel of RGB is just the texture converted to grayscale, the Green channel is the roughness map, and the Blue channel is the displacement map. I had to do this to get around the current limitation of CX loss where it requires a 3 channel output (otherwise I would have just made a 2 channel model, or separate single channel models). As of right now the channels need to be manually split from each other but I will be making a tool for doing this automatically in the coming days.",
        "author": "Joey",
        "when": "2020-11-07T06:20:10.292000+00:00",
        "name": "1x_FrankenMapGenerator-CX-Lite",
        "hasLink": true
    },
    "1x-GainRESV3-Aggro": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_GainRESV3 (Aggro,Natural,Passive)\n**License:** WTFPL\n**Link:** https://mega.nz/folder/yg0lHQoJ#sP8_BfDk2YlshFjOL9Qrtg\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Anti-Aliasing,Deblur\n\n**Iterations:** 9k+\n**batch_size:** 8\n**HR_size:** 32\n**Epoch:** 1k+\n**Dataset:** 5K resolution shots from paladins rendered by 200% for hr and 37.5%(1080p) for lr then downscaled to 1080\n**Dataset_size:** 150\n**OTF Training** No\n**Pretrained_Model_G:** BCGONE_DetailedV2\n\n**Description:** To eliminate aliasing and general artifacts caused by not enough resolution while bringing out details\nIm stopping its training here because it's getting worse, i think of some aligment issues by game's rendering pipeline + downscaling...\nOriginal -> SSAntiAlias9x -> GainRESV3(Aggro)",
        "author": "CF2lter",
        "when": "2022-02-26T14:07:19.693000+00:00",
        "name": "1x_GainRESV3 (Aggro,Natural,Passive)",
        "hasLink": true
    },
    "1x-GainRESV3-Natural": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_GainRESV3 (Aggro,Natural,Passive)\n**License:** WTFPL\n**Link:** https://mega.nz/folder/yg0lHQoJ#sP8_BfDk2YlshFjOL9Qrtg\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Anti-Aliasing,Deblur\n\n**Iterations:** 9k+\n**batch_size:** 8\n**HR_size:** 32\n**Epoch:** 1k+\n**Dataset:** 5K resolution shots from paladins rendered by 200% for hr and 37.5%(1080p) for lr then downscaled to 1080\n**Dataset_size:** 150\n**OTF Training** No\n**Pretrained_Model_G:** BCGONE_DetailedV2\n\n**Description:** To eliminate aliasing and general artifacts caused by not enough resolution while bringing out details\nIm stopping its training here because it's getting worse, i think of some aligment issues by game's rendering pipeline + downscaling...\nOriginal -> SSAntiAlias9x -> GainRESV3(Aggro)",
        "author": "CF2lter",
        "when": "2022-02-26T14:07:19.693000+00:00",
        "name": "1x_GainRESV3 (Aggro,Natural,Passive)",
        "hasLink": true
    },
    "1x-GainRESV3-Passive": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_GainRESV3 (Aggro,Natural,Passive)\n**License:** WTFPL\n**Link:** https://mega.nz/folder/yg0lHQoJ#sP8_BfDk2YlshFjOL9Qrtg\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Anti-Aliasing,Deblur\n\n**Iterations:** 9k+\n**batch_size:** 8\n**HR_size:** 32\n**Epoch:** 1k+\n**Dataset:** 5K resolution shots from paladins rendered by 200% for hr and 37.5%(1080p) for lr then downscaled to 1080\n**Dataset_size:** 150\n**OTF Training** No\n**Pretrained_Model_G:** BCGONE_DetailedV2\n\n**Description:** To eliminate aliasing and general artifacts caused by not enough resolution while bringing out details\nIm stopping its training here because it's getting worse, i think of some aligment issues by game's rendering pipeline + downscaling...\nOriginal -> SSAntiAlias9x -> GainRESV3(Aggro)",
        "author": "CF2lter",
        "when": "2022-02-26T14:07:19.693000+00:00",
        "name": "1x_GainRESV3 (Aggro,Natural,Passive)",
        "hasLink": true
    },
    "1x-Ghibli-Grain": {
        "content": "Name: Ghibli_Grain.pth\nLicense: CC BY-NC-SA 4.0\nLink: https://drive.google.com/file/d/1URPhkwgraCzWAIkB8Pk4ZPr1LTdfdwxJ/view?usp=sharing\nModel Architecture: ESRGAN \nScale: 1\nPurpose: Realistic Ghibli Grain \n\nIterations: 15,000\nbatch_size: 6\nHR_size: 1280\nEpoch: unknown\nDataset: Images from Kiki's Delivery Service Hayao Miyazaki collectors edition\nDataset_size: 10,000\nOTF Training no\nPretrained_Model_G: none\n\nDescription: Experiment Everything!  <a:ASPikaDab:515424398682095629>\nAttempt to get the nostalgic grain feel of classically animated Ghibli movies. Got the idea from  researching the making of Ronin's <a:PKT_blue:941251780224909373> 1x_AnimeFilmGrain28k. This was made running Nate video denoising with the preset \"more denoising\" on kiki's delivery service then overlaying it 66.65 percent over the original with DaVinci. On digital drawn anime it gives a slightly [10%] more organic/sharp feel to black lines. Matches well with content that already has a light digital grain. Run Twice for heavy grain. Eternal Thanks to all that indulge all my <:psyduck:971562758254493736>  Wonderings <a:element_fire_neon:748292335372337192> \nhttps://slow.pics/c/M6dNTu6G\nhttps://imgsli.com/MTM0Nzcw/9/8",
        "author": "nonogamester",
        "when": "2022-11-16T17:29:53.784000+00:00",
        "name": "Ghibli_Grain.pth",
        "hasLink": true
    },
    "1x-ITF-SkinDiffDDS-v1": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** x1_ITF_SkinDiffDDS_v1\n**License:** CC BY-NC-SA 4.0\n**Link:** https://drive.google.com/drive/folders/1y-QoWGAnF8YiX764rWrDOGzndWtpCfF3?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** DDS Compressed Skin Diffuse Textures\n\n**Iterations:** 100k\n**batch_size:** 48\n**HR_size:** 32\n**Dataset:** Custom\n**Dataset_size:** 870\n**OTF Training:** No\n**Pretrained_Model_G:** 1x_BC1-smooth2.pth\n\n**Description:** \nRemoves banding, blocking, dithering, aliasing, noise and color tint on DDS Compressed Skin Diffuse Textures. \n\nThis should work extremely well on most modern DDS compression types. The training set was compressed with BC3/DXT5, BC3/DXT5 Fast, BC2/DXT3, BC2/DXT3 Fast, and a small number of JPEG compressed images to cover outliers.\n\nThis model is trained to remove the slight green color tint that DDS compression tends to add to skin textures, so the model output will not match the original color tone of the input image. This is the desired result though, as DDS compression shifts the colors to a sickly green tint and this model corrects that to more natural color tones.\n\nThe training set included faces, body parts, eyes, mouths and hair in a variety of skin types and tones so it should work well on most related diffuse textures.\n\nHowever it's not just limited to skin, many other images and textures can be cleaned with this model. Designed to be used as a first step cleaning pass before applying additional models after. Check out the other ITF Models.\n\n**Comparisons:** \n||https://imgsli.com/MTIyMzE5||\n||https://imgsli.com/MTIyMzIw||\n||https://imgsli.com/MTIyMzIx||",
        "author": "intheflesh",
        "when": "2022-08-24T09:45:39.048000+00:00",
        "name": "x1_ITF_SkinDiffDDS_v1",
        "hasLink": true
    },
    "1x-ITF-SkinDiffDetail-Lite-v1": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** x1_ITF_SkinDiffDetail_Lite_v1\n**License:** CC BY-NC-SA 4.0\n**Link:** https://drive.google.com/drive/folders/1VkT6tpbCPn2gKZYPtawDJGMpLg6EyRpO?usp=sharing\n**Model Architecture:** ESRGAN-lite\n**Scale:** 1\n**Purpose:** Skin Diffuse Textures\n\n**Iterations:** 41k\n**batch_size:** 12\n**HR_size:** 64\n**Dataset:** Custom\n**Dataset_size:** 400\n**OTF Training** No\n**Pretrained_Model_G:** 50/50 Interpolation of DIV2K-Lite and SpongeBC1-Lite\n\n**Description:** Adds plausible high frequency detail and removes subtle blur. This is an early unfinished attempt at a x1 Lite model designed specifically for enhancing detail on skin diffuse textures of 3d characters. Even in its current state it works quite well.\n\nBest suited for uncompressed or cleaned textures - otherwise it may just enhance any existing compression artefacts too. The training set included faces, body parts, eyes and hair in a variety of skin types and tones so it should work well on most related diffuse textures.\n\nHowever it's not just limited to skin, many other images and textures can be enhanced with this model. The results are subtle, so run multiple times if desired.",
        "author": "intheflesh",
        "when": "2022-08-04T08:57:45.932000+00:00",
        "name": "x1_ITF_SkinDiffDetail_Lite_v1",
        "hasLink": true
    },
    "1x-JPEGDestroyer": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 1x_JPEGDestroyer\n**License:** GNU GPLv3\n**Link:** https://drive.google.com/file/d/1C0Lnn3PfXrm4UX9trjQIWsmxtspjQjmm/view?usp=drivesdk\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** JPEG Images with a minimum of 40% compression.\n\n**Iterations:** 104000 + more on the pretrain\n**batch_size:** 1\n**HR_size:** 128\n**Epoch:** 9\n**Dataset:** LR: 40-60% quality 4:2:0/4:4:4 randomly compressed images\n**Dataset_size:** 10100 tiles\n**OTF Training** No\n**Pretrained_Model_G:** 1xESRGAN + previous attempts\n**Tensorboard Stats:** psnr: 33.984, ssim: 0.89233, lpips: 0.0062408\n\n**Description:** This model is meant to reduce or eliminate JPEG Compression without making the original images too smooth or killing detail. It manages to do a fairly good job but don't expect overly compressed images to work with this.\nIt can deal with blocking and most edge artifacts.",
        "author": "BlackScout",
        "when": "2020-02-19T11:43:27.872000+00:00",
        "name": "1x_JPEGDestroyer",
        "hasLink": true
    },
    "1x-KDM003-scans": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Notice:** Trained mostly on illustrations on matte white card paper. Experimental use for glossy paper and lightweight paper. Not recommended for photos. Will probably add paint texture, based on predominant illustration style in dataset.\n**Scale:** 1x\n**Purpose:** Clean up model for scanned illustrations.\n**Description:** Made to remove moire patterns, reduce small imperfections, and correct mild compression artifacts in scanned Kingdom Death: Monster illustrations. CMYK printing often shifts colors, so this is intended to reverse that color shifting as well.\n**Link:** https://www.dropbox.com/s/52hsqa7ymt5pgga/KDM003_scans_1x.pth\n**Iterations:** 160000\n**batch_size:** 2\n**HR_size:** 128\n**Epoch:** 14\n**Dataset_size:** 11120\n**Pretrained_Model_G:** Failed attempts based on ESRGAN_1x_JPEG_80to100",
        "author": "BoxDrop (he/him)",
        "when": "2019-07-29T17:06:01.132000+00:00",
        "name": null,
        "hasLink": true
    },
    "1x-Kim2091-DeJpeg-v0": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** Kim2091_DeJpeg_v0\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://mega.nz/file/jcZQ1J6A#qLlSGBFQPWyQ0kUbOcvPN5kEYalPGUu9iR6EkEuS-T4>\n**Model Architecture:** ESRGAN-superlite\n**Scale:** 1\n**Purpose:** JPEG 75+\n\n**Iterations:** 230k\n**Dataset:** Custom JPEG dataset\n\n**Description:** Model I forgot to release. This doesn't totally remove JPEG artifacts, but it does a decent job at a fast rate. It seemingly does a better job of retaining detail than some other JPEG models. The model is incomplete, I need to train it further on compact rather than ESRGAN. This is just a temporary release\n\nhttps://cdn.discordapp.com/attachments/903415274521374750/1041539960672632832/1668392853.8196685.png\nhttps://cdn.discordapp.com/attachments/903415274521374750/1041540021175463956/1668392885.0326388.png",
        "author": "Kim",
        "when": "2022-11-14T02:34:28.623000+00:00",
        "name": "Kim2091_DeJpeg_v0",
        "hasLink": true
    },
    "1x-Loyaldk-SharpKeroro": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: Platoon Model Set\nLicense: CC BY-NC-SA 4.0\nLink: <https://mega.nz/folder/5UAlQCRJ#2Bj1lq8PYeB3P9k5UKSvkQ>\nModel Architecture: ESRGAN (NF=32,48,64 NB=12,18,24)\nScale: 1, 2, 4\nPurpose: Upscale Anime while keeping as much of the original detail as possible. Isn't among the sharpest models and if the anime is somewhat blurry it will retain that detail but add more pixels to give a smoother edge. Doesn't change the color very much and may make some scene's very slightly darker.\nIterations: 500K + 150K\nbatch_size: 16,8\nHR_size: 128, 192, 320\nEpoch: 17-34\nDataset: Saga of Tanya The Evil, Gravity Falls, Lucky Star, One Punch Man, Redline, Soul Eater, Spirited Away, Your Lie in April and Your Name. Converted Episodes and Movies to 4:4:4 Chroma Subsampling using custom 1x model included. LR's Include H264 Compression, H265 Compression, 1x_DotCrawl_125000_G, 1x_DeteriorationFrames V1_102000_G, Gaussian Blur, Poisson Noise, Uniform Noise, JPEG Compression, 4:2:0 Chroma Subsampling, Dithering.\nDataset_size: 70,263 PNG's\nOTF Training: No\nPretrained_Model_G: Pony Models\n<https://imgsli.com/NTcxMjE>, <https://imgsli.com/NTcxMjI>, <https://imgsli.com/NTcxMjQ>, <https://imgsli.com/NTcxMjU>, <https://imgsli.com/NTcxMjY>, <https://imgsli.com/NTcxMjg>, <https://imgsli.com/NTcxMjk>, <https://imgsli.com/NTcxMzA>, <https://imgsli.com/NTcxMzE>",
        "author": "ChaseMMD",
        "when": "2021-06-07T19:17:25.846000+00:00",
        "name": "Platoon Model Set",
        "hasLink": true
    },
    "1x-MangaJPEGHQ": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** MangaJPEG\n**License:** CC BY-NC-SA 4.0\n**Link:** https://mega.nz/folder/fAg21TRI#kBk0synXgNqTGaXiW6nSZA\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Remove JPEG artifacts from manga without destroying screentone and other details\n\n**Iterations:** 150k for the MQ model, LQ and HQ finetuned for ~80k\n**batch_size:** 5\n**HR_size:** 64\n**Epoch:** 2~3\n**Dataset:** Expanded self curated custom manga dataset\n**Dataset_size:** 87.000 manga pages in various resolutions\n**OTF Training** Yes\n**Pretrained_Model_G:** MQ: None, LQ/HQ: MQ\n\n**Description:**\nI trained these to test esrgan lite models with single channel images. Really happy with the results, considering the size and performance of the models.",
        "author": "Bunzero",
        "when": "2021-06-18T02:14:25.912000+00:00",
        "name": "MangaJPEG",
        "hasLink": true
    },
    "1x-MangaJPEGHQPlus": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** MangaJPEG\n**License:** CC BY-NC-SA 4.0\n**Link:** https://mega.nz/folder/fAg21TRI#kBk0synXgNqTGaXiW6nSZA\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Remove JPEG artifacts from manga without destroying screentone and other details\n\n**Iterations:** 150k for the MQ model, LQ and HQ finetuned for ~80k\n**batch_size:** 5\n**HR_size:** 64\n**Epoch:** 2~3\n**Dataset:** Expanded self curated custom manga dataset\n**Dataset_size:** 87.000 manga pages in various resolutions\n**OTF Training** Yes\n**Pretrained_Model_G:** MQ: None, LQ/HQ: MQ\n\n**Description:**\nI trained these to test esrgan lite models with single channel images. Really happy with the results, considering the size and performance of the models.",
        "author": "Bunzero",
        "when": "2021-06-18T02:14:25.912000+00:00",
        "name": "MangaJPEG",
        "hasLink": true
    },
    "1x-MangaJPEGLQ": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** MangaJPEG\n**License:** CC BY-NC-SA 4.0\n**Link:** https://mega.nz/folder/fAg21TRI#kBk0synXgNqTGaXiW6nSZA\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Remove JPEG artifacts from manga without destroying screentone and other details\n\n**Iterations:** 150k for the MQ model, LQ and HQ finetuned for ~80k\n**batch_size:** 5\n**HR_size:** 64\n**Epoch:** 2~3\n**Dataset:** Expanded self curated custom manga dataset\n**Dataset_size:** 87.000 manga pages in various resolutions\n**OTF Training** Yes\n**Pretrained_Model_G:** MQ: None, LQ/HQ: MQ\n\n**Description:**\nI trained these to test esrgan lite models with single channel images. Really happy with the results, considering the size and performance of the models.",
        "author": "Bunzero",
        "when": "2021-06-18T02:14:25.912000+00:00",
        "name": "MangaJPEG",
        "hasLink": true
    },
    "1x-MangaJPEGMQ": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** MangaJPEG\n**License:** CC BY-NC-SA 4.0\n**Link:** https://mega.nz/folder/fAg21TRI#kBk0synXgNqTGaXiW6nSZA\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Remove JPEG artifacts from manga without destroying screentone and other details\n\n**Iterations:** 150k for the MQ model, LQ and HQ finetuned for ~80k\n**batch_size:** 5\n**HR_size:** 64\n**Epoch:** 2~3\n**Dataset:** Expanded self curated custom manga dataset\n**Dataset_size:** 87.000 manga pages in various resolutions\n**OTF Training** Yes\n**Pretrained_Model_G:** MQ: None, LQ/HQ: MQ\n\n**Description:**\nI trained these to test esrgan lite models with single channel images. Really happy with the results, considering the size and performance of the models.",
        "author": "Bunzero",
        "when": "2021-06-18T02:14:25.912000+00:00",
        "name": "MangaJPEG",
        "hasLink": true
    },
    "1x-N64clean": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 1x_N64clean\n**License:** CC BY-NC 4.0\n**Link:** https://drive.google.com/file/d/1GNguVMV4E5t_4dn23w6UMXVKN6Ff9XPt/view?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Cleaning N64 textures\n\n**Iterations:** 277,000\n**batch_size:** 20\n**HR_size:** 48\n**Epoch:** 1369\n**Dataset:** CC0 Textures\n**Dataset_size:** 4034 (not tiled)\n**OTF Training:** No\n**Frequency Separation:** No\n**Pretrained_Model_G:** 1x_BC1-smooth2.pth\n\n**Description:** N64 textures use a color depth of 5-bits per channel, this model attempts to clean them, restoring smooth gradients in textures\n\nBelongs under: Artifact Removal > Color Reduction > Banding",
        "author": "BlueAmulet",
        "when": "2020-07-17T04:54:48.802000+00:00",
        "name": "1x_N64clean",
        "hasLink": true
    },
    "1x-NMKD-h264Texturize": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** NMKD h264Texturize\n**License:** WTFPL\n**Link:** <https://u.pcloud.link/publink/show?code=kZbhBpXZUTSB7Xi7zlQfgb8CNqPjsbv3dbpX>\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Tries to reverse heavy h264 compression. Fails. Can be used to texturize images though.\n\n**Iterations:** 500k\n**batch_size:** 1\n**HR_size:** 128\n**Dataset:** DIV2K Train + Valid\n**Dataset_size:** 3600 (900 Unique)\n**OTF Training** Only rotation/flipping\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** Kind of a failed experiment to improve h264 video frames.\nIt fails to remove blocking, but it does texturize blurry areas. \nJust play around with it and maybe interpolate it. 500k is just too many iterations to not release it 😛",
        "author": "nmkd",
        "when": "2020-10-20T16:41:56.600000+00:00",
        "name": "NMKD h264Texturize",
        "hasLink": false
    },
    "1x-NMKD-Jaywreck3-Lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** NMKD Jaywreck3 & Jaywreck3-Soft\n**License:** WTFPL\n**Link:** <https://u.pcloud.link/publink/show?code=kZ8FtFXZN0MakfJ5eWzEkGNOddw5pjp1paN7>\n**Model Architecture:** ESRGAN Lite [nf=32 nb=12]\n**Scale:** 1\n**Purpose:** JPEG-compressed images, including those that have been compressed multiple times \n\n**Iterations:** 320k\n**batch_size:** 4\n**HR_size:** 112\n**Dataset:** DIV2K Train + Valid\n**Dataset_size:** 1800 (900 unique)\n**OTF Training** Only rotation/flipping\n**Pretrained_Model_G:** 1x_DIV2K-Lite @ 375k\n\n**Description:** Improved version of Jaywreck2 by disabling GAN loss (avoids GAN artifacts).\nIt's also a Lite model, meaning it has fewer parameter, which results in a ~2x speed increase for upscaling while also using less VRAM.\nIncludes Jaywreck3-Soft, which slightly denoises images.\n\n**pCloud is having some problems right now, so if the link doesn't work, try again later!**",
        "author": "nmkd",
        "when": "2020-10-13T13:50:25.846000+00:00",
        "name": "NMKD Jaywreck3 & Jaywreck3-Soft",
        "hasLink": false
    },
    "1x-NMKD-Jaywreck3-Soft-Lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** NMKD Jaywreck3 & Jaywreck3-Soft\n**License:** WTFPL\n**Link:** <https://u.pcloud.link/publink/show?code=kZ8FtFXZN0MakfJ5eWzEkGNOddw5pjp1paN7>\n**Model Architecture:** ESRGAN Lite [nf=32 nb=12]\n**Scale:** 1\n**Purpose:** JPEG-compressed images, including those that have been compressed multiple times \n\n**Iterations:** 320k\n**batch_size:** 4\n**HR_size:** 112\n**Dataset:** DIV2K Train + Valid\n**Dataset_size:** 1800 (900 unique)\n**OTF Training** Only rotation/flipping\n**Pretrained_Model_G:** 1x_DIV2K-Lite @ 375k\n\n**Description:** Improved version of Jaywreck2 by disabling GAN loss (avoids GAN artifacts).\nIt's also a Lite model, meaning it has fewer parameter, which results in a ~2x speed increase for upscaling while also using less VRAM.\nIncludes Jaywreck3-Soft, which slightly denoises images.\n\n**pCloud is having some problems right now, so if the link doesn't work, try again later!**",
        "author": "nmkd",
        "when": "2020-10-13T13:50:25.846000+00:00",
        "name": "NMKD Jaywreck3 & Jaywreck3-Soft",
        "hasLink": false
    },
    "1x-NoiseToner-Poisson-Detailed": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_NoiseToner-Poisson-Detailed_108000_G\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/folders/12j6BvuR7SU48ui9DHjBPB1vzPpQri9bD?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Noise Remover\n\n**Iterations:** 108k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** 658\n**Dataset:** Realistic images\n**Dataset_size:** 657\n**OTF Training:** No\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 33.073, SSIM: 0.88333, LPIPS: 0.031368\n\n**Description:** Attempts to remove the damages done from noise.\n\nhttps://imgsli.com/NzQ5MzM/2/0",
        "author": "Sisyphean",
        "when": "2021-10-07T03:19:34.429000+00:00",
        "name": "1x_NoiseToner-Poisson-Detailed_108000_G",
        "hasLink": false
    },
    "1x-NoiseToner-Poisson-Soft": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_NoiseToner-Poisson-Soft_101000_G\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/folders/12j6BvuR7SU48ui9DHjBPB1vzPpQri9bD?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Noise Remover\n\n**Iterations:** 101k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** 615\n**Dataset:** Realistic images\n**Dataset_size:** 657\n**OTF Training:** No\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 34.025, SSIM: 0.88995, LPIPS: 0.061426\n\n**Description:** Attempts to remove the damages done from noise.\n\nhttps://imgsli.com/MjU1ODU/3/1",
        "author": "Sisyphean",
        "when": "2020-10-14T21:17:50.791000+00:00",
        "name": "1x_NoiseToner-Poisson-Soft_101000_G",
        "hasLink": false
    },
    "1x-NoiseToner-Uniform-Detailed": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_NoiseToner-Uniform-Detailed_100000_G\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/folders/12j6BvuR7SU48ui9DHjBPB1vzPpQri9bD?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Noise Remover\n\n**Iterations:** 100k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** 609\n**Dataset:** Realistic images\n**Dataset_size:** 657\n**OTF Training:** No\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 35.103, SSIM: 0.92593, LPIPS: 0.018224\n\n**Description:** Attempts to remove the damages done from noise.\n\nhttps://imgsli.com/NzUyMTc/2/0",
        "author": "Sisyphean",
        "when": "2021-10-08T22:46:23.433000+00:00",
        "name": "1x_NoiseToner-Uniform-Detailed_100000_G",
        "hasLink": false
    },
    "1x-NoiseToner-Uniform-Soft": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_NoiseToner-Uniform-Soft_100000_G\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/folders/12j6BvuR7SU48ui9DHjBPB1vzPpQri9bD?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Noise Remover\n\n**Iterations:** 100k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** 609\n**Dataset:** Realistic images\n**Dataset_size:** 657\n**OTF Training:** No\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 36.012, SSIM: 0.93184, LPIPS: 0.031909\n\n**Description:** Attempts to remove the damages done from noise.\n\nhttps://imgsli.com/MjU1Nzg/2/1",
        "author": "Sisyphean",
        "when": "2020-10-14T21:06:26.531000+00:00",
        "name": "1x_NoiseToner-Uniform-Soft_100000_G",
        "hasLink": false
    },
    "1x-NormalMapGenerator-CX-Lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_NormalMapGenerator-CX-Lite\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZEdNHXZ3bpVwUoge94d9igFWxbjQQrcbpny\n**Model Architecture:** ESRGAN Lite [nf=32 nb=12]\n**Scale:** 1\n**Purpose:** Generating normal maps from textures\n\n**Iterations:** 200k\n**batch_size:** Varied\n**HR_size:** 64\n**Epoch:** 149\n**Dataset:** CC0 textures/normals, 2K size, randomly downscaled OTF (by 1, 2, and 4) to get more variation\n**Dataset_size:** 14,848\n**OTF Training** Yes (for hr downscale)\n**Pretrained_Model_G:** 1x_DIV2K-Lite_450k.pth\n\n**Description:** After seeing the creators of contextual loss had created a normal map generator for their second paper, I decided to attempt to create a normal map generator model myself. The results turned out better than I expected.",
        "author": "Joey",
        "when": "2020-11-05T00:59:43.958000+00:00",
        "name": "1x_NormalMapGenerator-CX-Lite",
        "hasLink": true
    },
    "1x-PixelSharpen": {
        "content": "<@&560103931204861954> <@&577839492199874570>\nName: 1x_PixelSharpen_100000\nLicense: CC BY-NC-SA 4.0\nLink: https://1drv.ms/u/s!Aip-EMByJHY27TDhiFPB39jM0pwq?e=5zAFtO\nModel Architecture: ESRGAN\nScale: 1\nPurpose: Restores blurry/upscaled pixel art.\n\nIterations: 100k\nbatch_size: 3\nHR_size: 128\nEpoch: 300+\nDataset: PixelJoint pixel art\nDataset_size: 186\nOTF Training: Yes\nPretrained_Model_G: 1xESRGAN\nValidation Stats: PSNR: ??, SSIM: ??, LPIPS: ??\n\nDescription: Tries to sharpen edges by removing anti-aliasing. Will reduce and dither like converting to an 8-bit colourspace, but without actually reducing it to that colourspace. Might be useful for artists to pretend they know how to do pixel art.\n\nhttps://imgsli.com/NDUxNDQ/5/4",
        "author": "DinJerr",
        "when": "2021-03-19T15:07:28.621000+00:00",
        "name": "1x_PixelSharpen_100000",
        "hasLink": true
    },
    "1x-Plants": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_Plants_400000_G.pth\n**License:** Public domain\n**Link:** https://drive.google.com/drive/folders/1ZcMD3hy1YCm-hsrT6xYGvhboxSzhrJKg\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Images of plants, trees or other foliage upscaled with Photoshop Preserve Details 2.0. Sharpens and \"subdivides\" details and noise so it doesn't look upscaled.\n**Iterations:** 400,000\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 3099\n**Dataset:** plant.zip from OutdoorSceneTrain_v2 upscaled 200% with Preserve Details 2.0\n**Dataset_size:** 1036\n**OTF Training** No\n**Pretrained_Model_G:** 1x_NMKD-h264Texturize_500k.pth\n\n**Description:** This was a proof of concept model to see if refining the output from an existing upscale (from Adobe or Topaz) is feasible with 1x ESRGAN models. Might work, might produce total garbage. Use it or don't, complain about it or don't :)\n\nhttps://imgsli.com/NTg2MDk\nhttps://imgsli.com/NTg2MDg\nhttps://imgsli.com/NTg2MTA",
        "author": "Muf",
        "when": "2021-06-23T15:30:46.895000+00:00",
        "name": "1x_Plants_400000_G.pth",
        "hasLink": true
    },
    "1x-RoQ-nRoll": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x-RoQ_nRoll\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZN4XhVZwe96PiGYukb1N0yy4frn5BtuAlp7> <https://mega.nz/file/rUh0WCQI#rLy6foaaeagDxPKnVCVeMghO-VZr6AtTulrXmZMT6j4>\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Images or Video compressed with RoQ compression\n\n**Iterations:** 115k\n**batch_size:** 4 (8 virtual)\n**HR_size:** 32\n**Epoch:** 1793\n**Dataset:** Custom Dataset compressed with RoQ through ffmpeg. Consists of: Just Cause 3, Fallout 76, and Forza Horizon 3\n**Dataset_size:** 528 512x512 images\n**OTF Training** Yes (Quantization, Dithering, Image Gradient loss was used alongside these)\n**Pretrained_Model_G:** 1xESRGAN\n\n**Description:** This model decompresses images and video compressed using RoQ. Config and presets will be added when Mega decides to let me use their site! 😛\n\n**Comparions:**\n__**<https://imgsli.com/ODI5Njg>**__\nhttps://cdn.discordapp.com/attachments/911838608879677440/912086092755374200/unknown.png\nhttps://cdn.discordapp.com/attachments/911838608879677440/912084886792314880/unknown.png\nhttps://cdn.discordapp.com/attachments/911838608879677440/912085213859954788/unknown.png\nhttps://cdn.discordapp.com/attachments/911838608879677440/912087766366564372/unknown.png",
        "author": "Kim",
        "when": "2021-11-21T21:06:19.480000+00:00",
        "name": "1x-RoQ_nRoll",
        "hasLink": true
    },
    "1x-SBDV-DeJPEG-Lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_SBDV-DeJPEG-Lite\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZvfeFXZMyUSYScxAizUmTKjax0JQzTk9Gfk\n**Model Architecture:** ESRGAN (lite)\n**Scale:** 1\n**nf/nb:** 32/12\n**Purpose:** General purpose de-JPEG\n\n**Iterations:** 130k\n**batch_size:** 12\n**HR_size:** 64\n**Epoch:** 60\n**Dataset:** Season 11 SpongeBob frames and DIV2K, randomly downscaled and JPEG compressed with varying quality\n**Dataset_size:** SpongeBob: 24,110, DIV2K: 800\n**OTF Training** Yes\n**Pretrained_Model_G:** 1x_DIV2K-Lite_80k.pth\n\n**Description:** A test of dejpegging with a lite model. Seems to work pretty well.",
        "author": "Joey",
        "when": "2020-10-11T00:50:04.100000+00:00",
        "name": "1x_SBDV-DeJPEG-Lite",
        "hasLink": true
    },
    "1x-SheeepIt": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_SheeepIt\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZR4XhVZydMYvi4LDHHXuf4sOogdd7qhR0J7> <https://mega.nz/folder/GBQ10DbR#hfQ_Hl9EvUZg0wqdA_M4_A>\n**Model Architecture:** ESRGAN-superlite\n**Scale:** 1\n**Purpose:** Frames from the show \"Sheeep\"\n\n**Iterations:** 8.8k\n**batch_size:** 2/4\n**HR_size:** 96\n**Epoch:** 47\n**Dataset:** Sheeep release frames and production frames from tenpence in Animation Upscale\n**Dataset_size:** 4\n**OTF Training** No\n**Pretrained_Model_G:** None\n\n**Description:** This model was trained to restore \"Sheeep\" while retaining and enhancing the noise present in the show. The model amazingly finished training in only 8.8k iterations with no pretrain, with a dataset of only 4 image pairs. This model should work well for animes and cartoons with a lot of grain present. There are some slight haloing issues in dark colors unfortunately, but I was unable to fix it.\n\nI'd prefer the name \"SheepIt!\" but I don't want to mess up any file paths 😛\n\nhttps://cdn.discordapp.com/attachments/903415274521374750/999820017652740176/1658446060.8526323.png\nhttps://cdn.discordapp.com/attachments/903415274521374750/999821036583391292/1658446301.0955725.png\nhttps://cdn.discordapp.com/attachments/903415274521374750/999821212718993478/1658446346.8536465.png\nhttps://cdn.discordapp.com/attachments/903415274521374750/999820756043169803/1658446237.0734627.png",
        "author": "Kim",
        "when": "2022-07-21T23:33:48.397000+00:00",
        "name": "1x_SheeepIt",
        "hasLink": true
    },
    "1x-SpongeBC1-Lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_SpongeBC1-Lite\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZ8f9HXZaNaKIDTqP9YPl33NGlz7VmaA3kqV>\n**Model Architecture:** ESRGAN Lite [nf=32 nb=12]\n**Scale:** 1\n**Purpose:** BC1/DXT1 compression\n\n**Iterations:** 100k\n**batch_size:** 8\n**HR_size:** 64\n**Epoch:** 32\n**Dataset:** Spongebob season 11 frames downscaled 8x with box and compressed using Wand (imagemagick)'s BC1/DXT1 compression.\n**Dataset_size:** 24,123\n**OTF Training** No\n**Pretrained_Model_G:** 1x_DIV2K-Lite_80k\n\n**Description:** First ever lite BC1/DXT1 model. Probably only useful for cartoony textures like those in spongebob games or other cartoon licensed games.\nhttps://u.pcloud.link/publink/show?code=XZX29HXZpRGedp3MS5j4w7OwP8leJfktnlKX",
        "author": "Joey",
        "when": "2020-11-01T21:16:48.996000+00:00",
        "name": "1x_SpongeBC1-Lite",
        "hasLink": true
    },
    "1x-Spongebob-De-Quantize": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** Spongebob De-Quantize\n**License:** CC-BY-NC 4.0\n**Link:** https://drive.google.com/open?id=1qoiIvjMuazD6MCw_aZ5pEPESA6Pi03oM\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Removed color quantization/indexing and dithering from cartoon style images and textures\n\n**Iterations:** 130k\n**batch_size:** 1\n**HR_size:** 128\n**Epoch:** 26\n**Dataset:** 1 frame from every scene of every episode of season 11 of Spongebob, downscaled 50% with nearest neighbor, then downscaled by 25% with box for the HR, then color quantized with 256 colors for the LR\n**Dataset_size:** 4,803\n**OTF Training** Yes (for auto cropping)\n**Pretrained_Model_G:** 1x_1xDEDITHER_32_512_126900_G\n\n**Description:** This model was trained to remove the color quantization in BFBB's textures, but could be used for other things as well. I decided to train this becuase the other model left many textures blurry enough where they could not be upscaled correctly using another model after.",
        "author": "Joey",
        "when": "2019-11-03T03:29:36.509000+00:00",
        "name": "Spongebob De-Quantize",
        "hasLink": false
    },
    "1x-SpongeColor-Lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_SpongeColor-Lite\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZpAGFXZARxWIu9Pp4YDLUSIN8I8aFuJ5TKV\n**Model Architecture:** ESRGAN (lite) [nf=32 nb=12]\n**Scale:** 1\n**Purpose:** Colorization of grayscale images\n\n**Iterations:** 55k\n**batch_size:** 12\n**HR_size:** 64\n**Epoch:** 22\n**Dataset:** Spongebob season 11 and DIV2K, randomly downscaled and then converted to grayscale\n**Dataset_size:** SpongeBob: 24,110, DIV2K: 800\n**OTF Training** Yes\n**Pretrained_Model_G:** None\n\n**Description:** The first attempt at ESRGAN colorization that produces more than 2 colors. Doesn't work that great but it was a neat experiment.",
        "author": "Joey",
        "when": "2020-10-15T07:39:35.689000+00:00",
        "name": "1x_SpongeColor-Lite",
        "hasLink": true
    },
    "1x-SS-Anti-Alias-9x": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 1x_SSAntiAlias9x\n**License:** CC BY-NC 4.0\n**Link:** https://drive.google.com/open?id=11nMqVF9sUN2jRRagYc3wdWNX5acEy0MJ\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Anti Aliasing\n\n**Iterations:** 125280\n**batch_size:** 12\n**HR_size:** 32\n**Epoch:** 367\n**Dataset:** Randomly generated images\n**Dataset_size:** 4096\n**OTF Training** No\n**Pretrained_Model_G:** None\n\n**Description:** Attempts to add anti aliasing to an image. Model was trained with 9x SSAA from randomly generated pictures featuring textured triangles, circles, rotated rectangles, and thin lines.",
        "author": "BlueAmulet",
        "when": "2020-02-27T16:47:47.097000+00:00",
        "name": "1x_SSAntiAlias9x",
        "hasLink": true
    },
    "1x-sudo-inpaint-PartialConv2D": {
        "content": "**Name:** 1x_sudo_inpaint_PartialConv2D_424000_G.pth\n**License:** CC BY-NC-SA 4.0\n**Link:** https://e.pcloud.link/publink/show?code=kZQOu7ZldzmFyMPUcFNGkEvwqOxQ8Bl3CeX https://www.mediafire.com/folder/xwjs6qww1kzvr/sudo_inpaint_partconv2D\n**Model Architecture:** ESRGAN [nf=64, nb=23]\n**Scale:** 1\n**Purpose:** Inpainting\n**Iterations:** 424k\n**batch_size:** 1\n**HR_size:** 310\n**Epoch:** 26\n**Dataset:** Full res yande.re images\n**Dataset_size:** 9147\n**OTF Training:** No\n**VRAM during training:** ~15GB with AMP\n**Pretrained_Model_G:** None\n\nDescription: Experimental PartialConv2D attempt to paint with ESRGAN. Took ~10.4 days of training on a P100 and around 1.5 months in total due to Colab limits. Not sure if I will continue training it since training is **very** slow, but may get better.. Warning: Result can vary with different tilesizes. Try not to tile your data.",
        "author": "sudo rm -rf / --no-preserve-root",
        "when": "2020-12-05T11:42:30.268000+00:00",
        "name": "1x_sudo_inpaint_PartialConv2D_424000_G.pth",
        "hasLink": true
    },
    "1x-SwatKatsLite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 1x_SwatKatsLite_360000_G.pth\n**License:** CC BY-NC-SA 4.0\n**Link:** https://mega.nz/file/La5AQCKb#aXvqZE9ZhlImJICJ8DRGI9CZfdtL9J9quQvoRmYFan0\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Fix vertical blur / split lines / shadowing\n\n**Iterations:** 360,000\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** unknown\n**Dataset:** WEB-DL\n**Dataset_size:** 5,000+\n**OTF Training** Unknown\n**Pretrained_Model_G:** none\n\n**Description:** A 1x lite model of my 2xSwatKats. Resolves the same video problems as before, but 1x and faster and meant for chaining to other 2x models (or whatever). Input MUST be 540 vertical as the blur problem is very resolution sensitive.  \n\nhttps://slow.pics/c/F4ilU8PM",
        "author": "SaurusX",
        "when": "2021-12-22T22:39:41.416000+00:00",
        "name": "1x_SwatKatsLite_360000_G.pth",
        "hasLink": true
    },
    "1x-ThePi7on-Solidd-Deborutify-UltraLite": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: 1x_ThePi7on-Solidd_Deborutify_UltraLite_260k_G\nLicense: CC BY-NC-SA 4.0\nLink: https://mega.nz/file/CYoXCAKR#eRKwpP9rCL9vFCsO41oVImvmVwAEOtYUlp2CquX0Fw8\nModel Architecture: ESRGAN\nScale: 1\nPurpose: Sharpening, line darkening and slight line thinning, specifically made for the Boruto anime.\nIterations: 260K\nbatch_size: 1\nHR_size: 128\nEpoch: 2363\nDataset: Some frames from the sharper shippuden episodes\nDataset_size: 110 frames\nOTF Training: No\nLite model: NF=10, NB=5\nDescription: This is a joint effort between <@!658256524069568522> and me.\nThe purpose of the model is to improve the blurry/watery lineart of Boruto, trying to mimic the lineart of the sharper Shippuden episodes. Some comparisons here: https://slow.pics/c/pnM4Nxmc",
        "author": "ThePi7on",
        "when": "2021-05-09T14:28:21.839000+00:00",
        "name": "1x_ThePi7on-Solidd_Deborutify_UltraLite_260k_G",
        "hasLink": true
    },
    "1x-ToonVHS": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** ToonVHS\n**License:** CC BY-NC-SA 4.0\n**Link:** https://drive.google.com/file/d/1w88lL_XGrE8n9sSIq97nLcVASqQNCtP6/view?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** To reduce tape noise, reduce macro block visibility, sharpen lines and reduce halos for VHS recordings of cartoons and anime.\n\n**Iterations:** 300,000\n**batch_size:** 5\n**HR_size:** 64\n**Epoch:** 33,327\n**Dataset:** An old Cartoon Network VHS Tape upload.\n**Dataset_size:** 44\n**OTF Training** No\n**Pretrained_Model_G:** 1xESRGAN\n\n**Description:** Best when used on cartoons, it can work on anime. Due to the dataset it does struggle a bit with orange colors and grainy dark spots. This model is meant to be used to clean up the image before using it on a 2x or 4x model.\nExamples of ToonVHS + other models: https://imgsli.com/OTQ0NjM/0/1\nExamples of ToonVHS by itself: https://imgsli.com/OTQ0NjQ/4/5",
        "author": "Redswag Scalliwag",
        "when": "2022-02-06T08:14:04.136000+00:00",
        "name": "ToonVHS",
        "hasLink": true
    },
    "1x-VHS-Sharpen": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** VHS-Sharpen-1x\n**License:** CC BY-NC-SA 4.0\n**Link:** https://mega.nz/file/Z5IXBSrA#bE9VasE60zLqZ_NggOhs40Z9l2z8DapROZb5VX0F3h0\n**Model Architecture:** ESRGAN\n**Scale:** 1\n**Purpose:** Sharpens VHS footage.\n\n**Iterations:** 46K\n**batch_size:** 5\n**HR_size:** 64\n**Epoch:** 34\n**Dataset:** 4K Blu-Ray movie stills, de-grained and downscaled, and ran through a VCR onto a VHS.\n**Dataset_size:** 9,384\n**OTF Training** No\n**Pretrained_Model_G:** 1xESRGAN\n\n**Description:** Make old VHS footage _crispy_. This model will not work on video and images with noticeable JPEG/Video compression artifacts, noticeable interlacing or haloing, heavy tape distortion/artifacts and scenes with tons of detail. For best results, use a downscaled HD capture of the VHS tape you intend to use it on. https://imgsli.com/NDUxNDY/2/3\nEdit: If it's too crispy you can blur it with a radius of 0.5 pixels to look more natural",
        "author": "Redswag Scalliwag",
        "when": "2021-03-19T21:48:09.678000+00:00",
        "name": "VHS-Sharpen-1x",
        "hasLink": true
    },
    "2x-190562-vimeo-enchanced": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 190562_vimeo_enchanced.pth\n**License:** mit\n**Link:** http://91.232.191.56/models/190581.pth \n**Model Architecture:** CAIN <:troll:822195324638068767> (<https://gitlab.com/hubert.sontowski2007/cainapp>) \n**Scale:** 2x\n**Purpose:** RL videos\n\n**Iterations:** 190562\n**batch_size:** 1-24\n**HR_size:**  i trained it on 256-480p  and 480i/576i videos\n**Epoch:** i don't remember\n**Dataset:** vimeo90k + my data\n**Dataset_size:** over 100k\n\n**Description:**  cain model\nhere example on abba - summer night city\n[edit updated link]\nhttps://cdn.discordapp.com/attachments/724353640521007145/864602331080163328/1.mp4\nhttps://cdn.discordapp.com/attachments/724353640521007145/864602311203880970/mp4.mp4",
        "author": "Deleted User",
        "when": "2021-07-13T20:26:31.822000+00:00",
        "name": "190562_vimeo_enchanced.pth",
        "hasLink": true
    },
    "2x-anifilm-compact": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x and 4x-anifilm_compact\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZ48XhVZjc7TduHV05pTSo5vfbFVD5T8mEc7> <https://mega.nz/folder/XZgSmAoa#KNKbXZVBDq4UD1NJLHm1oQ>\n**Model Architecture:** Real-ESRGAN Compact\n**Scale:** 2/4\n**Purpose:** Dragon Ball Frames\n**Iterations:** 110k for 2x, 75k for 4x \n**batch_size:** 6\n**HR_size:** 64 for 2x, 128 for 4x\n**Dataset:** Dragon Ball Z Movies\n**Dataset_size:** 2836\n**OTF Training** No\n**Pretrained_Model_G:** 2x and 4x_Compact_Pretrain.pth (<@153566544469819393>'s Pretrains)\n\n**Description:** This model is based on a private model by <@447803386142654475> named `4x_eula_anifilm_v1_225k`. He sent me a copy of the model, and I decided to train a compact model based on it with his permission. This model seems to fix the majority of the issues the original model had while being far faster, it's just a tiny bit softer in some images with a bit less capacity for heavy artifact removal.\n\nThe dataset consists of Dragon Ball movies converted to YUV24 with <@184640817565138944>'s help to reduce artifacts, then upscaled with ArtClarity and eula_anifilm. LRs are the original frames right from DVD. As a result, this model corrects some color space issues. The 2x model's HRs were downscaled by 50% with Lanczos.\n\nThe 2x and 4x models are pretty close in output despite being trained separately.\n\nThe models in the `Real-ESRGAN Compatible` folder are the original output from Real-ESRGAN's training code for compatibility reasons.\n\n2x Comparison:\nhttps://cdn.discordapp.com/attachments/903415274521374750/1004152057739087972/1659478894.4425747.png\n<https://imgsli.com/MTE5MjUz>\n\n4x Comparison:\n<https://imgsli.com/MTE5MjU0>\nhttps://cdn.discordapp.com/attachments/903415274521374750/1004158991544365097/1659480539.543144.png",
        "author": "Kim",
        "when": "2022-08-02T23:12:47.850000+00:00",
        "name": "2x and 4x-anifilm_compact",
        "hasLink": true
    },
    "2x-AnimeClassics-UltraLite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_AnimeClassics_UltraLite_510K\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://mega.nz/file/taBFxAAR#6wW-DrGh6oy2uz6aAKJpa36ngE2eKcDNA8Psy11oK10>\n**Model Architecture:** ESRGAN\n**Scale:** 2\n**Purpose:** A general purpose 2x anime upscale model. Best when used on grainy or classic anime content.\n\n**Iterations:** 510,000\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** 111\n**Dataset:** Blu-ray Remaster HRs/Generated LRs, and DVD Sources\n**Dataset_size:** 4,222\n**OTF Training** No\n**Pretrained_Model_G:** 2X_DigitalFilmV5_Lite.pth\n\n**Description:** A 2x Ultra Lite model coming in under 8MB. Trained with over 15 sets of LRs ranging in a wide amount of issues. Handles Rainbows, Dot Crawl, MPEG/H.264 Compression, and may even assist in removing halos, and fixing blurriness in certain cases. This is my first public model for everyone. Best when used on old anime that is grainy. I can't say what anime it's best suited for as I have tried multiple series, and have found it does a good job on most all the tests. I wouldn't say use this for Western Animation, but it may work. I have done a few tests that I have shown in the upscale results, but that was chained with other models to achieve such a result. This model is meant to retain the more natural look of a series. There is a color shift on the end result, not drastic, but still noticable. I figure you should fix any color issues in post that way to give a more polished upscale. Big thanks to <@!812014326298181682> for the model name, and just helping out in general with anything.\n\nhttps://imgsli.com/OTg3MjM\nhttps://imgsli.com/OTg3MjQ\nhttps://imgsli.com/OTg3MjU\nhttps://imgsli.com/OTg3Nzg",
        "author": "CG1989",
        "when": "2022-03-09T00:51:49.907000+00:00",
        "name": "2x_AnimeClassics_UltraLite_510K",
        "hasLink": true
    },
    "2x-ATLA-KORRA": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_ATLA_KORRA_336200_G.pth\n**License:** WTFPL\n**Link:** https://www108.zippyshare.com/v/PYWFtETd/file.html\n**Model Architecture: **ESRGAN\n**Scale:** 2\n**Purpose:** Upscaling of Animation  based on The Legend of Korra.\n**Iterations:** 336,200\n**batch_size:** 4\n**HR_size:** 128\n**Dataset:** The Legend of Korra first episode DVD frames and BluRay frames.\n**Dataset_size:** 436 images\n**OTF Training:** No\n**Pretrained_Model_G:** 2xESRGAN.pth\n\nDescription: \nTrained this model to test the results on Avatar the Last Airbender DVD.\n\nhttps://imgsli.com/NjIzMzU\nhttps://imgsli.com/NjIzMTY\nhttps://imgsli.com/NjIzMTc\nhttps://imgsli.com/NjIzMTg\nhttps://imgsli.com/NjIzMTk\nhttps://imgsli.com/NjIzMjA\nhttps://imgsli.com/NjIzMjE\nhttps://imgsli.com/NjIzMjQ\nhttps://imgsli.com/NjIzMjc\nhttps://imgsli.com/NjIzMjY\nhttps://imgsli.com/NjIzMjU",
        "author": "Neo-Raws",
        "when": "2021-07-22T21:12:40.847000+00:00",
        "name": "2x_ATLA_KORRA_336200_G.pth",
        "hasLink": true
    },
    "2x-BIGOLDIES": {
        "content": "<@&560103931204861954> <@&577839492199874570>\nName: 2x_BIGOLDIES_415000_G.pth\nLicense: CC BY-NC-SA 4.0\nLink:  https://1fichier.com/?23hzkj20isef5b75cw3y  (new link)\nScale: 2\nPurpose: upscaling old anime. help to denoise and find lines and dehalo\nIterations: 415000\nbatch_size: 4\nHR_size: 128\nEpoch: sorry, can't remember, been trained few months ago 😦\nDataset: LR from severals dvd anime and HR from bluray anime\nDataset_size: 2300\nOTF Training: YES\nPretrained_Model_G: 2x_PSNR.pth\nhttps://imgsli.com/MzI0MDc",
        "author": "solidd93110",
        "when": "2020-12-09T17:15:59.713000+00:00",
        "name": "2x_BIGOLDIES_415000_G.pth",
        "hasLink": true
    },
    "2x-BS-Wolly": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 2xBS_Wolly\n**License:** GNU GPLv3\n**Link:** https://drive.google.com/file/d/1zlc8BWOcpCbSZpwf7ArMCqpb0Tf9DlZk/view?usp=drivesdk\n**Model Architecture:** ESRGAN\n**Scale:** 2\n**Purpose:** Pixar Movies or Wall-E pictures/frames\n\n**Iterations:** 38000\n**batch_size:** 1\n**HR_size:** 256\n**Epoch:** 3\n**Dataset:** BD 1080p of Wall-E (FFMPEG extraction at 1FPS with frames of the movie except intro but forgot the credits oops X)\n**Dataset_size:** 9000 tiles of 256x256 \n**OTF Training** No\n**Pretrained_Model_G:** 2xESRGAN\n**TensorBoard Stats:** https://i.imgur.com/fBRkpY9.png\n\n**Description:** Originally thought of by Yui, this model aims to upscale images of either the Wall-E movie or anything Wall-E related. \nIt could possibly work as a model to upscale Pixar movies in general, but I haven't tested it out yet.\nThe results could be better in my opinion, but I've gone pretty far with it and I still think the results are good enough.\n\nAs always, feel free to use as a pretrain.",
        "author": "BlackScout",
        "when": "2020-02-07T21:40:36.032000+00:00",
        "name": "2xBS_Wolly",
        "hasLink": true
    },
    "2x-BSTexty": {
        "content": "**Name:** 2x_BSTexty\n**License:** GNU GPLv3\n**Link:** https://drive.google.com/file/d/15ovbadCoYs7q8nSd5Mq02PqBOpwiBkoS/view\n**Model Architecture:** ESRGAN\n**Scale:** 2\n**Purpose**: Upscale text\n\n**Iterations:** 86000\n**batch_size:** 4\n**Epoch:** 7\n**Dataset:** Randomly generated text, with randomly chosen fonts with varying sizes. Black text on white background\n**Dataset Size:** 44550 tiles\n**OTF:** Never\n**Pretrain:** 2x_ESRGAN\n**Tensorboard Stats:** psnr: 19.065, ssim: 0.91045, lpips: 0.22223\n\n**Description:** As the name might suggest, this model aims to upscale text with less distortion that other models. It seems to do a good job generally, but don't expect it to be a state of the art model that can upscale magazines and stuff. It makes things more readable but since it was train on B/W pictures it desaturates them.",
        "author": "BlackScout",
        "when": "2020-03-04T21:36:31.356000+00:00",
        "name": "2x_BSTexty",
        "hasLink": true
    },
    "2x-Bubble-AnimeScale-Compact-v1": {
        "content": "<@&560103931204861954>  <@&577839492199874570> \n**Name:** 2x_Bubble_AnimeScale_Compact_v1\n**License:** CC BY-NC-SA 4.0\n**Link:** https://github.com/Bubblemint864/AI-Models/releases/tag/2x_Bubble_AnimeScale_Compact_v1\n**Model Architecture:** Compact\n**Scale:** 2\n**Purpose:** Upscaling anime frames\n\n**Iterations:** 1,355,000\n**batch_size:** 6\n**Epoch:** 4,438\n**Dataset:** SFW and NSFW anime frames that were sourced from Blu Ray discs and high bitrate Web releases. These were then degraded with various different methods.\n**Dataset_size:** 1,845\n**OTF Training:** No\n**Pretrained_Model_G:** Kim2091_CrappyCompactV2.pth\n\n**Description:** This is my first model, so it's not perfect, but I wanted to see if I could train an upscaling model that didn't result in a lot of detail loss and deblurring like the current Compact upscaling models. I believe I accomplished this, but I was unable to reduce contrast shifting. The contrast shifting may cause skin tones to appear incorrect on bright frames, but it's not too bad overall! I'll list a few examples below; more can be found by clicking the Overview link on the Github release page.\n\nhttps://imgsli.com/MTM0MzMx\nhttps://imgsli.com/MTM0MzM3\nhttps://imgsli.com/MTM0MzQw",
        "author": "Bubblemint",
        "when": "2022-11-13T15:11:56.217000+00:00",
        "name": "2x_Bubble_AnimeScale_Compact_v1",
        "hasLink": true
    },
    "2x-Bubble-AnimeScale-SwinIR-Small-v1": {
        "content": "<@&560103931204861954>  <@&577839492199874570> \n**Name:** 2x_Bubble_AnimeScale_SwinIR_Small_v1\n**License:** CC BY-NC-SA 4.0\n**Link:** https://github.com/Bubblemint864/AI-Models/releases/tag/2x_Bubble_AnimeScale_SwinIR_Small_v1\n**Model Architecture:** SwinIR (Small)\n**Scale:** 2\n**Purpose:** Upscaling anime frames\n\n**Iterations:** 175,000\n**batch_size:** 8\n**Epoch:** 1,343\n**Dataset:** SFW and NSFW anime frames that were sourced from Blu Ray discs. These were then degraded with various different methods.\n**Dataset_size:** 1,042\n**OTF Training:** No\n**Pretrained_Model_G:** None\n\n**Description:** 2x_Bubble_AnimeScale_SwinIR_Small_v1 was trained to upscale anime frames faithfully without major contrast shifting compared to my compact model. Although much slower compared to my compact model, the results look significantly better! A few example upscales are listed below; more can be found by clicking the Overview link on the Github release page.\n\nhttps://imgsli.com/MTM2MjAx\nhttps://imgsli.com/MTM2MjAy\nhttps://imgsli.com/MTM2MjAz",
        "author": "Bubblemint",
        "when": "2022-11-26T17:11:08.311000+00:00",
        "name": "2x_Bubble_AnimeScale_SwinIR_Small_v1",
        "hasLink": true
    },
    "2x-cainliteanime": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** cainliteanime.pth\n**License:** mit(<https://gitlab.com/hubert.sontowski2007/cainapp/-/blob/main/LICENSE>)\n**Model Architecture:** CAIN lite (<https://gitlab.com/hubert.sontowski2007/cainapp>) \n**Scale:** 2x\n**Purpose:** Anime\n\n**Iterations:** 450533\n**batch_size:** 3\n**HR_size:** 848x480\n**Epoch:** 207\n\nhttps://cdn.discordapp.com/attachments/724353640521007145/873861876352704533/1.webm",
        "author": "Deleted User",
        "when": "2021-08-06T12:01:17.549000+00:00",
        "name": "cainliteanime.pth",
        "hasLink": false
    },
    "2x-cainREALTIME": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** cainREALTIME\n**License:**  mit\n**Model Architecture:**  CAIN 1 group\n**Scale:** 2\n\n**Dataset:** Mostly abba music videos",
        "author": "Deleted User",
        "when": "2022-01-28T14:59:03.835000+00:00",
        "name": "cainREALTIME",
        "hasLink": false
    },
    "2x-CGIMaster-v1": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** CGIMaster_v1\n**License:** GNU GPL3\n**Model Architecture:** ESRGAN\n**Scale:**  2\n**Purpose:** 2D/3D CGI cartoons\n**Link:** https://lbry.tv/@Madiator2011:e/x2_CGIMaster_v1:6\n**Iterations:** 409k\n**batch_size:** 6\n**HR_size:** 256\n**Epoch:** 1920\n**Dataset:** Full frames of Dragon Prince as HQ and for LR frames compressed with YouTube compression\n**Dataset_size:** 1800\n**OTF Training** No\n**Pretrained_Model_G:** Sharp_Anime_v2\n\n**Description:** Model is trained on mixed 3D/2D CGI animations on some 2D animations it might sharpen the edges.\nGeneral usage is to upscale CGI animations compressed by YouTube.\nhttps://imgsli.com/Mzc1NTM",
        "author": "Madiator",
        "when": "2021-01-14T10:18:47.677000+00:00",
        "name": "CGIMaster_v1",
        "hasLink": true
    },
    "2x-cvpv6": {
        "content": "<@&560103931204861954> <@&577839492199874570>\nName: cvpv6.pth\nLicense: mit(https://gitlab.com/hubert.sontowski2007/cainapp/-/blob/main/LICENSE)\nModel Architecture: CAIN lite (https://gitlab.com/hubert.sontowski2007/cainapp) \nScale: 2x\nPurpose: Anime\nhttps://files.catbox.moe/nxh1cv.pth",
        "author": "Deleted User",
        "when": "2021-09-21T17:14:50.158000+00:00",
        "name": "cvpv6.pth",
        "hasLink": true
    },
    "2x-DigiGradients-Lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_DigiGradients_Lite_486k.pth\n**License:** CC BY-NC-SA 4.0\n**Link:** https://mega.nz/file/PfhxWCRT#R59B_ovEwdiojZbas76SCbUSv-uNd1ZSOdZ0vQp-6w4\n**Model Architecture:** ESRGAN (lite)\n**Scale:** 2\n**Purpose:** Upscale digital animation of TMNT 2003 quickly\n\n**Iterations:** 486,000\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** unknown\n**Dataset:** Images from The Batman (series) HD WEB-DL.\n**Dataset_size:** 1,500 - 3,000\n**OTF Training** no\n**Pretrained_Model_G:** none\n\n**Description:** A very focused model meant for upscaling the TMNT 2003 DVDs. Degradations were added via AVISynth in order to match the video on the TMNT 2003 DVDs to correct the source problems. Problems corrected include aliased red chroma, chroma vertical blur, bad deinterlacing, banding, compression \"grain\", and poor animation line detail. The AVS scripts for the LR's were run through HCEnc to get authentic low bit rate MPEG-2 artifacts for fixing. By design, the final model gives a very digital-looking result and does not do a good job of retaining textures as the style of TMNT 2003 is all flats and gradients.\nhttps://imgsli.com/MTAxNjc1\nhttps://imgsli.com/ODQ2NzI",
        "author": "SaurusX",
        "when": "2022-08-24T16:19:26.139000+00:00",
        "name": "2x_DigiGradients_Lite_486k.pth",
        "hasLink": true
    },
    "2x-DigitalFilmV5-Lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** DigitalFilmV5 Lite\n**License:** WTFPL\n**Link:** Attached.\n**Model Architecture:** ESRGAN\n**Scale:** 2\n**Purpose:** Upscaling Dragon Ball Z DBox DVD's. \n\n**Iterations:** 43k\n**batch_size:** 4\n**HR_size:** 256\n**Epoch:** 920\n**Dataset:** DB/DBZ/DBGT Dragon Box DVD frames, upscaled with various models such as DigitalFrames2.0, FilmFrames677k, SharpAnimeV2, and manually edited to only show the best parts of each model. I call it image stacking.\n**Dataset_size:** 561\n**OTF Training** No\n**Pretrained_Model_G:** None due to being lite.\n\n**Description:** Built over a. long. time... this model was stressing the limits of what can be done with ESRGAN and grainy sources. It keeps some grain, but also doing some cleaning, sharpening, and fixing.\n\n**Screenshot Samples:**\n<https://imgsli.com/NDE0MzQ>\n<https://imgsli.com/NDE0MzU>\nhttps://imgsli.com/NDE0MzY\n<https://imgsli.com/NDE0Mzc>\n<https://imgsli.com/NDE0Mzg>\n<https://imgsli.com/NDE0Mzk>",
        "author": "OptimusPrimal",
        "when": "2021-02-19T20:06:33.284000+00:00",
        "name": "DigitalFilmV5 Lite",
        "hasLink": false
    },
    "2x-DigitoonLite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_DigitoonLite_216k\n**License:** CC BY-NC-SA 4.0\n**Link:** https://mega.nz/file/XWpFzRID#398qZx763bMSBsPB2MfgZqROGxdB0VldrIgH_prwa7U\n**Model Architecture:** ESRGAN (lite)\n**Scale:** 2\n**Purpose:** High speed digital animation upscaling\n\n**Iterations:** 216,000\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** 104\n**Dataset:** Blu-ray, HDTV, and DVD\n**Dataset_size:** 21,000\n**OTF Training** unknown\n**Pretrained_Model_G:** Private model\n\n**Description:** Meant as a versatile model for upscaling high detail digital anime and cartoons. Has debanding, MPEG-2 correction, and halo reduction. Trained to handle both 4:3 and 16:9 DVD material with equal efficacy. Will retain a lot of textures except for the really high freq stuff. https://imgsli.com/MTE1Mzg4",
        "author": "SaurusX",
        "when": "2022-07-06T01:46:59.802000+00:00",
        "name": "2x_DigitoonLite_216k",
        "hasLink": true
    },
    "2x-explodV1": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** explodV1\n**License:**  BSD-3-Clause\n**Link:** https://mega.nz/file/M49hFKrZ#oiJ27D52UL4vV2SCkKbiDS7PErBidGpseDX3D3FMpuQ\n**Model Architecture:** CAIN YUV\n**Scale:**  2x\n**Purpose:** Anime\n**Iterations:** 506951\n**HR_size:**  1920x1080\n**Epoch:** 83\n**Dataset:** Animethemes\n\n**Description:** I think one of sharpest models for anime\nPlz don't steal without credits, k thx.",
        "author": "Deleted User",
        "when": "2022-07-29T19:46:28.656000+00:00",
        "name": "explodV1",
        "hasLink": true
    },
    "2x-Faithful": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** Faithful 2x\n**License:** CC-BY-NC 4.0\n**Link:** https://drive.google.com/open?id=1BzT-KwUBiZzACxGdIN6w930aOVvvfhg7\n**Model Architecture:** ESRGAN\n**Scale:** 2\n**Purpose:** General upscaler, pixel art, small textures. Made to replicate the faithful 32x32 minecraft texture pack.\n\n**Iterations:** 130k\n**batch_size:** 80\n**HR_size:** 32\n**Epoch:** 3713\n**Dataset:** Faithful 32x32 for Minecraft 1.13 and 1.14 (HR) and default Minecraft 1.13 and 1.14 resources (LR)\n**Dataset_size:** 2,858\n**OTF Training** Yes, technically (was used for the auto-cropping), but I still had HR/LR pairs\n**Pretrained_Model_G:** 2xESRGAN. Technically I used my other unreleased faithful model as pretrained model for this one though.\n\n**Description:** This model was trained on the faithful 32x32 minecraft texture/resource pack, using both the 1.13 and 1.14 versions (which have different textures). For those that don't know, this texture pack is meant to upscale the 16x16 textures to 32x32 faithfully, as in make them as close as possible to the original, but higher resolution. The model does a decent job at replicating this.\n\nNote: Due to an error in the dataset, sometimes the model turns jagged white lines green. I've only encountered this in one texture I tried, so I think it is unlikely to happen. In almost all the images I tried, it worked great.",
        "author": "Joey",
        "when": "2019-09-08T19:28:34.082000+00:00",
        "name": "Faithful 2x",
        "hasLink": true
    },
    "2x-Faithful-Lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_Faithful-Lite\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=XZHawFXZhqD7dF4qtYHnqJVPnQd0ey0vW1JX\n**Model Architecture:** ESRGAN (lite)\n**Scale:** 2\n**nf/nb:** 32/12\n**Purpose:** Pixel art, game textures\n\n**Iterations:** 275k\n**batch_size:** 128\n**HR_size:** 32\n**Epoch:** 12398\n**Dataset:** Faithful32x Minecraft texture pack for versions 1.13 and 1.16\n**Dataset_size:** 2,928\n**OTF Training** No\n**Pretrained_Model_G:** None\n\n**Description:** A \"lite\" model version of my Faithful model",
        "author": "Joey",
        "when": "2020-10-12T19:22:56.630000+00:00",
        "name": "2x_Faithful-Lite",
        "hasLink": true
    },
    "2x-FaithfulSPSR": {
        "content": "<@&560103931204861954> @wiki \n**Name:** 2xFaithfulSPSR\n**License:** CC BY-NC-SA\n**Link:** https://drive.google.com/file/d/1Ful_AALrK-gxSwsTeMkzvwX5hh1ah8X8/view?usp=sharing\n**Model Architecture:** SPSR\n**Scale:** 2\n**Purpose:** Pixel art, general sharper textures\n\n**Iterations:** 150k\n**batch_size:** 30\n**HR_size:** 32\n**Epoch:** 1441\n**Dataset:** The 1.13 and 1.16 versions of the Faithful32x Minecraft texture pack\n**Dataset_size:** 2,928\n**OTF Training** No\n**Pretrained_Model_G:** 2xFaithful32_1316\n\n**Description:** Mainly just a test for SPSR. Seems to work better than the original 2xFaithful32_1316 that I used as a pretrained, even though it uses the same dataset. SPSR is looking good so far imo.",
        "author": "Joey",
        "when": "2020-07-19T21:13:24.713000+00:00",
        "name": "2xFaithfulSPSR",
        "hasLink": true
    },
    "2x-FakeFaith-Lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_FakeFaith-Lite\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZUMKFXZUG6BpzHR6wFGack19pvWgbQQ9OA7\n**Model Architecture:** ESRGAN (lite)\n**Scale:** 2\n**nf/nb:** 32/12\n**Purpose:** Pixel art, game textures\n\n**Iterations:** 105k\n**batch_size:** 24\n**HR_size:** 64\n**Epoch:** 102\n**Dataset:** Spongebob Season 11 frames, downscaled 50% with NN for the HR, and 50% NN again for the LR\n**Dataset_size:** 24,110\n**OTF Training** Yes\n**Pretrained_Model_G:** 2x_Faithful-Lite\n\n**Description:** An attempt at recreating the \"faithful\" style without using the faithful dataset -- aka keeping the \"pixel art\" style of pixel art. I'm not too sure about how it turned out, but I'm releasing it anyway 🙂",
        "author": "Joey",
        "when": "2020-10-12T22:39:29.072000+00:00",
        "name": "2x_FakeFaith-Lite",
        "hasLink": true
    },
    "2x-fidelbd-pokemodel": {
        "content": "<@&560103931204861954> @wiki \n**Name:** fidelbd_pokemodel\n**License:** WTFPL\n**Link:** https://www23.zippyshare.com/v/lhOStpVa/file.html\n**Model Architecture:** ESRGAN\n**Scale:** 2\n**Purpose:** upscale old anime\n\n**Iterations:** 300 0000 \n**batch_size:** 4\n**HR_size:**128\n**Epoch:** 114\n**Dataset:** Made with images from the first episode of pokemon released on blu ray for the HRs and 112.5% upscaled images for the LRs to match 1/2x the HRs.\n**Dataset_size:** 10 242 files\n**OTF Training** Yes\n**Pretrained_Model_G:** 2xESRGAN\n\n**Description:** Made this model to upscale old anime that looks blurry. https://imgsli.com/MjExMjQ/",
        "author": "Neo-Raws",
        "when": "2020-08-29T06:19:08.064000+00:00",
        "name": "fidelbd_pokemodel",
        "hasLink": true
    },
    "2x-Futsuu-Anime": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_Futsuu_Anime\n**License:** WTFPL\n**Link:** https://mega.nz/file/wdszCQRZ#bs7VIgvjgRiKiL5JVPqzCLlNl0kIQnDeD48-6ltWSLw\n**Model Architecture:** Real-ESRGAN Compact\n**Scale:** 2\n**Purpose:** General purpose anime and cartoon upscaler.\n\n**Iterations:** 130K\n**batch_size:** Variable (1-4)\n**HR_size:** 192\n**Dataset:** Frames from a wide variety of animated sources\n**Dataset_size:** 2661\n**Pretrained_Model_G:** 2x_Compact_Pretrain.pth\n\n**Description:** This model upscales while doing some sharpening and line darkening. Can also clean up some minor artifacts of various types. It is intended to to be a good general purpose upscaler that will work well with most animation.\nhttps://imgsli.com/MTQ4MDM2/",
        "author": "Zarxrax",
        "when": "2023-01-17T23:33:11.003000+00:00",
        "name": "2x_Futsuu_Anime",
        "hasLink": true
    },
    "2x-Gen5-Alpha": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_Gen5-Alpha\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZkR7hXZhfCA4WvVQvbxXkNN50tazuxj73uX\n**Model Architecture:** ESRGAN (4 in/out nc)\n**Scale:** 2\n**Purpose:** Pixel art with transparency\n\n**Iterations:** 175k\n**batch_size:** 8\n**HR_size:** 96\n**Epoch:** 409\n**Dataset:** Gen 5 Pokemon sprites\n**Dataset_size:** 3,376\n**OTF Training** Yes (for nearest-neighbor downscaling)\n**Pretrained_Model_G:** 4xFireAlpha\n\n**Description:** My attempt at making a 4 channel (RGBA) model for pixel art with alpha (transparency). For best use, make sure there is only black \"behind\" the transparency of the image you are upscaling. I'll be implementing something into basicsr and my esrgan fork soon that'll do that automatically for consistency, but for now you'll have to do it manually if its an issue. You'll know if its an issue cuz you'll get random blobs where transparency used to be. Anyway, it seems to work pretty well. If anybody wants to train a 4 channel model, just ask me and I'll let you know what config I used for losses.",
        "author": "Joey",
        "when": "2021-02-04T04:57:20.904000+00:00",
        "name": "2x_Gen5-Alpha",
        "hasLink": true
    },
    "2x-GT-evA": {
        "content": "<@&560103931204861954>  <@&577839492199874570>\nName: 2xGT-evA.pth\nLicense: CC BY-NC-SA 4.0\nLink: https://drive.google.com/file/d/1bwJpBHynGTIedIaZwp5XapCrJjO4BruF/view?usp=share_link\nModel Architecture: realESRGAN compact\nScale: 2\nPurpose: upscale old anime\nIterations: 120k\nbatch_size: 20\nHR_size: 128\nEpoch: \nDataset:  German blu ray of detective Conan and other shows \nDataset_size: 2710\nOTF Training: no\nPretrained_Model_G: 2x_Compact_Pretrain.pth by Zarxrax\nDescription: \nThis is my first 2xcompact model, the main purpose of it is to upscale Dragon ball GT.\nfor upscaling videos that have grain I would recommend denoising and dehaloing it before passing it to the model for temporal stability.",
        "author": "evA-01",
        "when": "2022-12-12T06:53:30.704000+00:00",
        "name": "2xGT-evA.pth",
        "hasLink": true
    },
    "2x-KcjpunkAnime-2-0-Lite": {
        "content": "**Name:** 2X_KcjpunkAnime_2.0_Lite_196496_G\n**License:**CC BY-NC-SA 4.0\n**Link:** https://mega.nz/folder/4oVRHQaA#vE3Lplfetc9z9SJOxZD6nA\n**Model Architecture:** ESRGAN-Light\n**Scale:** 2\n**Purpose:** Up-scaling Digital Animation \n**Iterations:** 196496\n**Batch_size:** 2\n**HR_size:** 128\n**Epoch:** 459\n**Datasets:** 1080P BD anime frames\n**Datasets_size:** 6840\n**OTF Training:** No\n**Pre-trained_Model_G:** None\n\n__**Description:**__\nThis is my first attempt to make Light model so I started with 2x version. This model is much faster and give better results than my previous one. \n\n  \nclick the links for image comparison : \nhttps://imgsli.com/NjgyODM\nhttps://imgsli.com/NjgyODI\nhttps://imgsli.com/NjgyODU",
        "author": "Kcjpunk",
        "when": "2021-08-27T14:59:01.712000+00:00",
        "name": "2X_KcjpunkAnime_2.0_Lite_196496_G",
        "hasLink": true
    },
    "2x-KemonoScale-v2": {
        "content": "**Name:** 2x_KemonoScale_v2\n**License:** GNU GPL3\n**Link:** https://cdn.discordapp.com/attachments/717333114745257994/1035367485945430056/2x_KemonoScale_v2.pth\n**Model Architecture:** ESRGAN \n**Scale:** 2\n**Purpose:** Upscaling frames from Irodori anime (namely kemono friends) from 540p (the source render resolution) to 1080p, low resolution flat shaded art, de-JPEG of the aforementioned\n\n**Iterations:** 409k + 6k\n**batch_size:** 6/2\n**HR_size:** 256\n**Epoch:** 51\n**Dataset:** @irodori7 twitter, irodori KF artbook scans, KF guidebook scans, custom renders in similar style\n**Dataset_size:** 251\n**OTF Training:** Maybe\n**Pretrained_Model_G:** x2_CGIMaster_v1\n\n**Description:** My first attempt at training a model. Trained off of CGIMaster with the custom dataset for this specific case, then interpolated a little bit with cgimaster again. i am a bit nervous posting this here since this is my first model and stuff but sorry\n\nhttps://cdn.discordapp.com/attachments/705815801290293329/850614850735439892/unknown.png\nhttps://cdn.discordapp.com/attachments/706353388278775892/850603991632183316/RacoonHD.mp4",
        "author": "EzoGaming",
        "when": "2021-06-05T06:27:53.479000+00:00",
        "name": "2x_KemonoScale_v2",
        "hasLink": false
    },
    "2x-LD-Anime-Compact": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_LD-Anime_Compact\n**License:** CC BY-NC-SA 4.0\n**Link:** https://mega.nz/file/xI8D2SCJ#oQ0h3k9lemeNJcuysJql7z55WV_NhOsKRpzpErIhcGc\n**Model Architecture:** Real-ESRGAN Compact\n**Scale:** 2\n**Purpose:** For upscaling old animated content.\n\n**Iterations:** 330K\n**batch_size:** Variable (1-4)\n**HR_size:** 192\n**Dataset:** DVD frames upscaled using Skr's LD-Anime model and color corrected.\n**Dataset_size:** 1720\n**OTF Training** No\n**Pretrained_Model_G:** 2x_Compact_Pretrain.pth\n\n**Description:** I trained Skr's great LD-Anime model on compact architecture. It upscales while fixing numerous video problems, including: noise/grain, compression artifacts, rainbows, dot crawl, halos and color bleed. This compact version may look slightly worse than Skr's original model, but runs significantly faster and also retains the correct colors better than the original did.\nhttps://imgsli.com/MTQyMzM3/0/1",
        "author": "Zarxrax",
        "when": "2022-12-22T22:12:13.921000+00:00",
        "name": "2x_LD-Anime_Compact",
        "hasLink": true
    },
    "2x-LD-Anime-Skr-v1-0": {
        "content": "**Name:** 2x_LD-Anime_Skr_v1.0\n**License:** CC BY-NC-SA 4.0\n**Link:** https://drive.google.com/file/d/18iWj4eWiMfUd2WyLCEzMlZK1JVT7L8nT/view?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 2\n**Purpose:** Denoise, dehalo, derainbow old anime\n\n**Iterations:** ~400,000\n**batch_size:** 8\n**HR_size:** 96\n**Epoch:** ~100\n**Dataset:** Laserdisc rips and remastered Blu-rays.\n**Dataset_size:** 50,000+\n**OTF Training:** No\n**Pretrained_Model_G:** 2xESRGAN\n\n**Description:** \nhttps://imgsli.com/NTA3MDU/\n\nThis is an interpolation of some laserdisc-based models I've been working on.\nIt was mostly made to combat the extreme haloing that is found on laserdiscs and clean up the lines.\nIt doesn't get everything perfectly (particularly rainbowing and signal dropouts), but this is where I'm currently up to. I plan to continue working on this.\nI've tried to put a variety of different images into my comparison link to give you an idea.",
        "author": "Skr",
        "when": "2021-04-17T17:31:16.983000+00:00",
        "name": "2x_LD-Anime_Skr_v1.0",
        "hasLink": true
    },
    "2x-Loyaldk-Giroro": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: Platoon Model Set\nLicense: CC BY-NC-SA 4.0\nLink: <https://mega.nz/folder/5UAlQCRJ#2Bj1lq8PYeB3P9k5UKSvkQ>\nModel Architecture: ESRGAN (NF=32,48,64 NB=12,18,24)\nScale: 1, 2, 4\nPurpose: Upscale Anime while keeping as much of the original detail as possible. Isn't among the sharpest models and if the anime is somewhat blurry it will retain that detail but add more pixels to give a smoother edge. Doesn't change the color very much and may make some scene's very slightly darker.\nIterations: 500K + 150K\nbatch_size: 16,8\nHR_size: 128, 192, 320\nEpoch: 17-34\nDataset: Saga of Tanya The Evil, Gravity Falls, Lucky Star, One Punch Man, Redline, Soul Eater, Spirited Away, Your Lie in April and Your Name. Converted Episodes and Movies to 4:4:4 Chroma Subsampling using custom 1x model included. LR's Include H264 Compression, H265 Compression, 1x_DotCrawl_125000_G, 1x_DeteriorationFrames V1_102000_G, Gaussian Blur, Poisson Noise, Uniform Noise, JPEG Compression, 4:2:0 Chroma Subsampling, Dithering.\nDataset_size: 70,263 PNG's\nOTF Training: No\nPretrained_Model_G: Pony Models\n<https://imgsli.com/NTcxMjE>, <https://imgsli.com/NTcxMjI>, <https://imgsli.com/NTcxMjQ>, <https://imgsli.com/NTcxMjU>, <https://imgsli.com/NTcxMjY>, <https://imgsli.com/NTcxMjg>, <https://imgsli.com/NTcxMjk>, <https://imgsli.com/NTcxMzA>, <https://imgsli.com/NTcxMzE>",
        "author": "ChaseMMD",
        "when": "2021-06-07T19:17:25.846000+00:00",
        "name": "Platoon Model Set",
        "hasLink": true
    },
    "2x-Loyaldk-Keroro": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: Platoon Model Set\nLicense: CC BY-NC-SA 4.0\nLink: <https://mega.nz/folder/5UAlQCRJ#2Bj1lq8PYeB3P9k5UKSvkQ>\nModel Architecture: ESRGAN (NF=32,48,64 NB=12,18,24)\nScale: 1, 2, 4\nPurpose: Upscale Anime while keeping as much of the original detail as possible. Isn't among the sharpest models and if the anime is somewhat blurry it will retain that detail but add more pixels to give a smoother edge. Doesn't change the color very much and may make some scene's very slightly darker.\nIterations: 500K + 150K\nbatch_size: 16,8\nHR_size: 128, 192, 320\nEpoch: 17-34\nDataset: Saga of Tanya The Evil, Gravity Falls, Lucky Star, One Punch Man, Redline, Soul Eater, Spirited Away, Your Lie in April and Your Name. Converted Episodes and Movies to 4:4:4 Chroma Subsampling using custom 1x model included. LR's Include H264 Compression, H265 Compression, 1x_DotCrawl_125000_G, 1x_DeteriorationFrames V1_102000_G, Gaussian Blur, Poisson Noise, Uniform Noise, JPEG Compression, 4:2:0 Chroma Subsampling, Dithering.\nDataset_size: 70,263 PNG's\nOTF Training: No\nPretrained_Model_G: Pony Models\n<https://imgsli.com/NTcxMjE>, <https://imgsli.com/NTcxMjI>, <https://imgsli.com/NTcxMjQ>, <https://imgsli.com/NTcxMjU>, <https://imgsli.com/NTcxMjY>, <https://imgsli.com/NTcxMjg>, <https://imgsli.com/NTcxMjk>, <https://imgsli.com/NTcxMzA>, <https://imgsli.com/NTcxMzE>",
        "author": "ChaseMMD",
        "when": "2021-06-07T19:17:25.846000+00:00",
        "name": "Platoon Model Set",
        "hasLink": true
    },
    "2x-Loyaldk-Kororo": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: Platoon Model Set\nLicense: CC BY-NC-SA 4.0\nLink: <https://mega.nz/folder/5UAlQCRJ#2Bj1lq8PYeB3P9k5UKSvkQ>\nModel Architecture: ESRGAN (NF=32,48,64 NB=12,18,24)\nScale: 1, 2, 4\nPurpose: Upscale Anime while keeping as much of the original detail as possible. Isn't among the sharpest models and if the anime is somewhat blurry it will retain that detail but add more pixels to give a smoother edge. Doesn't change the color very much and may make some scene's very slightly darker.\nIterations: 500K + 150K\nbatch_size: 16,8\nHR_size: 128, 192, 320\nEpoch: 17-34\nDataset: Saga of Tanya The Evil, Gravity Falls, Lucky Star, One Punch Man, Redline, Soul Eater, Spirited Away, Your Lie in April and Your Name. Converted Episodes and Movies to 4:4:4 Chroma Subsampling using custom 1x model included. LR's Include H264 Compression, H265 Compression, 1x_DotCrawl_125000_G, 1x_DeteriorationFrames V1_102000_G, Gaussian Blur, Poisson Noise, Uniform Noise, JPEG Compression, 4:2:0 Chroma Subsampling, Dithering.\nDataset_size: 70,263 PNG's\nOTF Training: No\nPretrained_Model_G: Pony Models\n<https://imgsli.com/NTcxMjE>, <https://imgsli.com/NTcxMjI>, <https://imgsli.com/NTcxMjQ>, <https://imgsli.com/NTcxMjU>, <https://imgsli.com/NTcxMjY>, <https://imgsli.com/NTcxMjg>, <https://imgsli.com/NTcxMjk>, <https://imgsli.com/NTcxMzA>, <https://imgsli.com/NTcxMzE>",
        "author": "ChaseMMD",
        "when": "2021-06-07T19:17:25.846000+00:00",
        "name": "Platoon Model Set",
        "hasLink": true
    },
    "2x-Loyaldk-LitePonyV1-0": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name**: 2x_Loyaldk-LitePony_380500 V1.0\n**License**: CC BY-NC-SA 4.0\n**Link**: https://mega.nz/file/RYxGEKRL#nTe0Zy2gB9e9QF5T8UhJSla1oEPQjSCXAB0BpNrTrRk\n**Model Architecture**: ESRGAN (NF=32, NB=12)\n**Scale**: 2\n**Purpose**: Upscale MLP episodes. Liter version.\n**Iterations**: 205K + 175K\n**batch_size**: 16\n**HR_size**: 128\n**Epoch**: 10 + 56\n**Dataset**: All MLP: FiM Episodes using scene detection frame grabbing as well as selected Prores4444 content https://shimmermare.com/4k-pony-project. Ran 2 separate datasets. LR's Include H264 Compression, Halo, Sharpening Halo/Fringe, Gaussian Blur, Dot crawl, Uniform Noise, Rainbow.\n**Dataset_size**: 330K + 50K\n**OTF Training**: No\n**Pretrained_Model_G**: 2x_ESRGAN",
        "author": "ChaseMMD",
        "when": "2021-03-01T23:03:37.364000+00:00",
        "name": "2x_Loyaldk-LitePony_380500 V1.0",
        "hasLink": true
    },
    "2x-Loyaldk-LitePonyV2-0": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: 2x_Loyaldk-LitePony_500000_V2.0\nLicense: CC BY-NC-SA 4.0\nLink: https://mega.nz/file/VM5WmLwb#QvO1n6w6llYRh8qauXXkXy4tElrBCwclCl5VHM3IH8M\nModel Architecture: ESRGAN (NF=32, NB=12)\nScale: 2\nPurpose: Upscale MLP episodes. Liter version. Able to handle compression better compared to V1.0 and no longer creates halo's/rainbows. Good for Vector 2D art however converts detail to blobs.\nIterations: 500K\nbatch_size: 20\nHR_size: 128\nEpoch: 66\nDataset: All MLP: FiM Episodes using scene detection frame grabbing as well as selected Prores4444 content https://shimmermare.com/4k-pony-project. Converted Episodes and Movies to 4:4:4 Chroma Subsampling using custom 1x model. LR's Include H264 Compression, H265 Compression, 1x_DotCrawl_125000_G, 1x_DeteriorationFrames V1_102000_G, Gaussian Blur, LitePony Chroma Bleed, Uniform Noise, JPEG Compression, 4:2:0 Chroma Subsampling.\nDataset_size: 132,120 PNG's\nOTF Training: No\nPretrained_Model_G: None",
        "author": "ChaseMMD",
        "when": "2021-04-06T20:52:47.504000+00:00",
        "name": "2x_Loyaldk-LitePony_500000_V2.0",
        "hasLink": true
    },
    "2x-Loyaldk-MediumPonyV2-0": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: 2x_Loyaldk-MediumPony_500000_V2.0\nLicense: CC BY-NC-SA 4.0\nLink: https://mega.nz/file/YMwm0TbZ#agDBMDkswkpMM7FE1tDRlGX4zIOE4D0WBzcWudkN7Mc\nModel Architecture: ESRGAN (NF=48, NB=18)\nScale: 2\nPurpose: Upscale MLP episodes. Medium Version. Able to handle compression better compared to V1.0 and no longer creates halo's/rainbows. Good for Vector 2D art however converts detail to blobs. Leaves more blobs when converting detail to blobs compared to litepony\nIterations: 500K\nbatch_size: 12\nHR_size: 128\nEpoch: 66\nDataset: All MLP: FiM Episodes using scene detection frame grabbing as well as selected Prores4444 content https://shimmermare.com/4k-pony-project. Converted Episodes and Movies to 4:4:4 Chroma Subsampling using custom 1x model. LR's Include H264 Compression, H265 Compression, 1x_DotCrawl_125000_G, 1x_DeteriorationFrames V1_102000_G, Gaussian Blur, LitePony Chroma Bleed, Uniform Noise, JPEG Compression, 4:2:0 Chroma Subsampling.\nDataset_size: 132,120 PNG's\nOTF Training: No\nPretrained_Model_G: None",
        "author": "ChaseMMD",
        "when": "2021-04-11T23:40:46.801000+00:00",
        "name": "2x_Loyaldk-MediumPony_500000_V2.0",
        "hasLink": true
    },
    "2x-Loyaldk-SuperPonyV1-0": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name**: 2x_Loyaldk-SuperPony_370000 V1.0\n**License**: CC BY-NC-SA 4.0\n**Link**: https://mega.nz/file/9dJFjYrT#LY4Op8myZGJsh6ytUvF6ITVGXpx8XODN_AG-kzvWg4Y\n**Model Architecture**: ESRGAN (NF=64, NB=23)\n**Scale**: 2\n**Purpose**: Upscale MLP episodes\n**Iterations**: 220K + 150K\n**batch_size**: 16\n**HR_size**: 128\n**Epoch**: 10 + 46\n**Dataset**: All MLP: FiM Episodes using scene detection frame grabbing as well as selected Prores4444 content https://shimmermare.com/4k-pony-project. Ran 2 separate datasets. LR's Include H264 Compression, Halo, Sharpening Halo/Fringe, Gaussian Blur, Dot crawl, Uniform Noise, Rainbow\n**Dataset_size**: 330K + 50K\n**OTF Training**: No\n**Pretrained_Model_G**: 2x_ESRGAN\n**Screenshot Comparsion**: https://imgsli.com/NDIwNTY\nhttps://imgsli.com/NDIwNTg\nhttps://imgsli.com/NDIwNjE\nhttps://imgsli.com/NDIwNjI\nhttps://imgsli.com/NDIwNjM",
        "author": "ChaseMMD",
        "when": "2021-02-24T22:52:40.131000+00:00",
        "name": "2x_Loyaldk-SuperPony_370000 V1.0",
        "hasLink": true
    },
    "2x-Loyaldk-SuperPonyV2-0": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: 2x_Loyaldk-SuperPony_500000_V2.0\nLicense: CC BY-NC-SA 4.0\nLink: https://mega.nz/file/IBhElRYJ#C_lVc8Vr2q8X6hEltJKD6I63U7oL_FDLeJgPh72AZmg\nModel Architecture: ESRGAN (NF=64, NB=24)\nScale: 2\nPurpose: Upscale MLP episodes. Full version. Able to handle compression better compared to V1.0 and no longer creates halo's/rainbows. Good for Vector 2D art however converts detail to blobs.\nIterations: 500K\nbatch_size: 8\nHR_size: 128\nEpoch: 66\nDataset: All MLP: FiM Episodes using scene detection frame grabbing as well as selected Prores4444 content https://shimmermare.com/4k-pony-project. Converted Episodes and Movies to 4:4:4 Chroma Subsampling using custom 1x model. LR's Include H264 Compression, H265 Compression, 1x_DotCrawl_125000_G, 1x_DeteriorationFrames V1_102000_G, Gaussian Blur, LitePony Chroma Bleed, Uniform Noise, JPEG Compression, 4:2:0 Chroma Subsampling.\nDataset_size: 132,120 PNG's\nOTF Training: No\nPretrained_Model_G: None",
        "author": "ChaseMMD",
        "when": "2021-04-30T14:42:33.575000+00:00",
        "name": "2x_Loyaldk-SuperPony_500000_V2.0",
        "hasLink": true
    },
    "2x-MangaScaleV3": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** MangaScaleV3\n**License:** CC BY-NC-SA 4.0\n**Link:** https://mega.nz/file/nR51EYYC#RcAKkxb8hAzvZAj4TLxnAukd9gbrYuUeyjUp2l1ffb8\n**Model Architecture:** ESRGAN+\n**Scale:** 2\n**Purpose:** To upscale manga including halftones, instead of trying to smooth them out.\n\n**Iterations:** At least 200k? Maybe?\n**batch_size:** Various\n**HR_size:** Around 96~256\n**Epoch:** (I didn't count)\n**Dataset:** Self curated custom manga dataset\n**Dataset_size:** 20.000 high-res manga pages\n**OTF Training** Yes\n**Pretrained_Model_G:** MangaScaleV2(not released)\n\n**Description:**\nhttps://imgsli.com/NTUyMjg\n\nThis model is trained on ESRGAN+ architecture. This means it ~~is incompatible~~ should work with Cupscale but as far as I can tell doesn't at the moment.\nTo use it at least for now, download Joey's ESRGAN fork https://github.com/JoeyBallentine/ESRGAN . \nThe model works best with up to 70 Quality JPEG compression, anything lower will probably not work too well.",
        "author": "Bunzero",
        "when": "2021-05-24T17:46:17.633000+00:00",
        "name": "MangaScaleV3",
        "hasLink": true
    },
    "2x-NMKD-YandereNeo": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** NMKD YandereNeo\n**License:** WTFPL\n**Link:** <https://icedrive.net/1/f0UAiRqz3N>\n**Model Architecture:** ESRGAN (Lite)\n**Scale:** 2/4\n**Purpose:** Upscaling of high-quality or compressed 2D art\n\n**Iterations:** 320k\n**batch_size:** 4\n**HR_size:** 192 (4x) and 96 (2x) = 48px LR\n**Dataset:** Yandere1200 (yande.re artworks)\n**Dataset_size:** 16800 (1200 unique)\n**OTF Training** Only rotation/flipping\n**Pretrained_Model_G:** 4x_DIV2K-Lite_1M (`4x_NMKD-YandereNeo-Lite_320k` for the 2x conversion)\n\n**Description:** My latest anime art upscaler, made to get rid of some problems that UltraYandere had. It should have less artifacting and less aliased edges. It only comes as a Lite model as it seems to perform very similarly to the \"big\" version, while running 2x-3x as fast.\n\n**Note:** The 2x model is a conversion of the 4x model. It was finetuned for 10k additional iterations.\n\nhttps://i.imgur.com/oxs71v5.png",
        "author": "nmkd",
        "when": "2021-01-26T14:39:45.148000+00:00",
        "name": "NMKD YandereNeo",
        "hasLink": true
    },
    "2x-pokemodel-lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\nName: 2x_pokemodel_lite_100000_G\n﻿License:﻿ WTFPL\n﻿Link:﻿ https://www115.zippyshare.com/v/O2do1VCf/file.html\nModel Architecture: ESRGAN\nScale: 2\nPurpose: Upscale old anime like pokemon\n﻿\nIterations: 100k\n﻿batch_size:﻿ 4\n﻿HR_size:﻿ 128\nEpoch: 3030\nDataset: Images from pokemon episodes\nDataset_size: 268\nPretrained_Model_G: none\nnetwork_G: nf = 32, nb = 12\n\nDescription: A 2x lite model for old pokemon episodes\nhttps://imgsli.com/Mjc3Nzk\n<https://imgsli.com/Mjc3Nzg>\n<https://imgsli.com/Mjc3ODA>\n<https://imgsli.com/Mjc3ODE>\n<https://imgsli.com/Mjc3ODI>",
        "author": "Neo-Raws",
        "when": "2020-11-01T17:34:20.536000+00:00",
        "name": "2x_pokemodel_lite_100000_G",
        "hasLink": true
    },
    "2x-SBS11-RRDB": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_SBS11-RRDB\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZ5QKLXZ5mPYmsGpGU75DLNII9AOlYQNqMjk\n**Model Architecture:** SOFVSR RRDB / SOFVESRGAN\n**Scale:** 2\n**Purpose:** Upscaling cartoon DVDs with bad haloing and noise\n\n**Iterations:** 111k (will probably be updated later)\n**batch_size:** 6\n**HR_size:** 128\n**Epoch:** 11\n**num_frames:** 3\n**Dataset:** Every 6 frames from every episode of Spongebob Season 11 (DVD and HD). DVD was resized with Spline36 to 540p. LR also got extra OTF JPEG (1/4 chance) and sharpening (15% chance)\n**Dataset_size:** 126,956\n**OTF Training** Yes\n**Pretrained_Model_G:** None\n\n**Description:** A version of 2x_SpongeBob_DVD but using SOFVSR-RRDB. Works really well on Spongebob season 2 but does fail to remove the ghosting unlike 2x_SpongeBob_DVD, unfortunately. But the shimmering seems to be a lot better due to the optical flow reconstruction. Hopefully this can be useful for other cartoon DVDs! Probably will continue training at some point, or will make a new version with adjusted loss weights.",
        "author": "Joey",
        "when": "2020-12-03T23:19:45.479000+00:00",
        "name": "2x_SBS11-RRDB",
        "hasLink": true
    },
    "2x-SHARP-ANIME-V1": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_SHARP_ANIME_V1\n**License:** CC BY-NC-SA 4.0\n**Link:** https://1fichier.com/?cnw3fwws08tdoaxycimj\n**Model Architecture:** ESRGAN\n**Scale:** 2\n**Purpose:** this model has been trained to work on lines and details\n\n**Iterations:** 300 000\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** 155\n**Dataset:** lr is from anime version dvd and HR from anime blu-ray\n**Dataset_size:** 7738 images\n**OTF Training** Yes\n**Pretrained_Model_G:** 2xPSNR.pth\n\n**Description:** works well on animes which have fairly fine lines at the base but also the video must be progressive or deinterlaced\n\nhttps://imgsli.com/MjIwMDY",
        "author": "solidd93110",
        "when": "2020-09-12T08:59:33.249000+00:00",
        "name": "2x_SHARP_ANIME_V1",
        "hasLink": true
    },
    "2x-SHARP-ANIME-V2": {
        "content": "<@&560103931204861954> <@&577839492199874570>\nName: 2x_SHARP_ANIME_V2.pth\nLicense: CC BY-NC-SA 4.0\nLink: https://1fichier.com/?020t1y83kjrusd95dh0a  (new link)\nScale: 2\nPurpose: new version of sharp_anime\nIterations: 275000\nbatch_size: 4\nHR_size: 128\nEpoch: sorry, can't remember 😦 (i deleted the train log)\nDataset: LR from severals dvd anime and HR from bluray anime\nDataset_size: 3600\nOTF Training: YES\nPretrained_Model_G: 2x_PSNR.pth\nhttps://imgsli.com/MzI0MDk",
        "author": "solidd93110",
        "when": "2020-12-09T17:26:01.876000+00:00",
        "name": "2x_SHARP_ANIME_V2.pth",
        "hasLink": true
    },
    "2x-sudo-RealESRGAN": {
        "content": "<@&577839492199874570> <@&560103931204861954> \nName: sudo_RealESRGAN2x_3.332.758_G.pth\nLicense: CC BY-NC-SA 4.0\nModel Architecutre: ESRGAN (nb=6, nf=64)\nScale: 2\n\nIterations: 3.332.758\nPretrained_Model_G: RealESRGAN_x4plus_anime_6B.pth\n\n#####\n\nName: sudo_RealESRGAN2x_Dropout_3.799.042_G.pth\nLicense: CC BY-NC-SA 4.0\n\nModel Architecutre: ESRGAN (nb=6, nf=64) + Dropout2d\nScale: 2\n\nIterations: 3.799.042\nPretrained_Model_G: RealESRGAN_x4plus_anime_6B.pth (sudo_RealESRGAN2x_3.332.758_G.pth)\n\n#####\n\nLink: https://e1.pcloud.link/publink/show?code=kZ7rGRZW2IcOpNMQeXDTTRQ4aPVBFyyJV5X\nhttps://www.mediafire.com/folder/gk7f61e2kut0z/sudo_RealESRGAN2x https://www.mediafire.com/folder/k877ci8tn1wui/sudo_RealESRGAN2x\nPurpose: General purpose model for drawings\nDescription: Tried to make the best 2x model there is for drawings. I think i archived that. And yes, it is **nearly 3.8 million iterations** (probably a record nobody will beat here), took me nearly half a year to train. It can happen that in one edge is a noisy pattern in edges. You can use padding/crop for that. I aimed for perceptual quality without zooming in like 400%. Since RealESRGAN is 4x, I downscaled these images with bicubic. I would recommend my VSGAN code though and just load the onnx. https://github.com/styler00dollar/VSGAN-tensorrt-docker I just wanted a good 2x model for animations, but that model can also be used for wallpapers and so on.\n\nBefore I hear people complaining, the dropout model is a modified architecture. cupscale or chaiNNer won't work with pth. Load the onnx with VSGAN or chaiNNer. I did add the model before switching to dropout though, which is normal ESRGAN pth, that works normally. I also converted everything into onnx, jit and ncnn. If you want to use ncnn, don't use nihuis code (that also includes cupscale), these codes don't include propper tiling in C++, which is very bad for this model. I think chaiNNer should have overlap/padding with ncnn, so use that instead if you really want ncnn.",
        "author": "sudo rm -rf / --no-preserve-root",
        "when": "2022-06-25T18:32:47.787000+00:00",
        "name": "sudo_RealESRGAN2x_3.332.758_G.pth",
        "hasLink": true
    },
    "2x-sudo-RealESRGAN-Dropout": {
        "content": "<@&577839492199874570> <@&560103931204861954> \nName: sudo_RealESRGAN2x_3.332.758_G.pth\nLicense: CC BY-NC-SA 4.0\nModel Architecutre: ESRGAN (nb=6, nf=64)\nScale: 2\n\nIterations: 3.332.758\nPretrained_Model_G: RealESRGAN_x4plus_anime_6B.pth\n\n#####\n\nName: sudo_RealESRGAN2x_Dropout_3.799.042_G.pth\nLicense: CC BY-NC-SA 4.0\n\nModel Architecutre: ESRGAN (nb=6, nf=64) + Dropout2d\nScale: 2\n\nIterations: 3.799.042\nPretrained_Model_G: RealESRGAN_x4plus_anime_6B.pth (sudo_RealESRGAN2x_3.332.758_G.pth)\n\n#####\n\nLink: https://e1.pcloud.link/publink/show?code=kZ7rGRZW2IcOpNMQeXDTTRQ4aPVBFyyJV5X\nhttps://www.mediafire.com/folder/gk7f61e2kut0z/sudo_RealESRGAN2x https://www.mediafire.com/folder/k877ci8tn1wui/sudo_RealESRGAN2x\nPurpose: General purpose model for drawings\nDescription: Tried to make the best 2x model there is for drawings. I think i archived that. And yes, it is **nearly 3.8 million iterations** (probably a record nobody will beat here), took me nearly half a year to train. It can happen that in one edge is a noisy pattern in edges. You can use padding/crop for that. I aimed for perceptual quality without zooming in like 400%. Since RealESRGAN is 4x, I downscaled these images with bicubic. I would recommend my VSGAN code though and just load the onnx. https://github.com/styler00dollar/VSGAN-tensorrt-docker I just wanted a good 2x model for animations, but that model can also be used for wallpapers and so on.\n\nBefore I hear people complaining, the dropout model is a modified architecture. cupscale or chaiNNer won't work with pth. Load the onnx with VSGAN or chaiNNer. I did add the model before switching to dropout though, which is normal ESRGAN pth, that works normally. I also converted everything into onnx, jit and ncnn. If you want to use ncnn, don't use nihuis code (that also includes cupscale), these codes don't include propper tiling in C++, which is very bad for this model. I think chaiNNer should have overlap/padding with ncnn, so use that instead if you really want ncnn.",
        "author": "sudo rm -rf / --no-preserve-root",
        "when": "2022-06-25T18:32:47.787000+00:00",
        "name": "sudo_RealESRGAN2x_3.332.758_G.pth",
        "hasLink": true
    },
    "2x-sudo-rife4-testV1": {
        "content": "<@&577839492199874570> <@&560103931204861954> \nName: sudo_rife4_269.662_testV1_scale1.pth\nLicense: CC BY-NC-SA 4.0\nModel Architecutre: Rife4\nPretrained_Model_G: RifeV4\nLink: https://e1.pcloud.link/publink/show?code=kZfoGRZNuvokO5THhVzVLOt7ocHkR9vDdF7\nhttps://www.mediafire.com/folder/ku90f7b7ggbvn/sudo_rife4_269.662_testV1_scale1 https://www.mediafire.com/folder/y4kvdjs45lefp/rvpV1\nPurpose: Animation interpolation.\nDescription: I never really mentioned it in <#579685650824036387> prior since I think not too many care about interpolation here, but I trained a rife4 model for animation a some months ago, which is better than rife4 and rife4.2 imo. Thought I should also mention it here as well. I also converted it into ncnn. (Nihuis rife ncnn models are only exported with the fastest mode and not the best quality. I exported ncnn models for the most important quality settings. Due to different export/quality settings, there are multiple models. For that reason alone, my ncnn models are much better too, since nihui only exported the fastest one.) My https://github.com/styler00dollar/VSGAN-tensorrt-docker also has the rife ncnn extention, which can use VMAF, dedup, scene detection and so on, which I would recommend. My models are in that extention as well, just select model 10, 11 or 12 and use the dev docker. \n\nThat test video is done with 2x framerate, enbemble True and FastMode False, combined with scene detection and dedup stuff, tta False. Towards the best quality rife can do.\n\nPlz don't steal without credits, k thx.",
        "author": "sudo rm -rf / --no-preserve-root",
        "when": "2022-06-25T19:57:28.309000+00:00",
        "name": "sudo_rife4_269.662_testV1_scale1.pth",
        "hasLink": true
    },
    "2x-sudo-UltraCompact": {
        "content": "<@&560103931204861954>  <@&577839492199874570> \nName: sudo_UltraCompact_2x_1.121.175_G.pth\nLicense: CC BY-NC-SA 4.0\nLink: https://e1.pcloud.link/publink/show?code=kZufYRZj1INMisvFehhfS763L4ow5YUy0VV https://www.mediafire.com/folder/o5yb9z8ue7ihn/sudo_UltraCompact_2x_1.121.175 https://www.mediafire.com/folder/7e7752gf42eky/sudo_UltraCompact_2x_1.121.175\nModel Architecutre: Compact (num_conv=8)\nScale: 2\nPurpose: Realtime animation restauration and doing stuff like deblur and compression artefact removal\n\nIterations: 1.121.175\nPretrained_Model_G: RealESRGANv2-animevideo-xsx2.pth\nTeacher: RealESRGANv2-animevideo-xsx2.pth\n\nDescription:\nMy *first attempt* to make a **REALTIME** 2x upscaling model while also applying teacher student learning. It beats Anime4k in every way. These benchmarks use a 3060ti and it shows that everything better than a 3060ti should be able to handle 1080p input if you create engine files and use my TensorRT code.  You can see in the readme how to convert onnx files into engines. The 2 right bars compare normal Compact2 and Ultracompact in speed, the 2 on the left showcase older apis I used which isn't too important for this showcase. \n\nTo use this, you need to use my code which is https://github.com/styler00dollar/VSGAN-tensorrt-docker. If you use Manjaro, it is also possible to pipe the data stream directly into mpv, so you can watch it in a video player without rendering a video. Yeah the model does seem a little noisy if you zoom in a lot, but don't forget that the model itself is only 1.2mb. I think it does quite well. I still try to improve on fast models, but this is good enough to share as a first model. Plz don't steal without credits, k thx.",
        "author": "sudo rm -rf / --no-preserve-root",
        "when": "2022-05-29T19:17:52.280000+00:00",
        "name": "sudo_UltraCompact_2x_1.121.175_G.pth",
        "hasLink": true
    },
    "2x-SwatKats": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_SwatKats_154000_G\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://mega.nz/file/vOwmULKK#nFznUd8ZY3-P0x9_63OyYE8eZifmTvZqDejIrMBXJtk>\n**Model Architecture:** ESRGAN\n**Scale:** 2\n**Purpose:** Remove vertical blur / split lines\n\n**Iterations:** 154,000\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** unknown\n**Dataset:** WEB-DL\n**Dataset_size:** 5,000+\n**OTF Training** unknown\n**Pretrained_Model_G:** 2x_ESRGAN\n\n**Description:** In addition to removing the vertical blur, the model upscales, sharpens and will remove MPEG-2 artifacting and a small amount of rainbowing and dot crawl. Another series afflicted with the vertical blur is Avatar the Last Airbender, which can be repaired by this model. The video fed into the model MUST be 540 vertical for the deblur to work properly.\n\nhttps://imgsli.com/NzM2MDY\nhttps://imgsli.com/ODIxMjU",
        "author": "SaurusX",
        "when": "2021-11-17T13:45:59.546000+00:00",
        "name": "2x_SwatKats_154000_G",
        "hasLink": true
    },
    "2x-UniScale-CartoonRestore-lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x-UniScale_CartoonRestore-lite\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZW8XhVZEPUNyXYqVn0KzR5DLjDtefPm5GAy> <https://mega.nz/folder/eBwyyKbA#CApT1_CrbpGBymLmc-FCTQ>\n**Model Architecture:** ESRGAN (lite)\n**Scale:** 2\n**Purpose:** This model has VERY strong compression removal and line restructuring that allows it to restore any heavily compressed drawings, animation, cartoons, or anime.\n\n**Iterations:** 165k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** 68\n**Dataset:** Custom Dataset consisting of lossless 4k frames from Metal Arms: Glitch in the System, Just Cause 3, Dirt 3, Forza Horizon 3, Sleeping Dogs, ATLA DVD, and self-edited photos from SignatureEdits\n**Dataset_size:** 18,909 tiles + 288 ATLA Frames\n**OTF Training** Yes (modified at various points: Linear, Bicubic, Nearest Neighbor, JPEG, BSRGAN_resize, resrgan_noise/resize, Combo_noise)\n**Pretrained_Model_G:** 4x-MMScale (Yes, 4x. mistakes were made)\n\n**Description:** This is an ESRGAN lite model, it renders frames *very* quickly and is very viable for restoring videos. Works great with animation (cartoons in particular), games, and DDS compressed textures.\n\nComparisons:\nhttps://cdn.discordapp.com/attachments/880826637543964712/882136618461458462/unknown.png\nhttps://cdn.discordapp.com/attachments/880826637543964712/882136455999262750/unknown.png\nhttps://cdn.discordapp.com/attachments/880826637543964712/882136979410673684/unknown.png\nhttps://cdn.discordapp.com/attachments/649861104645701637/882121519973666886/unknown.png\n<https://cdn.discordapp.com/attachments/649861104645701637/882124248368427078/unknown.png>\n\nVideo Example:\nhttps://cdn.discordapp.com/attachments/880826637543964712/882110147676221480/Spongebob_UniScale_CartoonRestore.mp4\nOriginal for comparison: <https://cdn.discordapp.com/attachments/547949806761410560/881971445813633044/Spongebob_Original.mp4>",
        "author": "Kim",
        "when": "2021-08-31T05:46:46.338000+00:00",
        "name": "2x-UniScale_CartoonRestore-lite",
        "hasLink": true
    },
    "2x-VHS-upscale-and-denoise-Film": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_VHS-upscale-and-denoise_Film_477000_G\n**License:** CC BY-NC-SA 4.0\n**Link:** https://drive.google.com/drive/folders/1NVT-5hmcrXc1d_DFjzNf-z-VfOa_xhwI?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 2\n**Purpose:** VHS captures of Film material, but may work on VHS recorded native SD-TV material as well. Also useable for cleaned up source material.\n\n**Iterations:** 477k\n**batch_size:** 7\n**HR_size:** 128\n**Epoch:** 2733\n**Dataset:** For LR, two decade old VHS recording of a movie from TV, captured with enabled TBC on the VCR. For HR, Blu-ray rip of that movie. \n**Dataset_size:** 1462 unique\n**OTF Training** No\n**Pretrained_Model_G:** Training was done from scratch\n\n**Description:** Upscales VHS footage and does certain degree of denoising. Might also reduce bleaching of color. Gives the image a slight sharper look and emphasizes color, thus the saturation of already colorful pictures to be upscaled must be reduced by 50%",
        "author": "Itewreed",
        "when": "2021-03-28T09:57:42.662000+00:00",
        "name": "2x_VHS-upscale-and-denoise_Film_477000_G",
        "hasLink": true
    },
    "2x-VimeoScale-Unet": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_VimeoScale_Unet\n**License:** CC-BY-SA\n**Link:** https://mega.nz/folder/qslgTZYQ#wE9l53sTH78B6wjPX5GRwQ\n**Model Architecture:** SOFVSR-RRDB (light arch, 3 frames, nf=32, nb=12)\n**Scale:** 2\n**Purpose:** Upscaling video content\n**Iterations:** 40k Pretrain + 60k Unet Training\n**batch_size:** 13\n**HR_size:** 128\n**Epoch:** N/A\n**Dataset:** Combination of Collected Vimeo scenes (CG, realistic/video, movie trailers, and motion graphics) at original quality and REDS_sharp\n**Dataset_size:** 90k+ images, 1,100 scenes/sequences\n**OTF Training** Yes\n**Pretrained_Model_G:** Self-Trained Base for the Unet finalisation with vgg_fea discrim\n**Description:** This model is meant to surpass VEAI 2x while being efficient to run quickly with fp16. The real-esrgan/BSRGAN augmentation and Unet should help with videos where the resolution is not ideal and can reconstruct details without effecting blurs in most cases. This model SHOULD run faster than real-esrgan while matching the resolving and enabling some multiframe feature extraction. No major denoising/compression/blurring effects (or artifacts) should be found.\nExamples: https://imgsli.com/NzAyMTc \nhttps://imgsli.com/NzAyMTg \nhttps://imgsli.com/NzAyMTk\npsnr/ssim results from the Vid4 dataset(combined) are in the zip vs VEAI ahq",
        "author": "Sazoji",
        "when": "2021-09-09T04:01:00.246000+00:00",
        "name": "2x_VimeoScale_Unet",
        "hasLink": true
    },
    "2x-Waifaux-NL3-SRResNet": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_Waifaux-NL3-SRResNet\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=XZPLWuXZbQz9nPrUhlytIN0ne4UjyzQRMHmX\n**Model Architecture:** EDSR (SRResNet)\n**Scale:** 2\n**Purpose:** Emulating Waifu2x at Noise Level 3\n\n**Iterations:** 200k\n**batch_size:** 8\n**HR_size:** 64\n**Epoch:** 564\n**Dataset:** Div2K + Spongebob frames, upscaled with Waifu2x (plus jpeg'd lrs)\n**Dataset_size:** A decent amount\n**OTF Training** No\n**Pretrained_Model_G:** None\n\n**Description:** Trained this one to see how training using SRResNet would go. Seems to work really well so I think that'll be the next thing I experiment with. It seems like a good alternative to lite models so I'll have to compare. **NOTE: You can't use this with regular esrgan forks or the bot, it has to be run through basicsr**",
        "author": "Joey",
        "when": "2021-02-25T06:46:14.123000+00:00",
        "name": "2x_Waifaux-NL3-SRResNet",
        "hasLink": true
    },
    "2x-Waifaux-NL3-SuperLite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x_Waifaux-NL3-SuperLite\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=XZRAmuXZRy3vkvvBdTftheNOFeJ0tLLv74eX\n**Model Architecture:** ESRGAN (NF=16, NB=6)\n**Scale:** 2\n**Purpose:** Emulating Waifu2x at Noise Level 3\n\n**Iterations:** 200k\n**batch_size:** 8\n**HR_size:** 64\n**Epoch:** 564\n**Dataset:** Div2K + Spongebob frames, upscaled with Waifu2x (plus jpeg'd lrs)\n**Dataset_size:** A decent amount\n**OTF Training** No\n**Pretrained_Model_G:** None\n\n**Description:** Trained this model to see how it would work trying to essentially get the same results as Waifu2x but with ESRGAN",
        "author": "Joey",
        "when": "2021-02-24T01:50:22.273000+00:00",
        "name": "2x_Waifaux-NL3-SuperLite",
        "hasLink": true
    },
    "3x-Video-TSSM": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 3x_Video_TSSM\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=XZMmozXZjW4hHqJu7ikbQpyeqUItTh7QJO6k>\n**Model Architecture:** SOFVSR\n**Scale:** 3\n**Purpose:** Upscaling cartoon DVD video frames\n\n**Iterations:** 70k (not much but gave up on this version of model for now)\n**batch_size:** 16\n**HR_size:** 192\n**Epoch:** N/A\n**num_frames:** 3\n**FPS:** 24\n**Dataset:** LR: The Spongebob Squarepants Movie (DVD) | HR: The Spongebob Squarepants Movie (Blu-ray). Randomly exported scenes.\n**Dataset_size:** 125,436 (Each frame was cut in 1/4)\n**OTF Training** No\n**Pretrained_Model_G:** None\n\n**Description:** First custom model release for Victorca25's modified video super resolution architecture. Works decently on scenes from TSSM but not from the show DVDs. Hopefully improvements can be made in both my dataset and in the upscaling aspect of this architecture to make the results better. I'm also training a 2x version right now so I can directly compare results with my previous ESRGAN model. Also yes you read that right, it's a 3x model.\n\nhttps://imgsli.com/Mjk5MzI\nhttps://u.pcloud.link/publink/show?code=XZLuozXZERcw9oHRinukIyDSM8IUWfIxA4DX",
        "author": "Joey",
        "when": "2020-11-17T06:12:47.741000+00:00",
        "name": "3x_Video_TSSM",
        "hasLink": true
    },
    "3x-Video-TSSM-RRDB": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 3x_Video_TSSM_RRDB\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=XZ2AERXZ9YpzzV4rRvySFiq3MsrTEYdEjtkV>\n**Model Architecture:** SOFVSR RRDB / VESRGAN ?\n**Scale:** 3\n**Purpose:** Upscaling cartoon DVD video frames\n\n**Iterations:** 80k\n**batch_size:** 6\n**HR_size:** 192\n**Epoch:** N/A\n**num_frames:** 3\n**FPS:** 24\n**Dataset:** LR: The Spongebob Squarepants Movie (DVD) | HR: The Spongebob Squarepants Movie (Blu-ray). Randomly exported scenes.\n**Dataset_size:** 125,436 (Each frame was cut in 1/4)\n**OTF Training** Yes (OTF JPEG)\n**Pretrained_Model_G:** None\n\n**Description:** New version of this model using the new RRDB (esrgan-like) SR part of SOFVSR. Much better than the last one! I may continue training it for longer but the results are good as is.\n\nhttps://u.pcloud.link/publink/show?code=XZUAERXZDsqT3rW3pYbAJ64sFYx5E86PJzUk",
        "author": "Joey",
        "when": "2020-11-25T06:10:11.803000+00:00",
        "name": "3x_Video_TSSM_RRDB",
        "hasLink": true
    },
    "4x-1ch-Alpha-Lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_1ch-Alpha-Lite\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZQfN4XZmIrkODabBWhtmmA95c0GMb19WUuy\n**Model Architecture:** ESRGAN Lite (nf=32, nb=12)\n**Scale:** 4\n**Purpose:** Alpha channels of PNGs\n\n**Iterations:** 212k\n**batch_size:** 16\n**HR_size:** 128\n**Epoch:** 682\n**Dataset:** Alpha channels from high resolution pokemon drawings and sprites, as well as 256x items from Sphax PureBDCraft. Randomly downscaled with a few different OTF filters.\n**Dataset_size:** 4,853\n**OTF Training** Yes\n**Pretrained_Model_G:** None\n\n**Description:** This is a single channel model (1 in_nc, 1 out_nc) meant to upscale just the alpha channel of PNGs. I will be adding functionality to my fork soon to be able to actually use this model properly, but if you know what you're doing you can use this now. (I think IEU has this feature as well but idk how it works or if it works with single channel models)",
        "author": "Joey",
        "when": "2020-12-15T20:16:32.691000+00:00",
        "name": "4x_1ch-Alpha-Lite",
        "hasLink": true
    },
    "4x-2C2-ESRGAN-Nomos2K": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_2C2-ESRGAN_Nomos2K_200000_G.pth\n**License:** MIT\n**Link:** https://github.com/joeyballentine/2C2-ESRGAN/tree/main/models\n**Model Architecture:** 2C2-ESRGAN\n**Scale:** 4\n**Purpose:** Real life images. Pretrained model for 2C2-ESRGAN\n\n**Iterations:** 200k\n**batch_size:** 6\n**HR_size:** 128\n**Epoch:** 16\n**Dataset:** Nomos2K\n**OTF Training** Yes\n**Pretrained_Model_G:** Technically my previous experiment was the pretrained model, but for all intents and purposes this was trained from scratch.\n\n**Description:** Pretrained model for the new architecture modification I made. You can read more about it in the github README. Basically it makes smaller ESRGAN models that theoretically can produce the same level of quality.\n\nNOTE: THIS WILL NOT WORK IN CUPSCALE, IEU, OR CHAINNER (yet)! You have to use my fork to use it for now.",
        "author": "Joey",
        "when": "2022-04-20T04:20:55.585000+00:00",
        "name": "4x_2C2-ESRGAN_Nomos2K_200000_G.pth",
        "hasLink": true
    },
    "4x-anifilm-compact": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 2x and 4x-anifilm_compact\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZ48XhVZjc7TduHV05pTSo5vfbFVD5T8mEc7> <https://mega.nz/folder/XZgSmAoa#KNKbXZVBDq4UD1NJLHm1oQ>\n**Model Architecture:** Real-ESRGAN Compact\n**Scale:** 2/4\n**Purpose:** Dragon Ball Frames\n**Iterations:** 110k for 2x, 75k for 4x \n**batch_size:** 6\n**HR_size:** 64 for 2x, 128 for 4x\n**Dataset:** Dragon Ball Z Movies\n**Dataset_size:** 2836\n**OTF Training** No\n**Pretrained_Model_G:** 2x and 4x_Compact_Pretrain.pth (<@153566544469819393>'s Pretrains)\n\n**Description:** This model is based on a private model by <@447803386142654475> named `4x_eula_anifilm_v1_225k`. He sent me a copy of the model, and I decided to train a compact model based on it with his permission. This model seems to fix the majority of the issues the original model had while being far faster, it's just a tiny bit softer in some images with a bit less capacity for heavy artifact removal.\n\nThe dataset consists of Dragon Ball movies converted to YUV24 with <@184640817565138944>'s help to reduce artifacts, then upscaled with ArtClarity and eula_anifilm. LRs are the original frames right from DVD. As a result, this model corrects some color space issues. The 2x model's HRs were downscaled by 50% with Lanczos.\n\nThe 2x and 4x models are pretty close in output despite being trained separately.\n\nThe models in the `Real-ESRGAN Compatible` folder are the original output from Real-ESRGAN's training code for compatibility reasons.\n\n2x Comparison:\nhttps://cdn.discordapp.com/attachments/903415274521374750/1004152057739087972/1659478894.4425747.png\n<https://imgsli.com/MTE5MjUz>\n\n4x Comparison:\n<https://imgsli.com/MTE5MjU0>\nhttps://cdn.discordapp.com/attachments/903415274521374750/1004158991544365097/1659480539.543144.png",
        "author": "Kim",
        "when": "2022-08-02T23:12:47.850000+00:00",
        "name": "2x and 4x-anifilm_compact",
        "hasLink": true
    },
    "4x-AnimeSharp": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-AnimeSharp (aka 4x-TextSharpV1)\n**License:** CC BY-NC-SA 4.0\n**Link:**<https://u.pcloud.link/publink/show?code=kZ74XhVZuBk47UIKgs8HkGl0yquX6FNY8RTy>  <https://mega.nz/file/CFZGHb4A#yo2NaVANd9-gk9n4lxA9zkzKPqVukZYpzendRVCinDw>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Anime or Text\n\n**Iterations:** Interpolation between 150k and 8k\n**batch_size:** 4/8\n**HR_size:** 128\n**Epoch:** Not Relevant\n**Dataset:** Custom\n**OTF Training** Yes\n**Pretrained_Model_G:** Interpolation between 4x-UltraSharp and 4x-TextSharp-v0.5\n\n**Description:** Works amazingly on anime. It also upscales text, but it's far better with anime content than text. \n\nI rebranded this model on 2/10/22 to 4x-AnimeSharp from 4x-TextSharpV1.\n\nhttps://cdn.discordapp.com/attachments/903415274521374750/925533775616696340/unknown.png\nhttps://cdn.discordapp.com/attachments/549525506585001985/941432207917084743/Clipboard-comparison.png\nhttps://cdn.discordapp.com/attachments/549525506585001985/941432638206541825/test_pic-comparison.png",
        "author": "Kim",
        "when": "2021-12-28T23:42:43.799000+00:00",
        "name": "4x-AnimeSharp (aka 4x-TextSharpV1)",
        "scale": 4
    },
    "4x-AnimeSharp-lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-AnimeSharp-lite\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZ74XhVZuBk47UIKgs8HkGl0yquX6FNY8RTy> <https://mega.nz/folder/bEoRQIRR#kEsaVHtwRL9vwfa5k2osyQ>\n**Model Architecture:** ESRGAN-lite\n**Scale:** 4\n**Purpose:** Anime\n\n**Iterations:** 98k\n**batch_size:** 4 (8 virtual)\n**HR_size:** 128\n**Epoch:** 293\n**Dataset:** Used only thisanimedoesnotexist, upscaled with 4x-AnimeSharp\n**Dataset_size:** ~2,700\n**OTF Training** Yes\n**Pretrained_Model_G:** 2x-UniScale-CartoonRestore-lite\n\n**Description:** This model is a lite version of AnimeSharp. It was trained using student-teacher learning (if i'm using the term properly), where the HRs are LRs upscaled by the full size AnimeSharp ESRGAN model, and the lite model is trained on those outputs as the HR. \n\nIt works best on clean or slightly blurry anime. **Downscale by 50% first in almost all cases**\n\nhttps://cdn.discordapp.com/attachments/547949405949657100/941776512049360986/unknown.png\nhttps://cdn.discordapp.com/attachments/547949405949657100/941770824543797268/unknown.png\nhttps://cdn.discordapp.com/attachments/903415274521374750/941779970013925506/unknown.png",
        "author": "Kim",
        "when": "2022-02-11T19:40:28.116000+00:00",
        "name": "4x-AnimeSharp-lite",
        "hasLink": true
    },
    "4x-BooruGan-600k": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: BooruGan\nLicense: WTFPL\nLink: 600k iterations: https://drive.google.com/file/d/1DRyqqy24OU6G_mwsGjy6Zaxan_Fy1LNU/view?usp=sharing\n650k iterations: https://drive.google.com/file/d/1psKp2DLWscPmDuzSTC8XqXL0LWnWuTYs/view?usp=sharing\nModel Architecture: SRGAN\nScale: 4\nPurpose: Anime artworks\n\nIterations: 650k\nbatch_size: 8\nHR_size: 128\nEpoch: 560\nDataset: I used highres artworks from gelbooru\nDataset_size: 9157 images, each are 1024x1024\nOTF Training Yes\nPretrained_Model_G: 4x_Manga109Attempt\n\nDescription: This model is designed to mainly upscale anime artworks. If you have issues with chroma then try the 600k iterations release.",
        "author": "Dell",
        "when": "2021-11-25T18:46:33.445000+00:00",
        "name": "BooruGan",
        "hasLink": true
    },
    "4x-BooruGan-650k": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: BooruGan\nLicense: WTFPL\nLink: 600k iterations: https://drive.google.com/file/d/1DRyqqy24OU6G_mwsGjy6Zaxan_Fy1LNU/view?usp=sharing\n650k iterations: https://drive.google.com/file/d/1psKp2DLWscPmDuzSTC8XqXL0LWnWuTYs/view?usp=sharing\nModel Architecture: SRGAN\nScale: 4\nPurpose: Anime artworks\n\nIterations: 650k\nbatch_size: 8\nHR_size: 128\nEpoch: 560\nDataset: I used highres artworks from gelbooru\nDataset_size: 9157 images, each are 1024x1024\nOTF Training Yes\nPretrained_Model_G: 4x_Manga109Attempt\n\nDescription: This model is designed to mainly upscale anime artworks. If you have issues with chroma then try the 600k iterations release.",
        "author": "Dell",
        "when": "2021-11-25T18:46:33.445000+00:00",
        "name": "BooruGan",
        "hasLink": true
    },
    "4x-Box": {
        "content": "<@&560103931204861954>  added 4xBox - general purpose RRDB_ESRGAN_x4 replacement for stuff that's supposed to look realistic. https://github.com/alsa64/AI-wiki/wiki/Model-Database#upscaling---realistic-photos-prerendered-3d-etc",
        "author": "Jacob_",
        "when": "2019-06-21T03:02:09.788000+00:00",
        "name": "4xBox",
        "hasLink": false
    },
    "4x-BS-Deviance": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x_BS_Deviance\n**License:** GNU GPLv3\n**Link:** https://drive.google.com/file/d/1hLhsp3aKurLys4cALQiEkYxyxGWNf1ET/view?usp=drivesdk\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Deviant-Art (Digital Drawings [Landscape and Scenery or Characters])\n\n**Iterations:** 60000\n**batch_size:** 12\n**HR_size:** 128\n**Epoch:** 134\n**Dataset:** A few drawings from DA\n**Dataset_size:** 5435 tiles\n**OTF Training** No\n**Pretrained_Model_G:** 4xESRGAN (edit: previously said 1x lmao)\n**Tensorboard Stats:** psnr: 31.249, ssim: 0.79837, lpips: 0.050361\n\n**Description:** This model upscales Digital Drawings. It was trained on random drawings found on DeviantArt. Mostly Landscape and Scenery and Illustrations of Characters.\nIt does fairly well and works on many different styles.\n\n(If using on actual DA art, make sure it's a PNG or edit the URL parameter q_75 or q_80 to q_99 or 100 since their CDN (wix) compresses JPEG and I did not account for that)",
        "author": "BlackScout",
        "when": "2020-02-25T19:37:12.277000+00:00",
        "name": "4x_BS_Deviance",
        "hasLink": true
    },
    "4x-BS-DevianceMIP": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x_BS_DevianceV2 / 4x_BS_DevianceMIP\n**License:** GNU GPLv3\n**Link:** https://drive.google.com/file/d/1OVqM2D-T5meRBCdFjsGVl0bgQZd8kZe5/view?usp=drivesdk\n**Model Architecture:** ESRGAN-FS\n**Scale:** 4\n**Purpose:** Deviant-Art (Digital Drawings [Landscape and Scenery or Characters])\n\n**Iterations:** 82000\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 1006\n**Dataset:** A few drawings from DA\n**Dataset_size:** 676 images (mipmapping technique) [hand-picked]\n**OTF Training:** No\n**Frequency Separation:** Yes\n**Pretrained_Model_G:** 4xESRGAN\n**Tensorboard Stats:** psnr: 27.683, ssim: 0.78147, lpips: 0.065944\n**IEU Compatible:** Yes\n\n**Description:** This model upscales Digital Drawings. It was trained on random drawings found on DeviantArt. Mostly Landscape and Scenery and Illustrations of Characters.\nIt does fairly well and works on many different styles.\n**V2/MIP does not suffer from the oversharpening problem of the SPSR and V1 versions.**",
        "author": "BlackScout",
        "when": "2020-07-25T22:14:38.780000+00:00",
        "name": "4x_BS_DevianceV2 / 4x_BS_DevianceMIP",
        "hasLink": true
    },
    "4x-BS-SbeveHarvey": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x_BS_S🅱️eveHarvey\n**License:** GNU GPLv3\n**Link:** https://drive.google.com/file/d/14daAyYuoX-nZ_Lo1U6oekbFqrZTQlQkG/view?usp=drivesdk\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Steve Harvey | Faces?\n\n**Iterations:** 62000\n**batch_size:** 2\n**HR_size:** 64\n**Epoch:** 8857\n**Dataset:** Steve Harvey\n**Dataset_size:** 60 images of Steve Harvey (of which repeat themselves in a funky way)\n**OTF Training** Never\n**Pretrained_Model_G:** 4xESRGAN\n**TB Stats:** psnr: 30.34, ssim: 0.80012, lpips: 0.056125\n\n**Description:** Upscale Steve Harvey, but maybe other things, somehow??\n\nSbeve.",
        "author": "BlackScout",
        "when": "2020-07-24T17:51:17.967000+00:00",
        "name": "4x_BS_S🅱️eveHarvey",
        "hasLink": true
    },
    "4x-BS-ScreenBooster-SPSR": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** BS_ScreenBooster_SPSR\n**License:** CC BY-NC-SA\n**Link:** https://drive.google.com/file/d/1JI8VZSSHpH9u7HzE8MvuNG351Dgn4jwU/view?usp=sharing\n**Model Architecture:** SPSR (BasicSR based)\n**Scale:** 4\n**Purpose:** Game Screenshots\n\n**Iterations:** 76000\n**batch_size:** 2 (SPSR requires more VRAM)\n**HR_size:** 128\n**Epoch:** 0 (given the dataset size, this isn't much of a concern imo, I might give it a few more iters later and update the link)\n**Dataset:** My own screenshots\n**Dataset_size:** 1.2M~ tiles (128x128)\n**OTF Training** No\n**Pretrained_Model_G:** Screenbooster V2\n\n**Description:** This model is designed to upscale game screenshots (3D Games) by 4 times.\nThe SPSR version is an improvement over the ESRGAN based V2.\nNote: **This is currently not compatible with IEU or ESRGAN, you will need SPSR.**\nYou can grab the testing code here, it features the tiling scripts and merging scripts by BA and Joey\nhttps://github.com/BlueAmulet/ESRGAN/tree/spsr",
        "author": "BlackScout",
        "when": "2020-07-18T16:23:39.976000+00:00",
        "name": "BS_ScreenBooster_SPSR",
        "hasLink": true
    },
    "4x-CountryRoads": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** CountryRoads\n**License:** WTFPL\n**Link:** <https://drive.google.com/drive/folders/1YCLuYo_xDErhBi9GkHSTvzPfxIDcvhZW?usp=sharing>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Streets with dense foliage in the background. Outdoor scenes.\n\n**Iterations:** 377k\n**batch_size:** 2/3\n**HR_size:** 128\n**Epoch:** 3632\n**Dataset:** Streetside photos taken on Samsung Galaxy A21 camera with all enhancements disabled using GCam and FreeDCam, downscaled to 1024\n**Dataset_size:** 236\n**OTF Training** Yes\n**Pretrained_Model_G:** 4xPSNR\n\n**Description:** First model I trained on my GPU instead of Colab. Model named after (not really) the location of the photos. \nhttps://cdn.discordapp.com/attachments/547949806761410560/880291337373618236/unknown.png\nhttps://media.discordapp.net/attachments/547949806761410560/880290211026853918/unknown.png\nhttps://media.discordapp.net/attachments/547949806761410560/880299665428447232/unknown.png\nhttps://cdn.discordapp.com/attachments/880291340301254657/880303552315146340/unknown.png",
        "author": "EzoGaming",
        "when": "2021-08-26T04:06:43.896000+00:00",
        "name": "CountryRoads",
        "hasLink": true
    },
    "4x-DeCompress": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-DeCompress and Decompress-Strong\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZL4XhVZB8qmB8uXcahRGnGRMIyC0JyaQoxy> <https://mega.nz/folder/vBgGgA5Z#qXsikSiIU8Edemaju91zDA>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** This is UniScaleV2, but intended for images with compression. Seems to work best on realistic images.\n\n**Iterations:** ~ 130k\n**batch_size:** 4\n**HR_size:** 112\n**Epoch:** 23-28\n**Dataset:** Custom Dataset consisting of lossless 4k frames from Metal Arms: Glitch in the System, Just Cause 3, Dirt 3, Forza Horizon 3, Sleeping Dogs, and self-edited photos from SignatureEdits + ATLA DVD images\n**Dataset_size:** 18,909 tiles + 288 ATLA Frames\n**OTF Training** Yes (JPEG artifacts, camera, poisson base_blur, bsrgan_resize)\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** This model set is UniScaleV2, but trained to remove image compression while bringing out details. In my tests, it comes close to DF2K_JPEG in many cases, and provides more realistic results in others. Hope you guys enjoy 🙂\n\nThe Strong model is meant for images with very strong JPEG compression. It'll work on normal or basic compressed images, but it'll lose detail.\n\nThere's an Extreme model included, but it can introduce a LOT of artifacts. Only use it if the others aren't working.\n\nSeems to work best on realistic images.\n\nComparisons:\nhttps://cdn.discordapp.com/attachments/884864597310459996/884952891721383936/unknown.png\nhttps://cdn.discordapp.com/attachments/884864597310459996/884957054807179274/unknown.png\nhttps://cdn.discordapp.com/attachments/884864597310459996/884960697883193354/unknown.png\nhttps://cdn.discordapp.com/attachments/884864597310459996/884953810097827850/unknown.png",
        "author": "Kim",
        "when": "2021-09-08T01:04:47.145000+00:00",
        "name": "4x-DeCompress and Decompress-Strong",
        "hasLink": true
    },
    "4x-DeCompress-Strong": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-DeCompress and Decompress-Strong\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZL4XhVZB8qmB8uXcahRGnGRMIyC0JyaQoxy> <https://mega.nz/folder/vBgGgA5Z#qXsikSiIU8Edemaju91zDA>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** This is UniScaleV2, but intended for images with compression. Seems to work best on realistic images.\n\n**Iterations:** ~ 130k\n**batch_size:** 4\n**HR_size:** 112\n**Epoch:** 23-28\n**Dataset:** Custom Dataset consisting of lossless 4k frames from Metal Arms: Glitch in the System, Just Cause 3, Dirt 3, Forza Horizon 3, Sleeping Dogs, and self-edited photos from SignatureEdits + ATLA DVD images\n**Dataset_size:** 18,909 tiles + 288 ATLA Frames\n**OTF Training** Yes (JPEG artifacts, camera, poisson base_blur, bsrgan_resize)\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** This model set is UniScaleV2, but trained to remove image compression while bringing out details. In my tests, it comes close to DF2K_JPEG in many cases, and provides more realistic results in others. Hope you guys enjoy 🙂\n\nThe Strong model is meant for images with very strong JPEG compression. It'll work on normal or basic compressed images, but it'll lose detail.\n\nThere's an Extreme model included, but it can introduce a LOT of artifacts. Only use it if the others aren't working.\n\nSeems to work best on realistic images.\n\nComparisons:\nhttps://cdn.discordapp.com/attachments/884864597310459996/884952891721383936/unknown.png\nhttps://cdn.discordapp.com/attachments/884864597310459996/884957054807179274/unknown.png\nhttps://cdn.discordapp.com/attachments/884864597310459996/884960697883193354/unknown.png\nhttps://cdn.discordapp.com/attachments/884864597310459996/884953810097827850/unknown.png",
        "author": "Kim",
        "when": "2021-09-08T01:04:47.145000+00:00",
        "name": "4x-DeCompress and Decompress-Strong",
        "hasLink": true
    },
    "4x-DeIndeo": {
        "content": "Name: deindeo_x4_130000_G\nLicense: WTFPL\nLink: https://drive.google.com/file/d/1-lfgZ_6vtS9DsWU-57ig8UJExH26XmCv/view?usp=sharing\nModel Architecture: ESRGAN\nScale: 4\nPurpose: Upscaling of digital art and old games video with indeo compression artifacts\n\nIterations: 130k\nbatch_size: 8\nHR_size: 128\nEpoch: ??\nDataset: own custom  dataset\nDataset_size: 3500 (no tiles)\nOTF Training: No\nPretrained_Model_G: 4xESRGAN\nValidation Stats: PSNR: 32.7, SSIM: 0.87791, LPIPS: 0.13121\n\nDescription: Another attempt to remove the indeo compression artifacts",
        "author": "Wild West Quest",
        "when": "2020-08-15T07:03:09.434000+00:00",
        "name": "deindeo_x4_130000_G",
        "hasLink": true
    },
    "4x-Deoldify": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** Deoldify\n**License:** CC0*\n**Link:** https://drive.google.com/open?id=1-mxmDF1Dh-PnQqRz_PeCrvsTkHjYCbi3\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Old black and white photo restoration.\n\n**Iterations:** 790,000\n**batch_size:** 16\n**HR_size:** 128\n**Epoch:** 0*\n**Dataset:** ADE20K(https://groups.csail.mit.edu/vision/datasets/ADE20K/), DIV2K(https://data.vision.ee.ethz.ch/cvl/DIV2K/), Library of Congress Scans (http://www.loc.gov/pictures/). I hand picked few thousand images here.\n**Dataset_size:** 2,931,607*\n**OTF Training** No\n**Pretrained_Model_G:** Falcon Fanart*\n\n**Description:** Does a good job at denoising and deblurring photos. All outputs are greyscale. This was initially a proof of concept.\n\nLines labeled with an \"*\" at the end I'm not 100% sure are correct due to how I trained this model. It was trained in 2 stages, with 2 different datasets. I listed the original Pretrained Model, the combined total for Iterations and Dataset_Size. Also, I'm like 90% sure CC0 is the license equivalent of public domain. If its not, let me know and I'll change it to whatever gives the users the most rights.",
        "author": "Rastrum",
        "when": "2019-08-30T03:46:46.529000+00:00",
        "name": "Deoldify",
        "hasLink": true
    },
    "4x-detoon": {
        "content": "<@&560103931204861954> Detoon. A toon to realistic shading style model to wiki under drawings\nDirect link:\nhttps://drive.google.com/open?id=1uJvdx3g3GEY0VxMnHb0ItBvoc6pmGvuH",
        "author": "LyonHrt",
        "when": "2019-06-24T15:49:11.302000+00:00",
        "name": "Detoon",
        "hasLink": true
    },
    "4x-detoon-alt": {
        "content": "<@&560103931204861954> Detoon. A toon to realistic shading style model to wiki under drawings\nDirect link:\nhttps://drive.google.com/open?id=1uJvdx3g3GEY0VxMnHb0ItBvoc6pmGvuH",
        "author": "LyonHrt",
        "when": "2019-06-24T15:49:11.302000+00:00",
        "name": "Detoon",
        "hasLink": true
    },
    "4x-deviantPixelHD": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** deviantPixelHD\n**License:** You're free to use this model for all non-commercial projects and interpolation, please give credit if featured in any projects\n**Link:** <https://drive.google.com/file/d/114yFJKeYCcr6st7aNNo9FJ8wDbEzLpdz/view?usp=sharing>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Similar to Manga109, can be used as a general digital upscaler as well as with pixel art.\n\n**Iterations:** 250,000\n**batch_size:** 16\n**HR_size:** 128 \n**Epoch:** 1427\n**Dataset:** High Resolution Digital Art from Deviant Art\n**Dataset_size:** 2,797\n**OTF Training:** No\n**Pretrained_Model_G:** RRDB_PSNR_x4\n\n**Description:** This model was designed to upscale original LucasArts games' backgrounds. High Definition digital art from Deviant Art was used, reducing the LR training to 32 colours so the training images looked pixelised. Can be used with other environments such as manga or digital art. Examples can be found on Raúl Sangonzalo's YouTube account (https://www.youtube.com/channel/UCwBfuiHdSPQ-zslOmiW8OHg).",
        "author": "raulsangonzalo",
        "when": "2019-09-09T14:49:05.228000+00:00",
        "name": "deviantPixelHD",
        "hasLink": true
    },
    "4x-DigiPaint": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** DigiPaint\n**License:** CC0\n**Link:** https://drive.google.com/open?id=103MX2bvd3GW0MYpC53ABU5VYY6v1t99Y\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Digital Art Upscaler\n\n**Iterations:** 35000\n**batch_size:** 6\n**HR_size:** 128\n**Epoch:** 6\n**Dataset:** Random digital art scraped from the internet. Mostly material studies but other stuff as well.\n**Dataset_size:** 48493\n**OTF Training** No\n**Pretrained_Model_G:** 4xfalcoon300(manga).pth\n\n**Description:** A digital art upscaler designed to take brush strokes into account.",
        "author": "Rastrum",
        "when": "2019-09-08T03:29:25.349000+00:00",
        "name": "DigiPaint",
        "hasLink": true
    },
    "4x-DigitalFake-2-1": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_DigitalFake-2.1\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=XZQlqjXZPNgmLRJl5HLLG0M3VRmzmzmHr5hk\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Replica of DigitalFrames 2.1 but interpolatable\n\n**Iterations:** 100k\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 841\n**Dataset:** DIV2K and spongebob frames, downscaled with box and upscaled with Digitalframes 2.1\n**Dataset_size:** 946 \n**OTF Training** No\n**Pretrained_Model_G:** RRDB_ESRGAN_x4_old_arch.pth\n\n**Description:** Wanted to see how well it would work replicating an existing model with an arbitrary upscaled dataset. The result is a model that upscales very similarly to digitalframes 2.1, yet is interpolatable with other models. I wouldn't recommend using this over the original digitalframes, but if you ever want to interpolate it with something else (that isn't celframes or other klexos models) then you can do that with this model. Anyway, this is good news for some other stuff I want to try out since this was a success 🙂",
        "author": "Joey",
        "when": "2021-02-18T08:52:19.470000+00:00",
        "name": "4x_DigitalFake-2.1",
        "hasLink": true
    },
    "4x-escale": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** escale\n**License:** WTFPL\n**Link_G:** https://drive.google.com/file/d/15VfEwR61Y1Je8EyPMWl40wW01S6bJuPJ/view?usp=share_link\n**Link_D:** https://drive.google.com/file/d/18q-4ktFNZ8tPjkszuoG6kVBhCjKl8iFa/view?usp=sharing (patchgan)\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Upscaling of visual novel art or lossless anime-adjacent artwork\n\n**Iterations:** 100000\n**batch_size:** 12\n**HR_size:** 128\n**Epoch:** 0\n**Dataset:** Various eroge dumps\n**Dataset_size:** 68120 (augmented to 613080)\n**OTF Training** No\n**Pretrained_Model_G:** 4x_muy4_035_1.pth\n\n**Description:** Third iteration of my eroge upscaling model\nhttps://slow.pics/c/fqjAhnjH",
        "author": "katoumegumi_",
        "when": "2022-12-18T02:15:43.994000+00:00",
        "name": "escale",
        "hasLink": true
    },
    "4x-eula-anifilm-v1": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x_eula_anifilm_v1_225k\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://cdn.discordapp.com/attachments/450036622940045325/1052663903064162426/4x_eula_anifilm_v1_225k.pth>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Cel animation\n\n**Iterations:** 225k\n**batch_size:** 24\n**HR_size:** 128\n**Dataset:** Dragon Ball Level Sets\n**Dataset_size:** ~51k frame pairs\n**OTF Training** No\n**Pretrained_Model_G:** 4xPSNR\n\n**Description:** Upscaling cel animation. Trained this more than a year ago, releasing cause I've got a much better v2 and v3 now.\n\nComparisons:\nhttps://slow.pics/c/UrnFYuXX\nhttps://slow.pics/c/zy1Kd841",
        "author": "end user license agreement",
        "when": "2022-12-14T19:19:57.042000+00:00",
        "name": "4x_eula_anifilm_v1_225k",
        "hasLink": true
    },
    "4x-eula-digimanga-bw-v1": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x_eula_digimanga_bw_v1_860k\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://cdn.discordapp.com/attachments/450036622940045325/936099965300768808/4x_eula_digimanga_bw_v1_860k.pth>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Black and white digital manga with halftones.\n\n**Iterations:** 860k\n**batch_size:** 4\n**HR_size:** 128\n**Dataset:** 600dpi digital manga. LR images repeated with different compression strengths, and gamma adjustments.\n**Dataset_size:** ~4000 pages\n**OTF Training** No\n**Pretrained_Model_G:** 4xPSNR\n\n**Description:** This is very specifically for manga art with screentones, resulting in a very sharp halftoned image with pure blacks and whites; if your image has color or smooth grayscale gradients, it will try to convert them to B/W halftones and won't be what you need. So this will work for the bread and butter of most mangas, just make sure to sort your color/grayscale pages out and use a different model for them. This should also reasonably work for other B/W art and comics. Won't work for raw scans unless you preprocess them; clean them as best as you can and ensure the levels are perfect. It can get pretty patchy with very degraded LR's but I can't train it to be better anymore.\n\nComparisons:\nClean examples (3 comparisons): <https://slow.pics/c/lp9O3GnZ>\nVery degraded examples (6 comparisons): <https://slow.pics/c/AxmHpF3m>\n<https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=2583686e-629e-11ec-abb7-b9a7ff2ee17c>\n<https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=52c76032-629e-11ec-abb7-b9a7ff2ee17c>\n\nhttps://cdn.discordapp.com/attachments/450036622940045325/936113306526179378/Clipboard-comparison.png\nhttps://cdn.discordapp.com/attachments/450036622940045325/936113307931250808/mimosa-comparison.png",
        "author": "end user license agreement",
        "when": "2022-01-27T04:20:56.119000+00:00",
        "name": "4x_eula_digimanga_bw_v1_860k",
        "hasLink": true
    },
    "4x-eula-digimanga-bw-v2-nc1": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x_eula_digimanga_bw_v2_nc1_307k\n**License:** CC BY-NC-SA 4.0\n**Link:** \n<https://cdn.discordapp.com/attachments/894310505622155264/1008498310597857290/4x_eula_digimanga_bw_v2_nc1_307k.pth>\n**Model Architecture:** ESRGAN (1 channel)\n**Scale:** 4 \n**Purpose:** Black and white digital manga with halftones.\n\n**Iterations:** 307k\n**batch_size:** 6\n**HR_size:** 384 \n**Dataset:** v1's dataset + real-life LRs upscaled with v1\n**Dataset_size:** 19,019\n**OTF Training** No\n**Pretrained_Model_G:** 4x_eula_digimanga_bw_v1_860k\n\n**Description:** Vast improvement over v1 in low frequency detail; moiré and artifacting reduced significantly and less random noise from JPEG artefacts in the input. Also now only works on 1 channel images, so it runs slightly faster on average and resulting images are much smaller but it might not work on some ESRGAN implementations, I personally recommend using chaiNNer. v1 may still be better in some edge cases.\nThere's also a supplementary 1x model that denoises very low quality LRs and smooths halftones so the image works better with the 4x model. Only trained it to help build the dataset and it's useless for already decent-ish LRs but may help you in some situations. <https://cdn.discordapp.com/attachments/450036622940045325/1008506998423306331/1x_eula_digimanga_bw_v3_nc1_52k.pth>\n\nComparisons (11 imgs):\n<https://slow.pics/c/Asbu0xgz>",
        "author": "end user license agreement",
        "when": "2022-08-17T12:28:11.316000+00:00",
        "name": "4x_eula_digimanga_bw_v2_nc1_307k",
        "hasLink": true
    },
    "4x-Fabric": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-Fabric (and Alt)\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZ18XhVZ5SGyo9J3qeQRCylhkBM970MNIqzV> <https://mega.nz/folder/6EJjwADL#oDvv9p0z1t9nQvObbvBRBQ>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** This model upscales fabric or cloth textures (works on cats too!)\n\n**Iterations:** 115k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** about 3350\n**Dataset:** Custom Dataset consisting of edited RAW images taken by myself. Macro shots of fabrics and various other materials.\n**Dataset_size:** 346 tiles\n**OTF Training** Yes\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** This model upscales fabric textures. The images need to be minimally compressed or passed through a decompression model first. It works with DDS compression though.\n\nThe Alt model is just an earlier iteration version. It may work better on some images. Hope you guys enjoy 🙂\n\nhttps://cdn.discordapp.com/attachments/887850596214902835/887852703873638410/unknown.png\nhttps://cdn.discordapp.com/attachments/887850596214902835/887852170156847124/unknown.png\nhttps://cdn.discordapp.com/attachments/887850596214902835/887856392269094952/unknown.png\nhttps://cdn.discordapp.com/attachments/887850596214902835/887853344545194084/unknown.png",
        "author": "Kim",
        "when": "2021-09-16T00:30:23.690000+00:00",
        "name": "4x-Fabric (and Alt)",
        "hasLink": true
    },
    "4x-Fabric-Alt": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-Fabric (and Alt)\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZ18XhVZ5SGyo9J3qeQRCylhkBM970MNIqzV> <https://mega.nz/folder/6EJjwADL#oDvv9p0z1t9nQvObbvBRBQ>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** This model upscales fabric or cloth textures (works on cats too!)\n\n**Iterations:** 115k\n**batch_size:** 4\n**HR_size:** 64\n**Epoch:** about 3350\n**Dataset:** Custom Dataset consisting of edited RAW images taken by myself. Macro shots of fabrics and various other materials.\n**Dataset_size:** 346 tiles\n**OTF Training** Yes\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** This model upscales fabric textures. The images need to be minimally compressed or passed through a decompression model first. It works with DDS compression though.\n\nThe Alt model is just an earlier iteration version. It may work better on some images. Hope you guys enjoy 🙂\n\nhttps://cdn.discordapp.com/attachments/887850596214902835/887852703873638410/unknown.png\nhttps://cdn.discordapp.com/attachments/887850596214902835/887852170156847124/unknown.png\nhttps://cdn.discordapp.com/attachments/887850596214902835/887856392269094952/unknown.png\nhttps://cdn.discordapp.com/attachments/887850596214902835/887853344545194084/unknown.png",
        "author": "Kim",
        "when": "2021-09-16T00:30:23.690000+00:00",
        "name": "4x-Fabric (and Alt)",
        "hasLink": true
    },
    "4x-Face-Ality-V1": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** Face-Ality_V1\n**License:** MPLv2\n**Link:** <https://de-next.owncube.com/index.php/s/mZRG4HB3KdP2iP6>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Faces!\n\n**Iterations:** 310k\n**batch_size:** 10\n**HR_size:** 128\n**Epoch:** 257\n**Dataset:** Lots of faces, mostly real life but some anime.\n**Dataset_size:** 13.3k tiles\n**OTF Training** No\n**Pretrained_Model_G:** 4x_Faces_04_N_180000_G.pth\n\n**Description:** Upscales small faces to big faces.",
        "author": "twittman",
        "when": "2019-11-09T18:06:14.281000+00:00",
        "name": "Face-Ality_V1",
        "hasLink": true
    },
    "4x-Faces-04-N": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Scale:** 4x\n**Purpose:** Upscale faces both pixelized and real\n**Description:** Can upscale various types of pixel art images and real life face images\n**Link:** <https://de-next.owncube.com/index.php/s/PF5GTJ9jix4Log8>\n**Iterations:** 180k\n**batch_size:** 10\n**HR_size:** 128\n**Epoch:** 237\n**Dataset_size:** 7510\n**Pretrained_Model_G:** 4x_Faces_N_250000.pth",
        "author": "twittman",
        "when": "2019-07-23T08:24:17.176000+00:00",
        "name": null,
        "hasLink": true
    },
    "4x-Fallout-Weapons": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Notice:** This model was was trained on the Fallout 4 hd weapons and armor though could be used for general weapon texture upscaling. Make sure your input images have as little compression as possible or it will get artifacts.\n**Scale:** 4x\n**Purpose:** Upscaling Fallout 3/New Vegas weapon/armor textures\n**Description:** This models was trained using Fallout 4's official hd armor/weapon textures but could be used on other weapon and armor textures.\n**Link:** <https://1drv.ms/u/s!AiWox1lAWLoTg1bUeJouJNOFZ_Jj>\n**Iterations:** 120,000\n**batch_size:** 13\n**HR_size:** 128\n**Epoch:** 2,973\n**Dataset_size:** 531\n**Pretrained_Model_G:** Manga109Attempt",
        "author": "Robert",
        "when": "2019-08-06T02:38:47.074000+00:00",
        "name": "Fallout 4 hd weapons",
        "hasLink": true
    },
    "4x-Fallout-Weapons-V2": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** Fallout Weapons V2\n**License:** No liscense\n**Link:** <https://1drv.ms/u/s!AiWox1lAWLoThANuXjxIR-hqA6os>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Video game textures, mostly metal rusty, clean or painted.\n\n**Iterations:** 180,000\n**batch_size:** 13\n**HR_size:** 128 \n**Epoch:** 1190\n**Dataset:** Fallout 4 HD DLC(Weapon,Armor/Clothes,Vehicle and Interior/Architecture textures to be exact)\n**Dataset_size:** 1,999\n**OTF Training** Yes\n**Pretrained_Model_G:** Fallout 4 Weapons\n\n**Description:** This model was trained using my previous fallout model as well as a much larger dataset.",
        "author": "Robert",
        "when": "2019-08-23T18:33:17.171000+00:00",
        "name": "Fallout Weapons V2",
        "hasLink": true
    },
    "4x-FatalimiX": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** FatalimiX\n**License:** MPLv2\n**Link:** <https://de-next.owncube.com/index.php/s/jYnFtncarkBmpcF>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Upscaling Comic and Cartoon style images\n\n**Iterations:** 260k\n**batch_size:** 10\n**HR_size:** 128px\n**Epoch:** 31\n**Dataset:** Digital Comics\n**Dataset_size:** 69k, then 79k half-way through training\n**OTF Training** No\n**Pretrained_Model_G:** 4x_Fatality_MKII_90000_G_02.pth\n\n**Description:** Make those low res comics high res again!",
        "author": "twittman",
        "when": "2019-10-29T17:38:51.756000+00:00",
        "name": "FatalimiX",
        "hasLink": true
    },
    "4x-FatePlus-lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-FatePlus-lite\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZe4XhVZ0dV5nGDnbRbsolT2PQL2xVIOIrEy> <https://mega.nz/folder/zRYh3SII#QIm6T-rzhxjBLeYF1zSDpg>\n**Model Architecture:** ESRGAN-lite (nf 32)\n**Scale:** 4\n**Purpose:** Anime PSP games, Fate Extra\n\n**Iterations:** 125k\n**batch_size:** 4/8\n**HR_size:** 128\n**Epoch:** 101\n**Dataset:** Compiled mainly by <@840750872954273852>  from various sources, including the different games released and art books by Wada Arco\n**Dataset_size:** 21k\n**OTF Training** Yes, quantization, noise, and jpeg\n**Pretrained_Model_G:** AnimeSharp-lite\n\n**Description:** This model was trained as a favor to Demon and the Fate Extra community. It leaves a nice grain on the images and upscales lines and details accurately without looking odd. This model works on most anime-style PSP games. Enjoy!\n\nIt works best on content with dithering and quantization.\n\n**NOTICE:**\nI have included both NCNN and ONNX models to make upscaling easier if you rely on either of these. \nFor NCNN, there's two versions. One is FP16 and the other is FP32. FP16 works best on RTX GPUs. Choose FP32 if in doubt about compatibility, or if FP16 doesn't work for you.\nTo use ONNX, download chaiNNer and upscale through there with the ONNX nodes.\n\n**Comparisons:**\nCG: <https://imgsli.com/MTE1MTg2>\nTamamo: <https://imgsli.com/MTE1MTg5>\nNero: <https://imgsli.com/MTE1MTg5/2/3>\nNameless: <https://imgsli.com/MTE1MTg5/4/5>\nPersona 3 Portable: https://cdn.discordapp.com/attachments/709996467019972658/992930482356494376/b3_164_0.bin_1-comparison.png",
        "author": "Kim",
        "when": "2022-07-04T00:09:34.490000+00:00",
        "name": "4x-FatePlus-lite",
        "hasLink": true
    },
    "4x-FireAlpha": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** FireAlpha\n**License:** CC BY-NC 4.0\n**Link:** https://drive.google.com/open?id=192Mw2_yUwCgqt3tAJ2sRxZ8hYHe4CKrZ\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Channels:** 4 (RGBA)\n**Purpose:** Artwork with an alpha channel\n\n**Iterations:** 1285000\n**batch_size:** Don't remember\n**HR_size:** 128\n**Epoch:** Don't remember\n**Dataset:** Fire Emblem artwork with transparency\n**Dataset_size:** 103875 tiles, 1237 images\n**OTF Training** No\n**Pretrained_Model_G:** None\n\n**Description:** An attempt to create a 4 channel model that handles transparency without splitting and upscaling separately.\n**Notice:** IEU can use the model but will attempt to split and handle tranparency itself, my fork of ESRGAN is required to use this model. <https://github.com/BlueAmulet/ESRGAN>",
        "author": "BlueAmulet",
        "when": "2019-11-12T18:17:44.081000+00:00",
        "name": "FireAlpha",
        "hasLink": true
    },
    "4x-FSDedither": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4xFSDedither\n**License:** GNU GPL3\n**Link:** https://drive.google.com/file/d/1H4KQyhcknOoExjvDdsoxAgTBMO7JuJ3w/view\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** For photos/realistic images, but worth trying on other images that have reduced colors and dithering along with fine details. Trained using the ESRGAN-FS code (https://github.com/ManuelFritsche/real-world-sr/tree/master/esrgan-fs/codes) for better details compared to plain ESRGAN.\n\n**Iterations:** 55,000\n**batch_size:** 16\n**HR_size:** 128\n**Dataset:** Div2K+Flickr2K, linearly downscaled and dithered (half Floyd-Steinberg, half ordered) using GIMP\n**Dataset_size:** 3450\n**OTF Training** No\n**Pretrained_Model_G:** RRDB_ESRGAN_x4\n\n**Description:** Trained by me to upscale Riven images (https://imgsli.com/MTI1NTY\\). This worked a lot better than multiple past attempts using custom ESRGAN models.",
        "author": "Jacob_",
        "when": "2020-01-30T03:21:40.308000+00:00",
        "name": "4xFSDedither",
        "hasLink": true
    },
    "4x-FSDedither-Manga": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n\nName: 4xFSDedither_Manga\nLicense: GNU GPLv3\nLinks: https://drive.google.com/open?id=1Vtwt9kgnmju1edibl07WI6Rg7CrAlHIx\nModel Architecture: ESRGAN\nScale: 4x\nPurpose: Cartoons/pixel art/other non-realistic stuff with dithering\n\nIterations: 120,000\nbatch_size: 16\nHR_size: 128px\nEpoch: 1263\nDataset: Manga109 copied 14 times with different dithering and scaling\nDataset_size: 1526\nOTF Training No\nPretrained_Model_G: RRDB_ESRGAN_x4.pth\n\nDescription: Trained with ESRGAN-FS frequency filters for better details. Similar to 4xFSDedither but using Manga109 instead of DF2K, making it more suitable for non-realistic stuff, also added nearest neighbor scaling so it can handle pixel art. Dithering added with FFMPEG and GIMP using the ordered and error-diffusion methods from both programs with different color palettes.\n\nhttps://imgsli.com/MTQ3Nzc",
        "author": "Jacob_",
        "when": "2020-04-21T23:30:11.883000+00:00",
        "name": "4xFSDedither_Manga",
        "hasLink": true
    },
    "4x-FSDedither-Riven": {
        "content": "<@&560103931204861954> <@&577839492199874570>\nName: 4xFSDedither_Riven\nLicense: GNU GPLv3\nLink: https://buildism.net/files/4xFSDedither_Riven.pth\nModel Architecture: ESRGAN (with Frequency Separation) | ESRGAN-FS\nScale: 4\nPurpose: Upscale photographs and 3D renders with dithering (reduced color depth).\n\nIterations: 200,000\nbatch_size: 24\nHR_size: 128\nEpoch: 260\nDataset: Photos - DIV2K+Flickr2K with different types of dithering applied\nDataset_size: 20,640 images in 2K resolution, used random tile selection\nOTF Training: No\nFrequency Separation: Yes\nPretrained_Model_G: 4xESRGAN\n\nDescription: Fine-tuned 4xFSDedither to upscale images from the game Riven, but should be better in general, particularly on ordered dithering. I adjusted the dataset to have a better variety of dithering parameters, and turned up the HFEN and pixel loss to get better details and color restoration with less noise. \n\nhttps://imgsli.com/MTg5NTM\nhttps://imgsli.com/MTg5NTQ\nhttps://imgsli.com/MTg5NTU\nhttps://i.imgur.com/j7Wtn0G.png",
        "author": "Jacob_",
        "when": "2020-07-12T02:35:27.109000+00:00",
        "name": "4xFSDedither_Riven",
        "hasLink": true
    },
    "4x-FSMangaV2": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: FSMangaV2\nLicense: CC BY-NC-SA 4.0\nLink: https://drive.google.com/file/d/1uraeNuUPx46HKZ77xUI1PCBBoH-LhikY/view?usp=sharing\nModel Architecture: ESRGAN\nScale: 4\nPurpose: Manga-style images with or without dithering - cartoons, maybe pixel art, etc\n\nIterations: 400K\nbatch_size: 16\nHR_size: 128\nEpoch: 4208\nDataset: Manga109 copied a bunch of times with different dithering methods, and both linear and nearest-neighbor scaling.\nDataset_size: 1526\nOTF Training No\nPretrained_Model_G/D: RRDB_PSNR_x4\nDescription: Same dataset as 4xFSDedither_Manga but using some of the new training options - Charbonnier pixel and feature loss, contextual loss and SWA. Should be slightly better all around. Due to the dataset it likes to add lines so it may not be suitable for all art styles. https://i.imgur.com/oVBVthF.png",
        "author": "Jacob_",
        "when": "2020-12-14T20:18:03.860000+00:00",
        "name": "FSMangaV2",
        "hasLink": true
    },
    "4x-FuzzyBox": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x_FuzzyBox\n**License:** CC BY-NC 4.0\n**Link:** https://drive.google.com/file/d/1IHp3Ibs3B8wSKZg2iGAVy4DP5bTSTOBb/view\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Photographs, Artwork, Textures, Anything really\n\n**Iterations:** 197,000\n**batch_size:** 16\n**HR_size:** 128\n**Epoch:** 316\n**Dataset:** CC0 Textures\n**Dataset_size:** 9962 (not tiled)\n**OTF Training:** No\n**Frequency Separation:** No\n**Pretrained_Model_G:** RRDB_ESRGAN_x4\n\n**Description:** Tried out a new pixel loss idea based on ensuring the HR downscaled matches the LR. Colors are pretty good as well as edges, but generated details seem slightly fuzzy hence the name.\n\nBelongs under: Photographs and Photorealism > Misc / Kitchen Sink",
        "author": "BlueAmulet",
        "when": "2020-07-22T01:53:34.635000+00:00",
        "name": "4x_FuzzyBox",
        "hasLink": true
    },
    "4x-GameAI-2-0": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x_GameAI_2.0\n**License:** WTFPL\n**Link:** \n**GameAI_2.0:** https://drive.google.com/file/d/15KhApV05F8nEqTa5ZjLE821rhe1k8VOg/view?usp=sharing\n**GameAI_1.0:** https://drive.google.com/file/d/1-s7iSK9ivJaoyvTXwDZvMjuyGG72bTQA/view?usp=sharing\n**Model Architecture:** SRGAN\n**Scale: 4**\n**Purpose:** PS2 textures, cartoonish and realistic game textures.\n\n**Iterations:** 4.25Million iterations in total but 1.04m for GameAI_2.0 and 3.21m for GameAI_1.0: \n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 367\n**Dataset:**\nI used textures from, \"(A Hat in Time, Kingdom Hearts 3 and World of Final Fantasy) for GameAI_2.0\"\n \"(Skyrim, The Witcher 3, Resident Evil 4, Final Fantasy XV and Tales of Vesperia ) for GameAI_1.0\"\nDataset_size: 21.9k to 70k textures , up to 1024x1024 per texture\n**OTF Training** No\n**Pretrained_Model_G:** 4x_GameAI_1.0\n\n**Description:** This model is intended to mainly handle PS2 compression and a mixture of Realistic and cartoonish textures, it's not meant to be used for very low resolution textures such as item icons.",
        "author": "Dell",
        "when": "2022-02-27T21:46:02.932000+00:00",
        "name": "4x_GameAI_2.0",
        "hasLink": true
    },
    "4x-HDCube": {
        "content": "<@&560103931204861954>  <@&577839492199874570> \n**Name:** 4x_HDCube\n**License:** CC0-1.0\n**Link:** https://mega.nz/folder/Pb5B1JLJ#wZ0aV39lHL0IlRjNys9FUg\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Gamecube and Wii textures.\n\n**Iterations:** 200k\n**batch_size:** 6\n**HR_size:** 128\n**Epoch:** 3600\n**Dataset:** artifact free textures and photos (stone, wood, metal, clothes, plants, characters, FX)\n**Dataset_size:** 2000+\n**Pretrained_Model_G:** NMKD Siax\n\n**Description:**\nGamecube and Wii textures (mainly DXT and 8bit color compression).\nis good for preserving fine details without affecting the original style too much.\nIt is not suitable for pixel art, small icons and text under 16 pixel.\n\nhttps://imgsli.com/MTA4MDA5\nhttps://imgsli.com/MTA3NTYy\nhttps://imgsli.com/MTA3NTY0\nhttps://slow.pics/c/rmQsQ6yb",
        "author": "Venomalia",
        "when": "2022-05-12T16:15:49.310000+00:00",
        "name": "4x_HDCube",
        "hasLink": false
    },
    "4x-HellinaCel": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_HellinaCel\n**License:** fuckin do whatever\n**Link:** https://mega.nz/file/GFFkSL5R#C-dDxaxTZpSPUh_heAUw_idGIF8Xizdr0ktyGi05yFk\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Upscaling cartoon images\n\n**Iterations:** 230000\n**batch_size:** 69\n**HR_size:** 128\n**Epoch:** 140000 or so\n**Dataset:** Trained on upscaling downscaled cel photographs run through DetoriationFrames\n**Dataset_size:** 69\n**OTF Training** No\n**Pretrained_Model_G:** 4xESRGAN\n\nA rougher alternative to 4xCelFrames with a focus on realistic looking cels over nice looking cels\nIt's trained on DetoriationFrames LRs, so if you give it an image straight from the source it will go twice as hard on it.\nThis can be used to your advantage, though. I recommend cleaning it up before running DetoriationFrames, or else it will come out rough.\nhttps://imgsli.com/NDU2MjU",
        "author": "Flowey the Flower",
        "when": "2021-03-21T21:06:37.119000+00:00",
        "name": "4x_HellinaCel",
        "hasLink": true
    },
    "4x-Jaypeg90": {
        "content": "<@&560103931204861954> <@&577839492199874570>\nName: 4xJaypeg90\nLicense: GNU GPLv3\nLink: https://drive.google.com/file/d/17k72r01xTkeOzqsaYCN6ZqnAktKqTLyw/view?usp=sharing\nModel Architecture: ESRGAN\nScale: 4\nPurpose: Photos/realistic 3D with JPEG compression, quality 85-95 and 4:2:0 chroma subsampling.\n\nIterations: 66000\nbatch_size: 24\nHR_size: 128\nEpoch: 229\nDataset: Flickr2K+Div2K, JPEG compressed with random quality from 85-95 and scaled with either cubic or NN scaling.\nDataset_size: 3440*2\nOTF Training: No\nFrequency Separation: Yes\nPretrained_Model_G: 4xESRGAN\n\nDescription: Created for Myst3 images, since all have 4:2:0 chroma subsampling and existing JPEG models did not give good results. Favors smoothing over over-sharpening. https://i.imgur.com/n09m2Rt.png",
        "author": "Jacob_",
        "when": "2020-07-20T17:58:13.994000+00:00",
        "name": "4xJaypeg90",
        "hasLink": true
    },
    "4x-KCJPUNK-1": {
        "content": "**Name:** 4X_KCJPUNK_1.0_233089_ G.pth\n**License:**CC BY-NC-SA 4.0\n**Link:** https://mega.nz/folder/QsUWWJjb#SIuqcPEzrkpLW0WNSth66g \n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Up-scaling Digital Animation \n**Iterations:** 233089\n**Batch_size:** 4\n**HR_size:** 128\n**Epoch:** 274\n**Datasets:** 1080P BD anime frames\n**Datasets_size:** 20000\n**OTF Training:** No\n**Pre-trained_Model_G:** 4x_fatal_Anime_500000_G.pth\n\n**Description:** \nThis was my first model so I went crazy with it. I used MagickUtils convert PNG LR images to JPEG LR images then \nchange the extension name using bulk rename so that I can get several quality version as LR images( 10%,30%,50%,60%,70%,100%). \nThis is my first train Model I'm planning to make more models in the future \n\n**Special Thanks for:**\nSpecial Credits goes to <@!608006945940701273> for providing wonderful BasicSR fork for free and help me out with training  also thanks for <@!299202661700337665>,<@!66238401078112256>,<@456226577798135808> providing more information for training and <@!193721819394605056> for providing cupscale,MagickUtils GUI for free personal use, and lastly for creator of the 4x_fatal_Anime_500000_G.pth for providing the model free as a pre-trained model for this non-commercial project.",
        "author": "Kcjpunk",
        "when": "2021-03-23T08:42:59.187000+00:00",
        "name": "4X_KCJPUNK_1.0_233089_ G.pth",
        "hasLink": true
    },
    "4x-LADDIER1": {
        "content": "Name: LADDIER1\nLicense: Unknown\nLink: https://drive.google.com/file/d/1GoUY7t5kE0Ubi-NDvlMB8lUs-_u_f6tS/view?usp=sharing\nModel Architecture: ESRGAN\nScale: 4\nPurpose: Remove noise, grain, box blur, lens blur and gaussian blur and increase overall image quality. \n\nIterations: 282k\nbatch_size: 8\nHR_size: 128\nEpoch: Unkown\nDataset: Mostly images of Nirvana some of other images\nDataset_size: 12.7k tiles\nOTF Training Unknown\nPretrained_Model_G: RRDB_ESRGAN_x4.pth\n\nDescription: Upscales images with noise, grain and different types of blur. \n\nhttps://drive.google.com/file/d/1GoUY7t5kE0Ubi-NDvlMB8lUs-_u_f6tS/view?usp=sharing",
        "author": "Alexander Syring",
        "when": "2019-11-13T23:59:29.696000+00:00",
        "name": "LADDIER1",
        "hasLink": true
    },
    "4x-Link": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_Link\n**License:** GNU GPL3\n**Link:** https://drive.google.com/drive/u/0/folders/1zM07-eMja0PISoUz0dWGqx8b_YspU-gl\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Chainmail game textures. Alternatively, it can be used to turn plain images into chainmail.\n\n**Iterations:** 50000\n**batch_size:** 8\n**HR_size:** 64\n**Epoch:** 7813\n**Dataset:** pexels.com, ambientcg.com, Wikimedia Commons. photos of chainmail from DeviantArt, game textures with chainmail, diffuse maps of chainmail from DAZ items. All items downscaled by factors of 2 to add capacity for LOD texture upscaling and increase generative effect of model.\n**Dataset_size:** 561\n**OTF Training** Yes\n**Pretrained_Model_G:** 4x_Nickelfront_14000G.pth\n\n**Description:** A highly generative model for chainmail game textures. Works fairly well on items without visible chainmail texture (ie just a grey area) or items with clear chainmail texture, less well on items with present but poorly defined chainmail texture. I'm not 100% happy with it but it is still an improvement over existing models in enough situations to be worth releasing.",
        "author": "Fielran",
        "when": "2022-10-06T20:54:56.505000+00:00",
        "name": "4x_Link",
        "hasLink": true
    },
    "4x-Lollypop": {
        "content": "<@&560103931204861954> \n**Name:** 4xLollypop\n**License:** CC BY-NC-SA 4.0\n**Link:** https://drive.google.com/u/1/uc?id=10h8YXKKOQ61ANnwLjjHqXJdn4SbBuUku&export=download\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** General plus dedithering\n**Iterations:** 235k\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 16\n**Dataset:** prerendered manga images\n**Dataset_size:** 109,564\n**OTF Training:** Yes\n**Pretrained_Model_G:** 4X_esrgan.pth\n\n**Description: **A universal model, that is aimed at prerendered images, but handles realistic faces, manga, pixel art and dedithering as well. Trained using the patchgan discriminator, with cx loss, cutmixup and frequency separation, it produces good results with a slight grain due to patchgan, with some sharpening using cutmixup.",
        "author": "LyonHrt",
        "when": "2020-09-11T16:11:27.317000+00:00",
        "name": "4xLollypop",
        "hasLink": true
    },
    "4x-Loyaldk-Giroro": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: Platoon Model Set\nLicense: CC BY-NC-SA 4.0\nLink: <https://mega.nz/folder/5UAlQCRJ#2Bj1lq8PYeB3P9k5UKSvkQ>\nModel Architecture: ESRGAN (NF=32,48,64 NB=12,18,24)\nScale: 1, 2, 4\nPurpose: Upscale Anime while keeping as much of the original detail as possible. Isn't among the sharpest models and if the anime is somewhat blurry it will retain that detail but add more pixels to give a smoother edge. Doesn't change the color very much and may make some scene's very slightly darker.\nIterations: 500K + 150K\nbatch_size: 16,8\nHR_size: 128, 192, 320\nEpoch: 17-34\nDataset: Saga of Tanya The Evil, Gravity Falls, Lucky Star, One Punch Man, Redline, Soul Eater, Spirited Away, Your Lie in April and Your Name. Converted Episodes and Movies to 4:4:4 Chroma Subsampling using custom 1x model included. LR's Include H264 Compression, H265 Compression, 1x_DotCrawl_125000_G, 1x_DeteriorationFrames V1_102000_G, Gaussian Blur, Poisson Noise, Uniform Noise, JPEG Compression, 4:2:0 Chroma Subsampling, Dithering.\nDataset_size: 70,263 PNG's\nOTF Training: No\nPretrained_Model_G: Pony Models\n<https://imgsli.com/NTcxMjE>, <https://imgsli.com/NTcxMjI>, <https://imgsli.com/NTcxMjQ>, <https://imgsli.com/NTcxMjU>, <https://imgsli.com/NTcxMjY>, <https://imgsli.com/NTcxMjg>, <https://imgsli.com/NTcxMjk>, <https://imgsli.com/NTcxMzA>, <https://imgsli.com/NTcxMzE>",
        "author": "ChaseMMD",
        "when": "2021-06-07T19:17:25.846000+00:00",
        "name": "Platoon Model Set",
        "hasLink": true
    },
    "4x-Loyaldk-Keroro": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: Platoon Model Set\nLicense: CC BY-NC-SA 4.0\nLink: <https://mega.nz/folder/5UAlQCRJ#2Bj1lq8PYeB3P9k5UKSvkQ>\nModel Architecture: ESRGAN (NF=32,48,64 NB=12,18,24)\nScale: 1, 2, 4\nPurpose: Upscale Anime while keeping as much of the original detail as possible. Isn't among the sharpest models and if the anime is somewhat blurry it will retain that detail but add more pixels to give a smoother edge. Doesn't change the color very much and may make some scene's very slightly darker.\nIterations: 500K + 150K\nbatch_size: 16,8\nHR_size: 128, 192, 320\nEpoch: 17-34\nDataset: Saga of Tanya The Evil, Gravity Falls, Lucky Star, One Punch Man, Redline, Soul Eater, Spirited Away, Your Lie in April and Your Name. Converted Episodes and Movies to 4:4:4 Chroma Subsampling using custom 1x model included. LR's Include H264 Compression, H265 Compression, 1x_DotCrawl_125000_G, 1x_DeteriorationFrames V1_102000_G, Gaussian Blur, Poisson Noise, Uniform Noise, JPEG Compression, 4:2:0 Chroma Subsampling, Dithering.\nDataset_size: 70,263 PNG's\nOTF Training: No\nPretrained_Model_G: Pony Models\n<https://imgsli.com/NTcxMjE>, <https://imgsli.com/NTcxMjI>, <https://imgsli.com/NTcxMjQ>, <https://imgsli.com/NTcxMjU>, <https://imgsli.com/NTcxMjY>, <https://imgsli.com/NTcxMjg>, <https://imgsli.com/NTcxMjk>, <https://imgsli.com/NTcxMzA>, <https://imgsli.com/NTcxMzE>",
        "author": "ChaseMMD",
        "when": "2021-06-07T19:17:25.846000+00:00",
        "name": "Platoon Model Set",
        "hasLink": true
    },
    "4x-Loyaldk-Kororo": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: Platoon Model Set\nLicense: CC BY-NC-SA 4.0\nLink: <https://mega.nz/folder/5UAlQCRJ#2Bj1lq8PYeB3P9k5UKSvkQ>\nModel Architecture: ESRGAN (NF=32,48,64 NB=12,18,24)\nScale: 1, 2, 4\nPurpose: Upscale Anime while keeping as much of the original detail as possible. Isn't among the sharpest models and if the anime is somewhat blurry it will retain that detail but add more pixels to give a smoother edge. Doesn't change the color very much and may make some scene's very slightly darker.\nIterations: 500K + 150K\nbatch_size: 16,8\nHR_size: 128, 192, 320\nEpoch: 17-34\nDataset: Saga of Tanya The Evil, Gravity Falls, Lucky Star, One Punch Man, Redline, Soul Eater, Spirited Away, Your Lie in April and Your Name. Converted Episodes and Movies to 4:4:4 Chroma Subsampling using custom 1x model included. LR's Include H264 Compression, H265 Compression, 1x_DotCrawl_125000_G, 1x_DeteriorationFrames V1_102000_G, Gaussian Blur, Poisson Noise, Uniform Noise, JPEG Compression, 4:2:0 Chroma Subsampling, Dithering.\nDataset_size: 70,263 PNG's\nOTF Training: No\nPretrained_Model_G: Pony Models\n<https://imgsli.com/NTcxMjE>, <https://imgsli.com/NTcxMjI>, <https://imgsli.com/NTcxMjQ>, <https://imgsli.com/NTcxMjU>, <https://imgsli.com/NTcxMjY>, <https://imgsli.com/NTcxMjg>, <https://imgsli.com/NTcxMjk>, <https://imgsli.com/NTcxMzA>, <https://imgsli.com/NTcxMzE>",
        "author": "ChaseMMD",
        "when": "2021-06-07T19:17:25.846000+00:00",
        "name": "Platoon Model Set",
        "hasLink": true
    },
    "4x-Loyaldk-LitePonyV2-0": {
        "content": "Name: 4x_Loyaldk-LitePony_500000_V2.0\nLicense: CC BY-NC-SA 4.0\nLink: https://mega.nz/file/hcoViIQQ#O1cmQ8FsLk5RAIP5lxB3Q2L0suh9IWX_dGHhuSuL9U4\nModel Architecture: ESRGAN (NF=32, NB=12)\nScale: 4\nPurpose: Upscale MLP episodes. Liter version. Able to handle compression better compared to V1.0 and no longer creates halo's/rainbows. Good for Vector 2D art however converts detail to blobs. Quality seems weak with this one. However posting anyways to see if a use is found.\nIterations: 500K\nbatch_size: 12\nHR_size: 240\nEpoch: 46\nDataset: All MLP: FiM Episodes using scene detection frame grabbing as well as selected Prores4444 content https://shimmermare.com/4k-pony-project. Converted Episodes and Movies to 4:4:4 Chroma Subsampling using custom 1x model. LR's Include H264 Compression, H265 Compression, 1x_DotCrawl_125000_G, 1x_DeteriorationFrames V1_102000_G, Gaussian Blur, LitePony Chroma Bleed, Uniform Noise, JPEG Compression, 4:2:0 Chroma Subsampling.\nDataset_size: 132,120 PNG's\nOTF Training: No\nPretrained_Model_G: None",
        "author": "ChaseMMD",
        "when": "2021-04-12T23:07:34.743000+00:00",
        "name": "4x_Loyaldk-LitePony_500000_V2.0",
        "hasLink": true
    },
    "4x-Loyaldk-MediumPonyV2-0": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: 4x_Loyaldk-MediumPony_500000_V2.0\nLicense: CC BY-NC-SA 4.0\nLink: https://mega.nz/file/lYo01DRI#HIiJ7M5YaXjwAb3zEbtSLzAI59iljwEX72Ad88CvUf0\nModel Architecture: ESRGAN (NF=48, NB=18)\nScale: 4\nPurpose: Upscale MLP episodes. Medium version. Able to handle compression better compared to V1.0 and no longer creates halo's/rainbows. Good for Vector 2D art however converts detail to blobs.\nIterations: 500K\nbatch_size: 8\nHR_size: 240\nEpoch: 66\nDataset: All MLP: FiM Episodes using scene detection frame grabbing as well as selected Prores4444 content https://shimmermare.com/4k-pony-project. Converted Episodes and Movies to 4:4:4 Chroma Subsampling using custom 1x model. LR's Include H264 Compression, H265 Compression, 1x_DotCrawl_125000_G, 1x_DeteriorationFrames V1_102000_G, Gaussian Blur, LitePony Chroma Bleed, Uniform Noise, JPEG Compression, 4:2:0 Chroma Subsampling.\nDataset_size: 132,120 PNG's\nOTF Training: No\nPretrained_Model_G: None",
        "author": "ChaseMMD",
        "when": "2021-04-30T15:06:04.725000+00:00",
        "name": "4x_Loyaldk-MediumPony_500000_V2.0",
        "hasLink": true
    },
    "4x-Loyaldk-SuperPonyV2-0": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: 4x_Loyaldk-SuperPony_500000_V2.0\nLicense: CC BY-NC-SA 4.0\nLink: https://mega.nz/file/BMhUCbJZ#kYz0wvc26RW0n0f8AUXekW_ZCuRamKmh2-iWf1eefn0\nModel Architecture: ESRGAN (NF=64, NB=24)\nScale: 4\nPurpose: Upscale MLP episodes. Full version. Able to handle compression better compared to V1.0 and no longer creates halo's/rainbows. Good for Vector 2D art however converts detail to blobs.\nIterations: 500K\nbatch_size: 8\nHR_size: 240\nEpoch: 66\nDataset: All MLP: FiM Episodes using scene detection frame grabbing as well as selected Prores4444 content https://shimmermare.com/4k-pony-project. Converted Episodes and Movies to 4:4:4 Chroma Subsampling using custom 1x model. LR's Include H264 Compression, H265 Compression, 1x_DotCrawl_125000_G, 1x_DeteriorationFrames V1_102000_G, Gaussian Blur, LitePony Chroma Bleed, Uniform Noise, JPEG Compression, 4:2:0 Chroma Subsampling.\nDataset_size: 132,120 PNG's\nOTF Training: No\nPretrained_Model_G: None",
        "author": "ChaseMMD",
        "when": "2021-04-30T14:52:59+00:00",
        "name": "4x_Loyaldk-SuperPony_500000_V2.0",
        "hasLink": true
    },
    "4x-MeguUp": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** MeguUp\n**License:** GNU GPLv3.\n**Link:** <https://gitlab.com/KatouMegumi_osu/esrgan_4x_meguup/-/raw/master/release/4x_MeguUp_105000_G.pth>\n**Link 2 (newest)** <https://s.katou.pw/4x_MeguUp_150000_G.pth>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Upscaling of lossless (uncompressed) anime art.\n\n**Iterations:** 105k\n**Config:** <https://gitlab.com/KatouMegumi_osu/esrgan_4x_meguup/-/blob/master/train2.json>\n**Log:** <https://gitlab.com/KatouMegumi_osu/esrgan_4x_meguup/-/blob/master/train_200911-045703.log> + others\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 199\n**Dataset:** SFW and NSFW anime artworks\n**Dataset_size:** 4193 \n**OTF Training** Yes\n**Pretrained_Model_G:** Interpolated custom, hence the license. \n**Interpolated Pretrain Credits:** SYNLA: LyonHrt (bloc97 dataset, MIT license). Fatal Pixels, Fatality Anime: twittman (CC BY-NC-SA 4.0, acquired special permission to relicense. <https://s.katou.pw/Discord_dPUiI1x3Pf.png>). Deviance: BlackScout (GNU GPLv3)\n\n**Description:** 80% of the dataset is plain hentai lol, still works pretty will with general anime stuff.\nTrained on \"GTX\" 745, which takes about 12-13 minutes for 200 iterations. Might train more and update later. (Old copies will remain on git)\nThe model seems a bit blurry (in some places), but honestly I quite like the effect.\n\nEdit 2: I had gone over the 10gb limit for gitlab\n<https://imgsli.com/MjIyNzk>",
        "author": "katoumegumi_",
        "when": "2020-09-16T06:23:44.576000+00:00",
        "name": "MeguUp",
        "hasLink": true
    },
    "4x-MinecraftAlphaUpscaler-with-Good-data": {
        "content": "<@&560103931204861954> <@&577839492199874570> \nName: MinecraftAlphaUpscaler with Good data\nLicense: GPLv3 - Please share what you change about it so we can all benefit.\nLink: https://drive.google.com/drive/folders/1XsOFqGx_cGMWQTBuE9K9smwULg_wiaWc?usp=sharing\nModel Architecture: ESRGAN\nScale: 4\nPurpose: This model was trained on data that was collected by a small community to try and upscale a single image, pack.png\nIterations: 50500\nbatch_size: 132\nHR_size: Probably 128\nEpoch: 50\nDataset: https://drive.google.com/open?id=1udZWK-NQVHyzm7wpjcw0JwB2z9s7plmu\nDataset_size: 90,000\nPretrained_Model_G: RRDB_PNSR_x4.pth\n\nDescription: This model was trained on 90,000 images from Minecraft Alpha 1.2.2 taken from a small community. The dataset was cleaned up and trained on 4 T4's. This is a *slightly* better version than the one above, and I included some different iterations in the folder(10k, 20k 30k and the final one)",
        "author": "Washed Up",
        "when": "2020-01-30T00:47:24.110000+00:00",
        "name": "MinecraftAlphaUpscaler with Good data",
        "hasLink": true
    },
    "4x-Minepack": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** Minepack\n**License:** CC BY-NC-SA\n**Link:** https://drive.google.com/file/d/1DbbM4saRpJu0dmgvccQFwfF4zI2rjiQ_\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Minecraft screenshots\n**Iterations:** 68000\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 10\n**Dataset:** Custom -> `https://drive.google.com/file/d/1N5BQgLJ59zNRwiPbf4nRsLLBT2WoVwaN/view` + `https://mega.nz/#!lpMHRAqS!bHN9Ioj7l2Gfvm1HOxTR8rqcGNrrx3Ai-1-bj7YejuU` (crowdsourced, posted in #post-your-screenshot-zips-here in SalC1's Discord Server)\n**Dataset_size:** 53390 images\n**OTF Training** Yes\n**Pretrained_Model_G:** 4xESRGAN.pth\n\n**Description:** \nUpscales Minecraft screenshots by 4x. May suffer from haloing, weird patterns on blocks and JPEG-like artifacts. \nIt can't handle screenshots that have been downscaled (exception for NN Scaling) and probably images with any sort of AA/Mipmapping but I am not sure about the latter",
        "author": "BlackScout",
        "when": "2020-01-27T05:09:33.938000+00:00",
        "name": "Minepack",
        "hasLink": true
    },
    "4x-Misc": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** escale\n**License:** WTFPL\n**Link_G:** https://drive.google.com/file/d/15VfEwR61Y1Je8EyPMWl40wW01S6bJuPJ/view?usp=share_link\n**Link_D:** https://drive.google.com/file/d/18q-4ktFNZ8tPjkszuoG6kVBhCjKl8iFa/view?usp=sharing (patchgan)\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Upscaling of visual novel art or lossless anime-adjacent artwork\n\n**Iterations:** 100000\n**batch_size:** 12\n**HR_size:** 128\n**Epoch:** 0\n**Dataset:** Various eroge dumps\n**Dataset_size:** 68120 (augmented to 613080)\n**OTF Training** No\n**Pretrained_Model_G:** 4x_muy4_035_1.pth\n\n**Description:** Third iteration of my eroge upscaling model\nhttps://slow.pics/c/fqjAhnjH",
        "author": "katoumegumi_",
        "when": "2022-12-18T02:15:43.994000+00:00",
        "name": "escale",
        "hasLink": false
    },
    "4x-muy4-035-1": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_muy4_035_1.pth\n**License:** WTFPL\n**Link:** <https://s.katou.pw/4x_muy4_035_1.pth>\n**Model Architecture:** ESRGAN \n**Scale:** 4\n**Purpose:** Upscaling of anime art (Specifically visual novel CG art)\n\n**Description:** Interpolated model between <@!193721819394605056>'s Yandere4 and my failed anime art upscaling model. I find it works better than most anime models, but that was during early 2020. Dunno if it would be able to compete with current models. \nhttps://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=278999ec-e802-11eb-abb7-b9a7ff2ee17c (Hakurei Reimu from Touhou Project illustrated by \"okawa friend\") \nhttps://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=f3d4781a-e801-11eb-abb7-b9a7ff2ee17c (Kitaooji Karen from Making \\* Lovers developed by SMEE.)",
        "author": "katoumegumi_",
        "when": "2021-07-18T19:31:57.006000+00:00",
        "name": "4x_muy4_035_1.pth",
        "hasLink": true
    },
    "4x-Nickelback": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x_Nickelback\n**License:** GNU GPLv3\n**Link:** https://drive.google.com/file/d/1d89zvzC6BKf5EKapBtc4MH2DX5GClKzl/view\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Upscale photographs. Alternative to 4xBox and 4xESRGAN\n\n**Iterations:** 70000\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 11\n**Dataset:** Wallpapers\n**Dataset_size:** 44550 tiles of 128x128\n**OTF Training** No\n**Pretrained_Model_G:** 4xESRGAN\n**TensorBoard Stats:** psnr: 29.368, ssim: 0.71595, lpips: 0.093889\n\n**Description:** Why Nickelback? Because you **look at this photograph** that you want to upscale and you wonder what model you should use. This goes straight to the point, it's the first thing you remember. /not\n\nJokes aside, this model aims to improve further on what has been achieved by the regular 4xESRGAN and also 4xBox. \nIt can upscale most pictures/photos (granted they are clean enough) without destroying as much detail as the aforementioned models.\n\nIt generates less moiré like patterns and keeps details without oversharpening or blurring the image too much.\nIt's also much more faithful to the LR counterpart, so keep that in mind.",
        "author": "BlackScout",
        "when": "2020-02-13T00:43:47.323000+00:00",
        "name": "4x_Nickelback",
        "hasLink": true
    },
    "4x-NickelbackFS": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x_NickelbackFS\n**License:** GNU GPLv3\n**Link:** https://drive.google.com/file/d/1GL0OjGPsMwSmCr_FmkqtRjZu2IM0s019/view\n**Model Architecture:** ESRGAN (with Frequency Separation) | ESRGAN-FS\n**Scale:** 4\n**Purpose:** Upscale photographs. Alternative to 4xBox and 4xESRGAN\n\n**Iterations:** 72000\n**batch_size:** 12\n**HR_size:** 128\n**Epoch:** 12\n**Dataset:** Wallpapers\n**Dataset_size:** 65434 tiles of 128x128\n**OTF Training:** No\n**Frequency Separation:** Yes\n**Pretrained_Model_G:** 4xESRGAN\n**TensorBoard Stats:** psnr: 30.285, ssim: 0.73746, lpips: 0.066114\n\n**Description:** This model aims to improve further on what has been achieved by the old Nickelback which was an improvement attempt over 4xESRGAN and also 4xBox. \nIt can upscale most pictures/photos (granted they are clean enough) without destroying as much detail as Box and basic ESRGAN.\n\nIt generates less moiré like patterns and keeps details without oversharpening or blurring the image too much.\nIt's also much more faithful to the LR counterpart, so keep that in mind.\n\nPS: FS seems to deal better with the type of dataset I have given to this. So, even Nearest Neighbor scaled pictures can look good.",
        "author": "BlackScout",
        "when": "2020-07-11T04:50:17.854000+00:00",
        "name": "4x_NickelbackFS",
        "hasLink": true
    },
    "4x-Nickelfront": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x_Nickelfront (<:GWlulurdMegaLul:402868018721456128> <:GWsetmyxPeepoWeird:405337568155009024> )\n**License:** GNU GPLv3\n**Link:** https://drive.google.com/file/d/1zC97RLUg5ejqb5-4Dj5q-SzdLPLiMozH/view\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Coins, Nickels, Face front pictures of metallic objects\n\n**Iterations:** 14000\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 105\n**Dataset:** Coin images\n**Dataset_size:** 1176 tiles, 90% for training, 10% for validation\n**OTF Training** No\n**Pretrained_Model_G:** 4xESRGAN\n**Tensorboard Stats:** psnr: 22.677, ssim: 0.60558, lpips: 0.37507\n\n**Description:** Upscale coins. That's it. If you were mad at me because Nickelback doesn't make any sense. Now you have the perfect solution to your problems. If you want to upscale nickels or anything with similar texture made out of metal, now you can.\nIt works pretty well for a joke. 👛",
        "author": "BlackScout",
        "when": "2020-02-13T09:14:47.246000+00:00",
        "name": "4x_Nickelfront (<:GWlulurdMegaLul:402868018721456128> <:GWsetmyxPeepoWeird:405337568155009024> )",
        "hasLink": true
    },
    "4x-NMKD-PatchySharp": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x NMKD-PatchySharp\n**License:** WTFPL\n**Link:** <https://u.pcloud.link/publink/show?code=kZcyv0XZOefT8dS3Vu0fsPXV6U9ELQL2CO6X>\n**Model Architecture:** ESRGAN (Old Arch)\n**Scale:** 4\n**Purpose:** Upscaler for clean images or images with compression artifacts (jpeg quality >75)\n\n**Iterations:** 100k (At the time of writing - initial release)\n**batch_size:** 5\n**HR_size:** 256\n**Epoch:** 276 (@ 100k)\n**Dataset:** DIV2K Clean + JPEG\n**Dataset_size:** 900+900\n**OTF Training** Rotation, Flipping, Cutmixup\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** A multi-purpose upscaler based on PatchGAN and Cutmixup, trained on both clean image pairs in JPEGs.\nProduces very sharp lines/edges due to NN-Filtered HR images.\nProven to produce very, very good results on drawings (sharp lines) and CGI, but should also work pretty well for real-world images.\n\nhttps://i.imgur.com/PzFkmAg.png",
        "author": "nmkd",
        "when": "2020-09-12T12:47:59.916000+00:00",
        "name": "4x NMKD-PatchySharp",
        "hasLink": false
    },
    "4x-NMKD-Siax-CX": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** NMKD Siax (\"CX\")\n**License:** WTFPL\n**Link:** https://u.pcloud.link/publink/show?code=kZ6LXzXZICHWHeIFh7kS4D5CdyB0h5ShcGVk\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Universal upscaler for clean and slightly compressed images (JPEG quality 75 or better)\n\n**Iterations:** 175k **(To be updated to 200k+)**\n**batch_size:** 2\n**HR_size:** 256 (= 64px LR)\n**Dataset:** DIV2K Train+Valid\n**Dataset_size:** 2700 (900 unique)\n**OTF Training** Only rotation/flipping\n**Pretrained_Model_G:** 4xPSNR\n\n**Description:** One of my best models yet - Siax relies heavily on Contextual Loss in combination with PatchGAN to improve the details.\nIt produces perfect results on uncompressed images and even handles JPEG artifacts to a certain degree.\nIt's made to be a stable upscaler and barely has any artifacts, even beating DF2K_JPEG in many cases.\n\nSuitable for real world images as well as art. It's so good that it might even beat UltraYandere for anime art upscaling.",
        "author": "nmkd",
        "when": "2020-11-06T19:50:26.237000+00:00",
        "name": "NMKD Siax (\"CX\")",
        "hasLink": false
    },
    "4x-NMKD-UltraYandere-Lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** NMKD UltraYandere Lite\n**License:** WTFPL\n**Link:** <https://u.pcloud.link/publink/show?code=kZ2siFXZ2WlliHPXR3Flw3HbpcDNXQXCXiLy>\n**Model Architecture:** ESRGAN Lite [nf=32 nb=12]\n**Scale:** 4\n**Purpose:** Fast Anime/Art upscaling\n\n**Iterations:** 280k\n**batch_size:** 3\n**HR_size:** 256\n**Dataset:** Yandere4800\n**Dataset_size:** 19200 (4800 unique)\n**OTF Training** Only rotation/flipping\n**Pretrained_Model_G:** 4x_DIV2K-Lite\n\n**Description:** The smol version of UltraYandere. It does not come with pixelart/dithering support, but otherwise it has almost the same capabilites as UltraYandere including JPEG artifact removal and halo removal, but in a lighter 19mb package.",
        "author": "nmkd",
        "when": "2020-10-15T12:48:36.779000+00:00",
        "name": "NMKD UltraYandere Lite",
        "hasLink": false
    },
    "4x-NMKD-YandereNeo": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** NMKD YandereNeo\n**License:** WTFPL\n**Link:** <https://icedrive.net/1/f0UAiRqz3N>\n**Model Architecture:** ESRGAN (Lite)\n**Scale:** 2/4\n**Purpose:** Upscaling of high-quality or compressed 2D art\n\n**Iterations:** 320k\n**batch_size:** 4\n**HR_size:** 192 (4x) and 96 (2x) = 48px LR\n**Dataset:** Yandere1200 (yande.re artworks)\n**Dataset_size:** 16800 (1200 unique)\n**OTF Training** Only rotation/flipping\n**Pretrained_Model_G:** 4x_DIV2K-Lite_1M (`4x_NMKD-YandereNeo-Lite_320k` for the 2x conversion)\n\n**Description:** My latest anime art upscaler, made to get rid of some problems that UltraYandere had. It should have less artifacting and less aliased edges. It only comes as a Lite model as it seems to perform very similarly to the \"big\" version, while running 2x-3x as fast.\n\n**Note:** The 2x model is a conversion of the 4x model. It was finetuned for 10k additional iterations.\n\nhttps://i.imgur.com/oxs71v5.png",
        "author": "nmkd",
        "when": "2021-01-26T14:39:45.148000+00:00",
        "name": "NMKD YandereNeo",
        "hasLink": true
    },
    "4x-NXbrz": {
        "content": "<@&560103931204861954> \nName: NXbrz\nLicense: CC BY-NC-SA 4.0\nLink: https://drive.google.com/drive/folders/1mYTMpwDlKQulBmjgKpcxL3-T6xAGXVxl?usp=sharing\nModel Architecture: ESRGAN\nScale: 4\nPurpose: Basic pixel art upscaling, for people who want a more simpler style and lightweight pixel art upscaling model.\nIterations: 150k\nDataset: Images upscaled with the Xbrz scaling filter and images upscaled with the ScaleNX filter, combined together to create a mix of both models.",
        "author": "ComputerK",
        "when": "2021-06-09T17:56:40.772000+00:00",
        "name": "NXbrz",
        "hasLink": true
    },
    "4x-OLDIES-ALTERNATIVE-FINAL": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_OLDIES_ALTERNATIVE_FINAL.pth\n**License:** WTFPL\n**Link:** https://1fichier.com/?u7kwdxdn6uljha3icte8\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Same model as OLDIES_FINAL, but with color correction made\n\n**Iterations:** 290 000\n**batch_size:** 16\n**HR_size:** 128\n**Epoch:** 483\n**Dataset:** different frames from different anime blu-ray\n**Dataset_size:** 1250\n**OTF Training** Yes\n**Pretrained_Model_G:** RRDB_PSNR_x4.pth\n\n**Description:** This model was made for my project captain tsubasa anime so i don't know if it works good for anything else. just try it ;)\n\n\nhttps://imgsli.com/MjEzMDU",
        "author": "solidd93110",
        "when": "2020-09-01T15:09:02.112000+00:00",
        "name": "4x_OLDIES_ALTERNATIVE_FINAL.pth",
        "hasLink": true
    },
    "4x-OLDIESFINAL": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_OLDIES_290000_G_FINAL\n**License:** WTFPL\n**Link:** https://1fichier.com/?tsmmdifzgwkwtdfcc5uo\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** upscale old anime\n\n**Iterations:** 290 000\n**batch_size:** 16\n**HR_size:** 128\n**Epoch:** 183\n**Dataset:** made with full images of anime blu ray.\n**Dataset_size:** 31 316 files\n**OTF Training** Yes\n**Pretrained_Model_G:** RRDB_PSNR_x4.pth\n\n**Description:** i made this model to upscale old anime and denoise.",
        "author": "solidd93110",
        "when": "2020-08-16T14:52:03.049000+00:00",
        "name": "4x_OLDIES_290000_G_FINAL",
        "hasLink": true
    },
    "4x-PackCraft-v4": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4xPackCraft_v4\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://drive.google.com/file/d/1Xe1JorGFJzPClR9tUuGmpuMyLhTA-d2T/view?usp=sharing>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Upscaling pack.png\n\n**Iterations:** 40k (model was hardly changing at this point since its v4)\n**batch_size:** 1\n**HR_size:** 512\n**Epoch:** I don't remember\n**Dataset:** Random screenshots from Alpha Minecraft taken at the correct angle, screen region, block placement, and cloud position (with slight random variance). LR images were OTF downscaled with box/area.\n**Dataset_size:** 60k\n**OTF Training** Yes\n**Pretrained_Model_G:** 4xESRGAN originally, but I used each previous version as a pretrained until I got to v4.\n\n**Description:** The best pack.png upscale so far. Even got a shoutout from SalC1 for it. The seed is found now so who cares anymore :)\n\nhttps://cdn.discordapp.com/attachments/436411385673547787/750481717307113543/pack.png",
        "author": "Joey",
        "when": "2020-09-06T00:13:38.100000+00:00",
        "name": "4xPackCraft_v4",
        "hasLink": true
    },
    "4x-PixelPerfectV4": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_PixelPerfectV4_137000_G\n**﻿License:**﻿ WTFPL\n**﻿Link:**﻿ https://drive.google.com/drive/u/1/folders/1C82o6p9jXzOZIZtMHeVXFW8BYV0s10FA\n﻿**Model Architecture:﻿** ESRGAN\n**﻿Scale:﻿** 4\n﻿**Purpose:﻿** Sprite Upscaler\n﻿\n**Iterations:** 137k\n**﻿batch_size:**﻿ 4\n﻿**HR_size:**﻿ 128\n**Epoch:** 1825\n**Dataset:** Random anime images\n**Dataset_size:** 300\n**Data-Efficient GAN Training:** Yes\n**﻿OTF Training:﻿** No\n**﻿Pretrained_Model_G:**﻿ 4xESRGAN\n﻿**Validation Stats:**﻿ PSNR: 26.518, SSIM: 0.91755, LPIPS: 0.052983\n﻿\n**Description:** A sprite upscaler that works best on small images. I trained this model by having the LR images be downscaled with 97% nearest neighbor and 3% mitchell before reducing the color depth by 48 colors for every image in the dataset. Afterwards, I added 32-bit Riemersma dithering.\n\nhttps://imgsli.com/MzgxMTc/1/2",
        "author": "Sisyphean",
        "when": "2020-11-16T20:28:33.549000+00:00",
        "name": "4x_PixelPerfectV4_137000_G",
        "hasLink": true
    },
    "4x-PocketMonsters-Alpha": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_PocketMonsters-Alpha\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZrIFhXZxnpi72FuwgLz7FIDw8kdrp2suIEk\n**Model Architecture:** ESRGAN (4 in/out nc)\n**Scale:** 4\n**Purpose:** Pixel art with transparency\n\n**Iterations:** 115k\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 209\n**Dataset:** Pokemon drawings, vector art, and Pokemon Home 3D render sprites, downscaled to 256x256 for HR and downscaled with nearest neighbor for LR.\n**Dataset_size:** 4,415\n**OTF Training** Yes (for nearest-neighbor downscaling)\n**Pretrained_Model_G:** 4xFireAlpha\n\n**Description:** A 4x model for upscaling pixel art with alpha channels that should perform better than any other. It should work well on both cartoon and 3D styled content.",
        "author": "Joey",
        "when": "2021-02-05T03:58:26.691000+00:00",
        "name": "4x_PocketMonsters-Alpha",
        "hasLink": true
    },
    "4x-realesrgan-x4minus": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** realesrgan-x4minus\n**License:** WTFPL\n**Link:** https://1drv.ms/u/s!Aip-EMByJHY282_jrOsPVdjxc3by?e=2paTnK\n**Model Architecture:** RealESRGAN\n**Scale:** 4\n**Purpose:** Images with dense pixels.\n\n**Iterations:** 400k \n**batch_size:** 2\n**HR_size:** 256\n**Epoch:** 44\n**Dataset:** Same as realesrgan-x4plus, but later modified to crop some shallow focus images.\n**Dataset_size:** 58k photos\n**OTF Training:** Yes\n**Pretrained_Model_G:** realesrgan-x4plus\n\n**Description:** Basically realesrgan-x4plus without the degradation training. Supposed to help retain more details, but unfortunately due to the dataset (I think) still blurs details adjacent to other objects.\n\nhttps://imgsli.com/MTA0OTA2",
        "author": "DinJerr",
        "when": "2022-04-20T02:59:06.421000+00:00",
        "name": "realesrgan-x4minus",
        "hasLink": true
    },
    "4x-RealisticRescaler": {
        "content": "**Name:** 4x_RealisticRescaler_100000_G\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/folders/13OC-hQNz_S-kX0EVjVgNO1eoGvcXrTfk?usp=sharing\n**Model Architecture:** Real-ESRGAN\n**Scale:** 4\n**Purpose:** General upscaler for realistic images\n\n**Iterations:** 100k\n**batch_size:** 3\n**HR_size:** 128\n**Dataset:** DIV2K, Flick2R, AmbientCG, Poly Haven, nomos2k\n**Dataset_size:** 861\n**OTF Training:** Yes\n**Pretrained_Model_G:** RealESRGAN_x4plus\n\n**Description:** This model was made to upscale realistic low-res textures that are compressed by either JPEG or BC1. From my testing, this works rather well on realistic GameCube textures such as the ones from Shrek Extra Large and the board textures from Mario Party 4. This model could also work on some real life images, especially the ones that are taken outdoors.\n\nhttps://imgsli.com/MTQ0Mzk1",
        "author": "Sisyphean",
        "when": "2023-01-01T23:02:12.504000+00:00",
        "name": "4x_RealisticRescaler_100000_G",
        "hasLink": true
    },
    "4x-REDSVAL-7f-RRDB-Lite": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_REDSVAL-7f-RRDB-Lite (both G and D)\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZojwLXZfeqA3Eoyv6SKXXt01AlSeuHH0WKX\n**Model Architecture:** SOFVSR RRDB / SOFVESRGAN Lite [nf=32, nb=12]\n**Scale:** 4\n**Purpose:** Upscaling real world video / Pretrained model for 7-frames\n\n**Iterations:** 200k\n**batch_size:** Varied\n**HR_size:** 128\n**Epoch:** N/A\n**num_frames:** 7\n**Dataset:** REDS validation set (OTF downscaled with various filters, 1/10 JPEG chance)\n**Dataset_size:** 3,000\n**OTF Training** Yes\n**Pretrained_Model_G:** None\n\n**Description:** Wanted to see how setting the number of frames used for the optical flow estimation to 7 instead of the default 3 would work. It seemed to take much longer to learn the optical flow, so if you do this I recommend doubling all the OF loss weights. I also recommend using this as a pretrained model so it hopefully doesn't take as long to train properly. The results for this aren't too great -- the shimmering isn't too bad which is good but the upscaling is sub par probably do to the variety of downscale types I used. Something more specialized would probably do better. Also, I'm pretty sure if you use this as a pretrained model you can increase either the nf or nb back to default (can't remember which one works) if you want it to have more than a lite model typically does. I also included the trained discriminator in case anyone wants to use that.",
        "author": "Joey",
        "when": "2020-12-03T20:05:17.473000+00:00",
        "name": "4x_REDSVAL-7f-RRDB-Lite (both G and D)",
        "hasLink": true
    },
    "4x-Rek-s-Effeks-Photoanime-v2": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** Rek's Effeks Photoanime v2\n**License:** GNU GPL3\n**Link:** https://u.pcloud.link/publink/show?code=XZbO5pXZTEQNLwBzQjQCo1g8havjhSHCCSsV\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Photo stylization from JPEGs\n\n**Iterations:** 150K\n**batch_size:** 8\n**HR_size:** 128\n**Dataset:** Frames extracted and upscaled from Hausu (1977)\n**Dataset_size:** 3074\n**OTF Training** No\n**Pretrained_Model_G:** NMKD Yandere2\n**Validation Stats:** PSNR: 30.864, SSIM: 0.89676, LPIPS: 0.068872\n\n**Description:** Trained on images upscaled by ISO Denoise v2 -> DeJPEG Fatality PlusULTRA -> NMKD Yan2. Essentially a combination of that model chain into one. Use if you're looking for a stylized output, not photo quality.",
        "author": "Rek",
        "when": "2020-10-17T06:48:49.247000+00:00",
        "name": "Rek's Effeks Photoanime v2",
        "hasLink": false
    },
    "4x-Remacri": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** Remacri\n**Made by:** Foolhardy\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZgSLsXZ0M1fT3kFGfRXg2tNtoUgbSI4kcSy\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** A creation of BSRGAN with more details and less smoothing, made by interpolating IRL models such as Siax, Superscale, Superscale Artisoft, Pixel Perfect, etc. This was, things like skin and other details don't become mushy and blurry.\n\n**Iterations:** 210K~\nAll the other information is unknown, as the model is an interpolation of previous IRL models, so the possible given answers would likely be inaccurate. Sorry!\n\n**Description:** This model is a re-creation of BSRGAN, ~~because BSRGAN isn't compatible for interpolation~~ So, apparently I should've used the old arch for interpolation lol, so I'm going to say that this is a re-make of BSRGAN for extra details Since BSRGAN is a great model, but it usually makes things like skin and details all mushy. This is an issue if you want BSRGAN to give more details instead of creating a mushy output. So, using things like Siax and Superscale, you can create a model which decreases the amount of mushiness and blur. This process is a lot easier to manipulate with Cupscale's interpolation function. I think it's pretty good. Also, \"Remacri\" means \"sharp reality\" in Latin. Fun little detail ;)",
        "author": "ComputerK",
        "when": "2021-04-09T18:36:02.489000+00:00",
        "name": "Remacri",
        "hasLink": true
    },
    "4x-RRDB-G-ResNet-D": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_RRDB-G_ResNet-D (Both G and D)\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZPxrYXZOBnSmRzYywHg9heV55yt0JvKC3gk\n**Model Architecture:** ESRGAN (G) / ResNet (D)\n**Scale:** 4\n**Purpose:** Clean bicubic downscales / Pretrained models (G/D)\n\n**Iterations:** 175k\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 1161\n**Dataset:** DIV2K train + val\n**Dataset_size:** 900\n**OTF Training** Yes (matlab_bicubic downscales)\n**Pretrained_Model_G:** RRDB_ESRGAN_x4_old_arch.pth\n\n**Description:** Trained this to test out my new ResNet discriminator. It's an alternative to the normal VGG discriminator but using ResNet instead of VGG. Theoretically it should be a lot better since ResNet is residual. I trained it the same way that the original ESRGAN model was trained, using the same settings, to get an idea of the difference. I can't really tell to much with just my validation images so it'll be interesting to see other comparisons if anybody decides to make them. I'm also interested to see if people end up with better results using this vs the regular VGG discriminator when training. You'll be able to use it whenever Vic merges my PR.",
        "author": "Joey",
        "when": "2021-01-17T09:22:24.805000+00:00",
        "name": "4x_RRDB-G_ResNet-D (Both G and D)",
        "hasLink": true
    },
    "4x-scalenx": {
        "content": "<@&560103931204861954> added ScaleNX style shader model pixel upscaler to wiki under specialised: \nDirect link: \nhttps://drive.google.com/open?id=1jE2le5Lsab-AcMUMm0Zr9L-OvFNhnRAG",
        "author": "LyonHrt",
        "when": "2019-06-15T20:53:58.922000+00:00",
        "name": null,
        "hasLink": true
    },
    "4x-ScreenBooster-V2": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** ScreenBooster V2\n**License:** CC BY-NC-SA\n**Link:** https://drive.google.com/file/d/1MkqAuELmbhrEGcD55IUJhDFFNniOmNWN\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Game Screenshots\n\n**Iterations:** 44000\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 7\n**Dataset:** My own screenshots (GTA V, ETS2, Minecaft, GTA SA, NFSUG2, Yakuza 0, Just Cause 3...)\n**Dataset_size:** 45000~ tiles (128x128)\n**OTF Training** No\n**Pretrained_Model_G:** 4x_ESRGAN.pth\n\n**Description:** This model is designed to upscale game screenshots (3D Games) by 4 times. \nNote: It may upscale compression, so mind this.\nEDIT: I have uploaded an updated version of this model which was trained from scratch and is much superior to the first one. I didn't feel the need to create yet another entry, so I am just editing this one instead.",
        "author": "BlackScout",
        "when": "2020-01-28T17:37:39.512000+00:00",
        "name": "ScreenBooster V2",
        "hasLink": true
    },
    "4x-SGI": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4xSGI\n**License:** GNU GPL3\n**Link:** https://drive.google.com/file/d/1kNSz5f2_krdlsJ700DwH78OEPYkkcjW5/view?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Upscaling and dedithering pre-rendered sprites, images and textures made in the 90s. Basically vintage CGI.\n\n**Iterations:** 103k\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 3372\n**Dataset:** Very hi-res (up to 2500px) vintage cgi images and prerendered promotional renders for games from the 90s to early 2000s, as well as HD stills from movies made with SGI workstations (early pixar movies and shrek)\n**Dataset_size:** About 158 unique images, trained both with dithered LR versions and non-dithered LR versions\n**OTF Training** Yes\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** This is a model made for upscaling and dedithering pre-rendered CGI, particularly ones made in the 90s and early 2000s. I've actually had this finished since a month ago, but never bothered to release it 😅 \nAlthough, I might have it go through some more iterations and maybe try to find more images for the dataset (although I think I found everything I could).\nhttps://imgur.com/a/iwLTjeB",
        "author": "ChrisNonyminus",
        "when": "2020-12-17T19:55:43.085000+00:00",
        "name": "4xSGI",
        "hasLink": true
    },
    "4x-SkyrimTex-v2-1": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-SkyrimTexV2.1 and V2_Fabric\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZx4XhVZACaRu7n2Efj1CaqgizlFWy0sNE2k> <https://mega.nz/folder/fcp2SLKC#tF3vG7zjvfpnXK7OLUaFRA>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** This model set upscales most Skyrim textures (not alphas)\n\n**Iterations:** ~46k\n**batch_size:** 12 (24 virtual)\n**HR_size:** 64\n**Epoch:** 2020\n**Dataset:** Extracted Skyrim Textures, HRs processed with BC1-Smooth2. IRL images of rocks and grass\n**Dataset_size:** 485\n**OTF Training** Yes (only for 2k iterations)\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** This model set upscales most Skyrim textures. I hope it helps 🙂\n\nThe base model (2.1) works well on stone, wood, metals, and most other textures. The Fabric model is a 50/50 interpolation with a stronger iteration of this model and my Fabric model. As you can tell by the name, it's intended for Fabric textures. The base model may work better interpolated with another model, but it should perform well on it's own.\n\nIf you want to upscale Alpha textures, use a model dedicated to it.\n\nThere's an INFO file in the folder, it just explains the extra models\n\nThanks to ALSA for some images for the dataset to refine the model\n\nhttps://cdn.discordapp.com/attachments/891300021453070387/891392169926098974/unknown.png\nhttps://cdn.discordapp.com/attachments/891393393849147423/891397870266224731/unknown.png\nhttps://cdn.discordapp.com/attachments/891393393849147423/891398051325960202/unknown.png",
        "author": "Kim",
        "when": "2021-09-25T19:04:06.356000+00:00",
        "name": "4x-SkyrimTexV2.1 and V2_Fabric",
        "hasLink": true
    },
    "4x-SkyrimTex-v2-Fabric": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-SkyrimTexV2.1 and V2_Fabric\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZx4XhVZACaRu7n2Efj1CaqgizlFWy0sNE2k> <https://mega.nz/folder/fcp2SLKC#tF3vG7zjvfpnXK7OLUaFRA>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** This model set upscales most Skyrim textures (not alphas)\n\n**Iterations:** ~46k\n**batch_size:** 12 (24 virtual)\n**HR_size:** 64\n**Epoch:** 2020\n**Dataset:** Extracted Skyrim Textures, HRs processed with BC1-Smooth2. IRL images of rocks and grass\n**Dataset_size:** 485\n**OTF Training** Yes (only for 2k iterations)\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** This model set upscales most Skyrim textures. I hope it helps 🙂\n\nThe base model (2.1) works well on stone, wood, metals, and most other textures. The Fabric model is a 50/50 interpolation with a stronger iteration of this model and my Fabric model. As you can tell by the name, it's intended for Fabric textures. The base model may work better interpolated with another model, but it should perform well on it's own.\n\nIf you want to upscale Alpha textures, use a model dedicated to it.\n\nThere's an INFO file in the folder, it just explains the extra models\n\nThanks to ALSA for some images for the dataset to refine the model\n\nhttps://cdn.discordapp.com/attachments/891300021453070387/891392169926098974/unknown.png\nhttps://cdn.discordapp.com/attachments/891393393849147423/891397870266224731/unknown.png\nhttps://cdn.discordapp.com/attachments/891393393849147423/891398051325960202/unknown.png",
        "author": "Kim",
        "when": "2021-09-25T19:04:06.356000+00:00",
        "name": "4x-SkyrimTexV2.1 and V2_Fabric",
        "hasLink": true
    },
    "4x-SmolFace": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x_SmolFace_200k\n**License:** CC BY-NC-SA 4.0\n**Link:**\nhttps://1drv.ms/u/s!Aip-EMByJHY29BRdcYSnTpgx0LWd?e=I6x5R7\n**Model Architecture:** ESRGAN\n**Scale:** 4 \n**Purpose:** Upscaling small pixelated anime faces.\n\n**Iterations:** 400k\n**batch_size:** 4\n**HR_size:** 64\n**Dataset:** FatalityFaces + danbooru side faces\n**Dataset_size:** 5500\n**OTF Training:** Yes\n**Pretrained_Model_G:** 4x_NMKD-UltraYandere_300k\n\n**Description:** A sharp upscaler trained specifically for small sprite faces. Does not blend, so avoid using on painted/photo portraits unless you were trying to retain more of the outlines somehow.\n\n**Comparisons:** https://imgsli.com/MTIxNjU1/8/9",
        "author": "DinJerr",
        "when": "2022-08-19T15:42:24.060000+00:00",
        "name": "4x_SmolFace_200k",
        "hasLink": true
    },
    "4x-SmoothRealism": {
        "content": "<@&560103931204861954> @wiki \n**Name:** 4xSmoothRealism\n**License:** CC-BY-NC 4.0\n**Link:** https://drive.google.com/open?id=1Uc9RUc2YpZKpPoGQeGxNUb3Ro-U720cH\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Pixel art, rocky/grainy textures? Quantization smoothing, adding detail.\n\n**Iterations:** 140k\n**batch_size:** 2\n**HR_size:** 64\n**Epoch:** Don't remember, and who really cares?\n**Dataset:** LR: Default minecraft. HR: R3DCraft Smooth Realism Texture Pack (64x resolution)\n**Dataset_size:** 679\n**OTF Training** No\n**Pretrained_Model_G:** 4x_RRDB_PSNR_old_arch.pth\n\n**Description:** Based on the somewhat-faithful but highly detailed minecraft texturepack R3DCraft (Smooth Realism variant). I'm still not 100% sure what this works best on but it works well on lots of different pixelated images.",
        "author": "Joey",
        "when": "2020-03-04T22:00:22.978000+00:00",
        "name": "4xSmoothRealism",
        "hasLink": true
    },
    "4x-SOFVSR-REDS-F3-V1": {
        "content": ".\n**Name:** SOFVSR_REDS_F3_V1\n**License:** WTFPL\n**Link:** https://mega.nz/file/bpA2QTxI#j_ALxzI5rNjt-T5XcOu8P6ogtAjmFHJeggJbXLGy_Nc\n**Model Architecture:** RRDB\n**Scale:** 4\n**Purpose:** upscale IRL videos\n\n**Iterations:** 100k\n**batch_size:** 12\n**HR_size:** 128\n**Dataset:** REDS\n**OTF Training:** No\n**Pretrained_Model_G:** None\n\n**Description:** i wanted to see how good is SOFVSR at 3 frames. well it's as good as 5 frames. i wouldn't recommend to use this model.\nsee https://discord.com/channels/547949405949657098/547949806761410560/813134988992053308",
        "author": "Sunseille",
        "when": "2021-02-26T18:12:22.352000+00:00",
        "name": "SOFVSR_REDS_F3_V1",
        "hasLink": true
    },
    "4x-Sol-Levante-NTSC2HD": {
        "content": "<@&560103931204861954> @wiki \n**Name:** Sol Levante NTSC2HD\n**License:** UNLICENSE (https://choosealicense.com/licenses/unlicense)\n**Link:** https://drive.google.com/open?id=1H3F8OVBnK2cd5NjbCyc3tGwTZJBF1gk7\n**Model Architecture:** ESRGAN (old-arch)\n**Scale:** 4\n**Purpose:** I would recommend anyone working on NTSC DVD models to use this as a base-model for anime models, I wouldn't recommend doing so for anything else.\n\n**Iterations:** 300,000\n**batch_size:** 32\n**HR_size:** 128\n**Epoch:** 1,522\n**Dataset:** ```HR: http://download.opencontent.netflix.com.s3.amazonaws.com/SolLevante/hdr10/SolLevante_HDR10_r2020_ST2084_UHD_24fps_1000nit.mov downscaled to 2880x1920 (720*4x480*4)\nLR: Encoded the original HR file to NTSC DVD-spec using DVDStyler and remuxed it to MKV using MakeMKV which it then had every frame exported as a png frame sequence, untouched.```\n**Dataset_size:** 6,309\n**OTF Training** No\n**Pretrained_Model_G:** RRDB_PSNR_x4\n\n**Description:** NTSC DVD-spec encode x4 scale super-resolution for Anime Drawing style content. The dataset has a LOT of data throughout almost every frame, so it had a lot of stuff to learn. The resulting DVD-spec encode also had some blocking at times so it also learned to fight off blocking.\n\nhttps://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=f687db4c-799e-11ea-a879-0edaf8f81e27\nRaw validation and original images are in /examples folder of download link",
        "author": "MiU",
        "when": "2020-04-08T14:33:23.818000+00:00",
        "name": "Sol Levante NTSC2HD",
        "hasLink": true
    },
    "4x-SpongeBob-CEL-2-HD-PHOENiX": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Notice:** Not 100% ready for primetime, could do with some more data+training but its usable\n**Scale:** 4\n**Purpose:** dempeg dehalo dealias\n**Description:** Restorative CEL Animation MPEG-1 and 2 Model specifically crafted for SpongeBob S01.\n**License:** GPLv3 - SHARE YOUR CHANGES - If you use this as a pretrained model, share your new model!!!\n**Link:** https://drive.google.com/open?id=1D3AjglmYlQ2HGda7iw4JXckSGbVBqsQr\n**Iterations:** 125,000\n**batch_size:** 16\n**HR_size:** 128\n**Epoch:** 5\n**Dataset_size:** 4,012\n**Pretrained_Model_G:** RRDB_PSNR_x4.pth",
        "author": "MiU",
        "when": "2019-12-24T23:23:41.377000+00:00",
        "name": "SpongeBob.CEL.2.HD.125ki.499e-PHOENiX.pth",
        "hasLink": true
    },
    "4x-SpongeBob-Reloaded": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_SpongeBob-Reloaded & 4x_SpongeBob-Reloaded-SWAG\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZh614XZIyy5JlX7rFm4d3HJJTBovbSz3ycy\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Cartoon Textures & Pixel Art\n\n**Iterations:** 153k\n**batch_size:** 20\n**HR_size:** 128\n**Epoch:** 122\n**Dataset:** SpongeBob Season 11, downscaled with linear. Random OTF JPEG, dither, and indexing.\n**Dataset_size:** 24,123\n**OTF Training** Yes\n**Pretrained_Model_G/D:** 4xSpongeBob\n\n**Description:** Wanted to test out the new SWA implementation as well as the Freeze implementation (for the pretrained disciminator). I decided to try something safe, which was finetuning 4xSpongeBob to fix some of the issues. I used mostly the same settings, but also used mixup augmentations and slightly different loss settings. From my tests, both these models look better than 4xSpongeBob. It was an interesting experiment and it turned out really great -- I recommend using either of these over the original 4xSpongeBob.",
        "author": "Joey",
        "when": "2020-12-12T04:38:25.848000+00:00",
        "name": "4x_SpongeBob-Reloaded & 4x_SpongeBob-Reloaded-SWAG",
        "hasLink": true
    },
    "4x-SpongeBob-Reloaded-SWAG": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_SpongeBob-Reloaded & 4x_SpongeBob-Reloaded-SWAG\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZh614XZIyy5JlX7rFm4d3HJJTBovbSz3ycy\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Cartoon Textures & Pixel Art\n\n**Iterations:** 153k\n**batch_size:** 20\n**HR_size:** 128\n**Epoch:** 122\n**Dataset:** SpongeBob Season 11, downscaled with linear. Random OTF JPEG, dither, and indexing.\n**Dataset_size:** 24,123\n**OTF Training** Yes\n**Pretrained_Model_G/D:** 4xSpongeBob\n\n**Description:** Wanted to test out the new SWA implementation as well as the Freeze implementation (for the pretrained disciminator). I decided to try something safe, which was finetuning 4xSpongeBob to fix some of the issues. I used mostly the same settings, but also used mixup augmentations and slightly different loss settings. From my tests, both these models look better than 4xSpongeBob. It was an interesting experiment and it turned out really great -- I recommend using either of these over the original 4xSpongeBob.",
        "author": "Joey",
        "when": "2020-12-12T04:38:25.848000+00:00",
        "name": "4x_SpongeBob-Reloaded & 4x_SpongeBob-Reloaded-SWAG",
        "hasLink": true
    },
    "4x-Spongebob-v6": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x Spongebob v6\n**License:** CC-BY-NC 4.0\n**Link:** https://drive.google.com/open?id=1NHuNF36dp_myvPH__IesJ5x6GuppUwS6\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Cartoons\n\n**Iterations:** 190k\n**batch_size:** 20\n**HR_size:** 128\n**Dataset:** 1 frame of every scene of season 11, downscaled 50% with nearest neighbor\n**Dataset_size:** 4,803\n**OTF Training** Yes\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** New version of the Spongebob model. Ideally it's a lot sharper and cleaner but I'm still not sure if it works better than the old one. From what I can tell it's better in many cases.\n\n\n**Name:** 4x Spongebob v6 De-Quantize\n**License:** CC-BY-NC 4.0\n**Link:** https://drive.google.com/open?id=1HFA1e75QvdostGAZVPBxFHtkiru5gLPa\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Quantized Cartoons\n\n**Iterations:** 90k\n**batch_size:** 20\n**HR_size:** 128\n**Dataset:** 1 frame of every scene of season 11, downscaled 50% with nearest neighbor\n**Dataset_size:** 4,803\n**OTF Training** Yes\n**Pretrained_Model_G:** 4x Spongebob v6\n\n**Description:** A model I trained to do both de-quantizing as well as upscaling. The results are pretty blurry but it works decently for what it is.\n\n\n**Name:** 4x Spongebob v6 Deblur\n**License:** CC-BY-NC 4.0\n**Link:** https://drive.google.com/open?id=1geNLDAnQzLadMvvoRLhVy4MXhQf5t8JP\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Cartoons\n\n**Iterations:** 65k\n**batch_size:** 20\n**HR_size:** 128\n**Dataset:** 1 frame of every scene of season 11, downscaled 50% with nearest neighbor\n**Dataset_size:** 4,803\n**OTF Training** Yes\n**Pretrained_Model_G:** 4x Spongebob v6\n\n**Description:** After not being entirely happy with the main Spongebob v6 model, I trained a new one with blurring OTF options and two different downscale types. This one is much better in my opinion.",
        "author": "Joey",
        "when": "2019-11-08T06:18:29.760000+00:00",
        "name": "4x Spongebob v6",
        "hasLink": true
    },
    "4x-Spongebob-v6-De-Quantize": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x Spongebob v6\n**License:** CC-BY-NC 4.0\n**Link:** https://drive.google.com/open?id=1NHuNF36dp_myvPH__IesJ5x6GuppUwS6\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Cartoons\n\n**Iterations:** 190k\n**batch_size:** 20\n**HR_size:** 128\n**Dataset:** 1 frame of every scene of season 11, downscaled 50% with nearest neighbor\n**Dataset_size:** 4,803\n**OTF Training** Yes\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** New version of the Spongebob model. Ideally it's a lot sharper and cleaner but I'm still not sure if it works better than the old one. From what I can tell it's better in many cases.\n\n\n**Name:** 4x Spongebob v6 De-Quantize\n**License:** CC-BY-NC 4.0\n**Link:** https://drive.google.com/open?id=1HFA1e75QvdostGAZVPBxFHtkiru5gLPa\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Quantized Cartoons\n\n**Iterations:** 90k\n**batch_size:** 20\n**HR_size:** 128\n**Dataset:** 1 frame of every scene of season 11, downscaled 50% with nearest neighbor\n**Dataset_size:** 4,803\n**OTF Training** Yes\n**Pretrained_Model_G:** 4x Spongebob v6\n\n**Description:** A model I trained to do both de-quantizing as well as upscaling. The results are pretty blurry but it works decently for what it is.\n\n\n**Name:** 4x Spongebob v6 Deblur\n**License:** CC-BY-NC 4.0\n**Link:** https://drive.google.com/open?id=1geNLDAnQzLadMvvoRLhVy4MXhQf5t8JP\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Cartoons\n\n**Iterations:** 65k\n**batch_size:** 20\n**HR_size:** 128\n**Dataset:** 1 frame of every scene of season 11, downscaled 50% with nearest neighbor\n**Dataset_size:** 4,803\n**OTF Training** Yes\n**Pretrained_Model_G:** 4x Spongebob v6\n\n**Description:** After not being entirely happy with the main Spongebob v6 model, I trained a new one with blurring OTF options and two different downscale types. This one is much better in my opinion.",
        "author": "Joey",
        "when": "2019-11-08T06:18:29.760000+00:00",
        "name": "4x Spongebob v6",
        "hasLink": true
    },
    "4x-Spongebob-v6-Deblur": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Name:** 4x Spongebob v6\n**License:** CC-BY-NC 4.0\n**Link:** https://drive.google.com/open?id=1NHuNF36dp_myvPH__IesJ5x6GuppUwS6\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Cartoons\n\n**Iterations:** 190k\n**batch_size:** 20\n**HR_size:** 128\n**Dataset:** 1 frame of every scene of season 11, downscaled 50% with nearest neighbor\n**Dataset_size:** 4,803\n**OTF Training** Yes\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** New version of the Spongebob model. Ideally it's a lot sharper and cleaner but I'm still not sure if it works better than the old one. From what I can tell it's better in many cases.\n\n\n**Name:** 4x Spongebob v6 De-Quantize\n**License:** CC-BY-NC 4.0\n**Link:** https://drive.google.com/open?id=1HFA1e75QvdostGAZVPBxFHtkiru5gLPa\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Quantized Cartoons\n\n**Iterations:** 90k\n**batch_size:** 20\n**HR_size:** 128\n**Dataset:** 1 frame of every scene of season 11, downscaled 50% with nearest neighbor\n**Dataset_size:** 4,803\n**OTF Training** Yes\n**Pretrained_Model_G:** 4x Spongebob v6\n\n**Description:** A model I trained to do both de-quantizing as well as upscaling. The results are pretty blurry but it works decently for what it is.\n\n\n**Name:** 4x Spongebob v6 Deblur\n**License:** CC-BY-NC 4.0\n**Link:** https://drive.google.com/open?id=1geNLDAnQzLadMvvoRLhVy4MXhQf5t8JP\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Cartoons\n\n**Iterations:** 65k\n**batch_size:** 20\n**HR_size:** 128\n**Dataset:** 1 frame of every scene of season 11, downscaled 50% with nearest neighbor\n**Dataset_size:** 4,803\n**OTF Training** Yes\n**Pretrained_Model_G:** 4x Spongebob v6\n\n**Description:** After not being entirely happy with the main Spongebob v6 model, I trained a new one with blurring OTF options and two different downscale types. This one is much better in my opinion.",
        "author": "Joey",
        "when": "2019-11-08T06:18:29.760000+00:00",
        "name": "4x Spongebob v6",
        "hasLink": true
    },
    "4x-Struzan": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_Struzan\n**License:** CC BY-NC-SA 4.0\n**Link:** https://drive.google.com/file/d/1RhEkTYy-1DxJP2ynLW5NSrz6BjVMy8xx/view\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Upscaling airbrush/pencil-based artwork\n\n**Iterations:** 300k\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** 430\n**Dataset:** High-res movie poster artwork by Drew Struzan and Bill Eaken\n**Dataset_size:** 2790\n**OTF Training** No\n**Pretrained_Model_G:** 4xPSNR\n\n**Description:** This model is a bit picky regarding image size, or better **feature** size (width of pencil strokes, etc.). Check the images on http://drewstruzan.com/illustrated/portfolio/ - these all work pretty well in terms of scale. But usually you need to DeJPEG (and sometimes DeSharpen) your input first. A few comparisons at 50k can be found here: https://drive.google.com/drive/folders/1fyeIWInDrM6r-xxrCW4U09oafWw08u9S?usp=sharing",
        "author": "Laserschwert",
        "when": "2020-10-12T23:57:50.376000+00:00",
        "name": "4x_Struzan",
        "hasLink": true
    },
    "4x-ThiefGold": {
        "content": "<@&560103931204861954> @wiki \n**Name:** 4x_ThiefGold_110000\n**License:** No idea\n**Link:** https://drive.google.com/file/d/1L8reADxi5Nt-8Tki0oY5jkQlv6kJg6w4/view?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Various game textures. Primary wood, metal, stone\n\n**Iterations:** 110000\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** ?\n**Dataset:** Thief Gold & Thief II community made textures\n**Dataset_size:** don't remember\n**OTF Training** No\n**Pretrained_Model_G:** RRDB_ESRGAN_x4\n\n**Description:** This model was based on community made HD textures for Thief Gold & Thief II: The Metal Age. Textures were reduced to 256 colors to simulate Thief Gold palette. Surprisingly enough, model works good with any kind of textures with the exception of brights colors which sometimes generate dotted artifacts. Especially good it upscales stone, metal and wood surfaces. It is a good model to upscale game textures especially from old games with low quality sources. Working of Thief Gold/Thief II ESRGAN mod I used this model 50% of the time. Often it gaves better results than Skyrim trained models from wiki. Example: https://imgsli.com/MTYzNjI (not sure from ThiefGold or ThiefGoldMod model).",
        "author": "Akven",
        "when": "2020-05-15T19:46:37.253000+00:00",
        "name": "4x_ThiefGold_110000",
        "hasLink": true
    },
    "4x-ThiefGoldMod": {
        "content": "<@&560103931204861954> @wiki \n**Name:** 4x_ThiefGoldMod_100000\n**License:** No idea\n**Link:** https://drive.google.com/file/d/1y9nYjuy8vG7_LHpveYTj0ihK_6fE2XKF/view?usp=sharing\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Various game textures. Primary wood, metal, stone\n\n**Iterations:** 100000\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** ?\n**Dataset:** Thief Gold & Thief II community made textures\n**Dataset_size:** don't remember\n**OTF Training** No\n**Pretrained_Model_G:** 4x_Manga109Attempt\n\n**Description:** Version of the previous model but based on Manga109 pretrained model and with slightly different dataset. Sometimes gives better results especially for wood and metal, sometimes worse. Sometime generates the same dotted artifacts on very bright/white images. Still better most of the time for Thief game series than Skyrim based models. Example: https://imgsli.com/MTYzNjI (not sure from ThiefGold or ThiefGoldMod model).",
        "author": "Akven",
        "when": "2020-05-15T19:58:05.838000+00:00",
        "name": "4x_ThiefGoldMod_100000",
        "hasLink": true
    },
    "4x-Training4Melozard-Anime": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_Training4Melozard_Anime\n**License:** Whatever <@!716378030121156669> wants\n**Link:** https://u.pcloud.link/publink/show?code=kZcUlBXZ9egcFjtf3cjX02lDqNAr5j4l3jFX\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Anime DVDs (I think)\n\n**Iterations:** 144k\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 5770\n**Dataset:** <@!716378030121156669>'s Anime dataset \n**Dataset_size:** 202\n**OTF Training** No\n**Pretrained_Model_G:** RRDB_ESRGAN_x4_old_arch.pth\n\n**Description:** <@!716378030121156669> wanted me to train this for him so I did. I think it turned out pretty well but I also don't really know what to test it on besides part of the dataset he gave me.",
        "author": "Joey",
        "when": "2021-03-17T21:41:52.581000+00:00",
        "name": "4x_Training4Melozard_Anime",
        "hasLink": true
    },
    "4x-UltraFArt-v3": {
        "content": "<@&560103931204861954>\nName: 4x_UltraFArt_v3 suite\nLicense: CC BY-NC-SA 4.0\nLink: https://1drv.ms/u/s!Aip-EMByJHY27UIFr1mk87zFytap?e=ZZliII\nModel Architecture: ESRGAN (NF=128, NB=23)\nScale: 4\nPurpose: Painting style with larger shaped features (?).\nIterations: ~240k each model\nbatch_size: 1\nHR_size: 320\nDataset: ArtStation + Fatality Faces tiles.\nDataset_size: 2000 digital illustrations\nOTF Training: Yes\nPretrained_Model_G: 4x_UltraFArt\nDescription: Trying to see whether large tile size and node network will result in better results, sadly I have to say it is mostly diminishing returns. But anyway, this release contains 1 base model, and 3 refined model based on more/less details to interpolate with. Since there's little anime in this dataset, it does not have the sharpness, thin lines that most other illustration models do. Sample images in folder.",
        "author": "DinJerr",
        "when": "2021-05-14T23:41:57.763000+00:00",
        "name": "4x_UltraFArt_v3 suite",
        "hasLink": false
    },
    "4x-UltraFArt-v3-Fine": {
        "content": "<@&560103931204861954>\nName: 4x_UltraFArt_v3 suite\nLicense: CC BY-NC-SA 4.0\nLink: https://1drv.ms/u/s!Aip-EMByJHY27UIFr1mk87zFytap?e=ZZliII\nModel Architecture: ESRGAN (NF=128, NB=23)\nScale: 4\nPurpose: Painting style with larger shaped features (?).\nIterations: ~240k each model\nbatch_size: 1\nHR_size: 320\nDataset: ArtStation + Fatality Faces tiles.\nDataset_size: 2000 digital illustrations\nOTF Training: Yes\nPretrained_Model_G: 4x_UltraFArt\nDescription: Trying to see whether large tile size and node network will result in better results, sadly I have to say it is mostly diminishing returns. But anyway, this release contains 1 base model, and 3 refined model based on more/less details to interpolate with. Since there's little anime in this dataset, it does not have the sharpness, thin lines that most other illustration models do. Sample images in folder.",
        "author": "DinJerr",
        "when": "2021-05-14T23:41:57.763000+00:00",
        "name": "4x_UltraFArt_v3 suite",
        "hasLink": false
    },
    "4x-UltraFArt-v3-Photo": {
        "content": "<@&560103931204861954>\nName: 4x_UltraFArt_v3 suite\nLicense: CC BY-NC-SA 4.0\nLink: https://1drv.ms/u/s!Aip-EMByJHY27UIFr1mk87zFytap?e=ZZliII\nModel Architecture: ESRGAN (NF=128, NB=23)\nScale: 4\nPurpose: Painting style with larger shaped features (?).\nIterations: ~240k each model\nbatch_size: 1\nHR_size: 320\nDataset: ArtStation + Fatality Faces tiles.\nDataset_size: 2000 digital illustrations\nOTF Training: Yes\nPretrained_Model_G: 4x_UltraFArt\nDescription: Trying to see whether large tile size and node network will result in better results, sadly I have to say it is mostly diminishing returns. But anyway, this release contains 1 base model, and 3 refined model based on more/less details to interpolate with. Since there's little anime in this dataset, it does not have the sharpness, thin lines that most other illustration models do. Sample images in folder.",
        "author": "DinJerr",
        "when": "2021-05-14T23:41:57.763000+00:00",
        "name": "4x_UltraFArt_v3 suite",
        "hasLink": false
    },
    "4x-UltraFArt-v3-Smooth": {
        "content": "<@&560103931204861954>\nName: 4x_UltraFArt_v3 suite\nLicense: CC BY-NC-SA 4.0\nLink: https://1drv.ms/u/s!Aip-EMByJHY27UIFr1mk87zFytap?e=ZZliII\nModel Architecture: ESRGAN (NF=128, NB=23)\nScale: 4\nPurpose: Painting style with larger shaped features (?).\nIterations: ~240k each model\nbatch_size: 1\nHR_size: 320\nDataset: ArtStation + Fatality Faces tiles.\nDataset_size: 2000 digital illustrations\nOTF Training: Yes\nPretrained_Model_G: 4x_UltraFArt\nDescription: Trying to see whether large tile size and node network will result in better results, sadly I have to say it is mostly diminishing returns. But anyway, this release contains 1 base model, and 3 refined model based on more/less details to interpolate with. Since there's little anime in this dataset, it does not have the sharpness, thin lines that most other illustration models do. Sample images in folder.",
        "author": "DinJerr",
        "when": "2021-05-14T23:41:57.763000+00:00",
        "name": "4x_UltraFArt_v3 suite",
        "hasLink": false
    },
    "4x-UltraSharp": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-UltraSharp (config and presets included!)\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZ9LXhVZGBUaBFseSGmByn5vmgoQULUFiixV> <https://mega.nz/folder/qZRBmaIY#nIG8KyWFcGNTuMX_XNbJ_g>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Any, it's universal. This model performs best on JPEG compressed images.\n\n**Iterations:** 150k\n**batch_size:** 4-8\n**HR_size:** 128\n**Epoch:** ~480\n**Dataset:** So many. I used: RAW images shot by myself, SignatureEdits, AdobeMIT-5K, DIV2K, TLOK from brucethemoose, some rock/stone images from ALSA, and many images provided by <@724494344270381097> (thanks!)\n**Dataset_size:** uh, ignore this. anywhere between 2k and 8k full size images throughout training\n**OTF Training** Yes (custom augmentation presets)\n**Pretrained_Model_G:** 4x-UniScale-Balanced\n\n**Description:** This is my best model yet! It generates lots and lots of detail and leaves a nice texture on images. It works on most images, whether compressed or not. It does work best on JPEG compression though, as that's mostly what it was trained on. It has the ability to restore highly compressed images as well!\n\nThe model was trained with KernelGAN (thanks musl for supplying the blur kernels), noise patches, custom augmentation presets (are in with the model), and the losses: pixel, feature, cx, ssim, lpips, and fft. Mixup was used for a while, but abandedoned due to stability issues.\n\nGradient Clipping helped immensely with model stability throughout training.\n\nBig thanks to musl for giving advice on how to further improve the model!\n\nhttps://cdn.discordapp.com/attachments/902853011913732097/902855337705611294/unknown.png\nhttps://cdn.discordapp.com/attachments/902853011913732097/902853340600360990/unknown.png\nhttps://cdn.discordapp.com/attachments/902853011913732097/902855036768489512/unknown.png\nhttps://cdn.discordapp.com/attachments/900506271566938122/902867618086670356/unknown.png",
        "author": "Kim",
        "when": "2021-10-27T10:39:08.509000+00:00",
        "name": "4x-UltraSharp (config and presets included!)",
        "hasLink": true
    },
    "4x-UniScale-Balanced": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-UniScale-Balanced/Strong (With Noise Removal [NR] versions)\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZNLXhVZXL6jpsc8x1ylnU3hYzjDVuOFhoDX> <https://mega.nz/folder/WEwUCDSJ#b1eXDT9b7yMKVlOURbR4FQ/folder/ed5xGaoa>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** This model works for nearly every image you'd put through it, including art.\n\n**Iterations:** About 140k\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** 29(ish)\n**Dataset:** Custom Dataset consisting of lossless 4k frames from Metal Arms: Glitch in the System, Just Cause 3, Dirt 3, Forza Horizon 3, Sleeping Dogs, and self-edited photos from SignatureEdits\n**Dataset_size:** 18,909 tiles\n**OTF Training** Yes (Linear, Bicubic, Nearest Neighbor, JPEG, BSRGAN_noise and Camera noise (traiNNer))\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** A great version of my past models. This set of models can achieve nearly everything those can but with better results. This time (for once), the names are self-explanatory.\n\nCompared to many other general upscaling models, these yield cleaner or more accurate results. These models aren't quite as sharp as some others, but that's intentional (If you want sharper results, interpolate one of these with MultiUpscale-T1, Siax, or UniversalUpscaler.) They work for nearly everything, even hand-drawn stuff.\n\nThe noise removal (NR) versions were trained with LR Camera noise injection, courtesy of traiNNer's recent update.\n\nUniScale-Interp is available here as well.\n\n<https://slow.pics/c/i75q0yN1> - Realistic image\n<https://slow.pics/c/xr3hBNka> - Realistic image\n<https://slow.pics/c/UkCwdK13> - Spongebob Comparison\n<https://slow.pics/c/qHDo50X7> - Somewhat terrible pixel art demonstration\nhttps://cdn.discordapp.com/attachments/549519631761539082/877997627025813526/unknown.png\n\n**EDIT:** Folder link was updated, and i've added new BSRGAN-like models added alongside those ^",
        "author": "Kim",
        "when": "2021-08-23T21:22:53.138000+00:00",
        "name": "4x-UniScale-Balanced/Strong (With Noise Removal [NR] versions)",
        "hasLink": true
    },
    "4x-UniScale-Restore": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-UniScale_Restore\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZvLXhVZwUerRGSKFCSsAaoGDsslQB1JIY5k> <https://mega.nz/folder/nExkQYZC#4Y5zXlXWXK-qL-4cmbgsxQ>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** This model has strong compression removal that helps with restoring heavily compressed or noisy images. It is intended to compete with BSRGAN.\n\n**Iterations:** 55400\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** 30\n**Dataset:** Custom Dataset consisting of lossless 4k frames from Metal Arms: Glitch in the System, Just Cause 3, Dirt 3, Forza Horizon 3, Sleeping Dogs, and self-edited photos from SignatureEdits\n**Dataset_size:** 18,909 tiles\n**OTF Training** Yes (Linear, Bicubic, Nearest Neighbor, JPEG, BSRGAN_resize, Combo_noise)\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** Works amazingly on games, textures, cartoons, and generally anything but detailed realistic images. This model usually doesn't need a model chained before it to remove noise or compression artifacts, including dithering, JPEG artifacts, h264 artifacts, and even DDS.\nIt has *strong* compression removal, and will restore most images with heavy compression.\n\nTrained with BSRGAN_Resize and Combo_Noise in traiNNer. \n\nhttps://cdn.discordapp.com/attachments/880826637543964712/880932754102026290/unknown.png\nhttps://cdn.discordapp.com/attachments/880826637543964712/880933836710641744/unknown.png\nhttps://cdn.discordapp.com/attachments/547949806761410560/880585159093665802/unknown.png\nhttps://cdn.discordapp.com/attachments/880826637543964712/880934732748169226/unknown.png",
        "author": "Kim",
        "when": "2021-08-27T22:16:05.035000+00:00",
        "name": "4x-UniScale_Restore",
        "hasLink": true
    },
    "4x-UniScale-Strong": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-UniScale-Balanced/Strong (With Noise Removal [NR] versions)\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZNLXhVZXL6jpsc8x1ylnU3hYzjDVuOFhoDX> <https://mega.nz/folder/WEwUCDSJ#b1eXDT9b7yMKVlOURbR4FQ/folder/ed5xGaoa>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** This model works for nearly every image you'd put through it, including art.\n\n**Iterations:** About 140k\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** 29(ish)\n**Dataset:** Custom Dataset consisting of lossless 4k frames from Metal Arms: Glitch in the System, Just Cause 3, Dirt 3, Forza Horizon 3, Sleeping Dogs, and self-edited photos from SignatureEdits\n**Dataset_size:** 18,909 tiles\n**OTF Training** Yes (Linear, Bicubic, Nearest Neighbor, JPEG, BSRGAN_noise and Camera noise (traiNNer))\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** A great version of my past models. This set of models can achieve nearly everything those can but with better results. This time (for once), the names are self-explanatory.\n\nCompared to many other general upscaling models, these yield cleaner or more accurate results. These models aren't quite as sharp as some others, but that's intentional (If you want sharper results, interpolate one of these with MultiUpscale-T1, Siax, or UniversalUpscaler.) They work for nearly everything, even hand-drawn stuff.\n\nThe noise removal (NR) versions were trained with LR Camera noise injection, courtesy of traiNNer's recent update.\n\nUniScale-Interp is available here as well.\n\n<https://slow.pics/c/i75q0yN1> - Realistic image\n<https://slow.pics/c/xr3hBNka> - Realistic image\n<https://slow.pics/c/UkCwdK13> - Spongebob Comparison\n<https://slow.pics/c/qHDo50X7> - Somewhat terrible pixel art demonstration\nhttps://cdn.discordapp.com/attachments/549519631761539082/877997627025813526/unknown.png\n\n**EDIT:** Folder link was updated, and i've added new BSRGAN-like models added alongside those ^",
        "author": "Kim",
        "when": "2021-08-23T21:22:53.138000+00:00",
        "name": "4x-UniScale-Balanced/Strong (With Noise Removal [NR] versions)",
        "hasLink": true
    },
    "4x-UniScaleNR-Balanced": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-UniScale-Balanced/Strong (With Noise Removal [NR] versions)\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZNLXhVZXL6jpsc8x1ylnU3hYzjDVuOFhoDX> <https://mega.nz/folder/WEwUCDSJ#b1eXDT9b7yMKVlOURbR4FQ/folder/ed5xGaoa>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** This model works for nearly every image you'd put through it, including art.\n\n**Iterations:** About 140k\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** 29(ish)\n**Dataset:** Custom Dataset consisting of lossless 4k frames from Metal Arms: Glitch in the System, Just Cause 3, Dirt 3, Forza Horizon 3, Sleeping Dogs, and self-edited photos from SignatureEdits\n**Dataset_size:** 18,909 tiles\n**OTF Training** Yes (Linear, Bicubic, Nearest Neighbor, JPEG, BSRGAN_noise and Camera noise (traiNNer))\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** A great version of my past models. This set of models can achieve nearly everything those can but with better results. This time (for once), the names are self-explanatory.\n\nCompared to many other general upscaling models, these yield cleaner or more accurate results. These models aren't quite as sharp as some others, but that's intentional (If you want sharper results, interpolate one of these with MultiUpscale-T1, Siax, or UniversalUpscaler.) They work for nearly everything, even hand-drawn stuff.\n\nThe noise removal (NR) versions were trained with LR Camera noise injection, courtesy of traiNNer's recent update.\n\nUniScale-Interp is available here as well.\n\n<https://slow.pics/c/i75q0yN1> - Realistic image\n<https://slow.pics/c/xr3hBNka> - Realistic image\n<https://slow.pics/c/UkCwdK13> - Spongebob Comparison\n<https://slow.pics/c/qHDo50X7> - Somewhat terrible pixel art demonstration\nhttps://cdn.discordapp.com/attachments/549519631761539082/877997627025813526/unknown.png\n\n**EDIT:** Folder link was updated, and i've added new BSRGAN-like models added alongside those ^",
        "author": "Kim",
        "when": "2021-08-23T21:22:53.138000+00:00",
        "name": "4x-UniScale-Balanced/Strong (With Noise Removal [NR] versions)",
        "hasLink": true
    },
    "4x-UniScaleNR-Strong": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-UniScale-Balanced/Strong (With Noise Removal [NR] versions)\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZNLXhVZXL6jpsc8x1ylnU3hYzjDVuOFhoDX> <https://mega.nz/folder/WEwUCDSJ#b1eXDT9b7yMKVlOURbR4FQ/folder/ed5xGaoa>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** This model works for nearly every image you'd put through it, including art.\n\n**Iterations:** About 140k\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** 29(ish)\n**Dataset:** Custom Dataset consisting of lossless 4k frames from Metal Arms: Glitch in the System, Just Cause 3, Dirt 3, Forza Horizon 3, Sleeping Dogs, and self-edited photos from SignatureEdits\n**Dataset_size:** 18,909 tiles\n**OTF Training** Yes (Linear, Bicubic, Nearest Neighbor, JPEG, BSRGAN_noise and Camera noise (traiNNer))\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** A great version of my past models. This set of models can achieve nearly everything those can but with better results. This time (for once), the names are self-explanatory.\n\nCompared to many other general upscaling models, these yield cleaner or more accurate results. These models aren't quite as sharp as some others, but that's intentional (If you want sharper results, interpolate one of these with MultiUpscale-T1, Siax, or UniversalUpscaler.) They work for nearly everything, even hand-drawn stuff.\n\nThe noise removal (NR) versions were trained with LR Camera noise injection, courtesy of traiNNer's recent update.\n\nUniScale-Interp is available here as well.\n\n<https://slow.pics/c/i75q0yN1> - Realistic image\n<https://slow.pics/c/xr3hBNka> - Realistic image\n<https://slow.pics/c/UkCwdK13> - Spongebob Comparison\n<https://slow.pics/c/qHDo50X7> - Somewhat terrible pixel art demonstration\nhttps://cdn.discordapp.com/attachments/549519631761539082/877997627025813526/unknown.png\n\n**EDIT:** Folder link was updated, and i've added new BSRGAN-like models added alongside those ^",
        "author": "Kim",
        "when": "2021-08-23T21:22:53.138000+00:00",
        "name": "4x-UniScale-Balanced/Strong (With Noise Removal [NR] versions)",
        "hasLink": true
    },
    "4x-UniScaleV2-Moderate": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-UniScaleV2_Soft/Moderate/Sharp\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZcLXhVZkfMVbFDuGFBb2XwCW9aLKLR8Imk0> <https://mega.nz/folder/zZhA1KoD#ds2nmgDNV4hfFpNSfEqN6Q>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Effectively a UniScale successor. It does nearly everything better, other than dealing with noise or compression. Use UniScale_Restore or UniScale_Iterp for that.\n\n**Iterations:** 111k\n**batch_size:** 4\n**HR_size:** 112\n**Epoch:** 8\n**Dataset:** Custom Dataset consisting of lossless 4k frames from Metal Arms: Glitch in the System, Just Cause 3, Dirt 3, Forza Horizon 3, Sleeping Dogs, and self-edited photos from SignatureEdits + ATLA DVD images\n**Dataset_size:** 18,909 tiles + 288 ATLA Frames\n**OTF Training** Yes (JPEG artifacts, base_blur, bsrgan_resize)\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** I really don't have a great description for this model set. It just works well on nearly everything (other than Anime sadly). They work best with realistic images. I hope you guys like it 😛\n\nThe Moderate and Soft models are interpolations between UniScaleV2_Sharp, UniScale-Strong, and UniScaleNR-Balanced.\n\nIf your image has compression, the Soft model will work the best. Moderate and Sharp will work on images with compression, but they won't be as clean. \n\nUniScale_Restore or UniScale_Interp (available in the main UniScale folder) will likely work better for images with heavy compression.\n\nComparisons:\nhttps://cdn.discordapp.com/attachments/884239326471393331/884296266857713704/unknown.png\nhttps://cdn.discordapp.com/attachments/884239326471393331/884294468709257216/unknown.png\nhttps://cdn.discordapp.com/attachments/884239326471393331/884298572894441512/unknown.png\nhttps://cdn.discordapp.com/attachments/884239326471393331/884302370136289323/unknown.png",
        "author": "Kim",
        "when": "2021-09-06T05:06:18.293000+00:00",
        "name": "4x-UniScaleV2_Soft/Moderate/Sharp",
        "hasLink": true
    },
    "4x-UniScaleV2-Sharp": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-UniScaleV2_Soft/Moderate/Sharp\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZcLXhVZkfMVbFDuGFBb2XwCW9aLKLR8Imk0> <https://mega.nz/folder/zZhA1KoD#ds2nmgDNV4hfFpNSfEqN6Q>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Effectively a UniScale successor. It does nearly everything better, other than dealing with noise or compression. Use UniScale_Restore or UniScale_Iterp for that.\n\n**Iterations:** 111k\n**batch_size:** 4\n**HR_size:** 112\n**Epoch:** 8\n**Dataset:** Custom Dataset consisting of lossless 4k frames from Metal Arms: Glitch in the System, Just Cause 3, Dirt 3, Forza Horizon 3, Sleeping Dogs, and self-edited photos from SignatureEdits + ATLA DVD images\n**Dataset_size:** 18,909 tiles + 288 ATLA Frames\n**OTF Training** Yes (JPEG artifacts, base_blur, bsrgan_resize)\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** I really don't have a great description for this model set. It just works well on nearly everything (other than Anime sadly). They work best with realistic images. I hope you guys like it 😛\n\nThe Moderate and Soft models are interpolations between UniScaleV2_Sharp, UniScale-Strong, and UniScaleNR-Balanced.\n\nIf your image has compression, the Soft model will work the best. Moderate and Sharp will work on images with compression, but they won't be as clean. \n\nUniScale_Restore or UniScale_Interp (available in the main UniScale folder) will likely work better for images with heavy compression.\n\nComparisons:\nhttps://cdn.discordapp.com/attachments/884239326471393331/884296266857713704/unknown.png\nhttps://cdn.discordapp.com/attachments/884239326471393331/884294468709257216/unknown.png\nhttps://cdn.discordapp.com/attachments/884239326471393331/884298572894441512/unknown.png\nhttps://cdn.discordapp.com/attachments/884239326471393331/884302370136289323/unknown.png",
        "author": "Kim",
        "when": "2021-09-06T05:06:18.293000+00:00",
        "name": "4x-UniScaleV2_Soft/Moderate/Sharp",
        "hasLink": true
    },
    "4x-UniScaleV2-Soft": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x-UniScaleV2_Soft/Moderate/Sharp\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZcLXhVZkfMVbFDuGFBb2XwCW9aLKLR8Imk0> <https://mega.nz/folder/zZhA1KoD#ds2nmgDNV4hfFpNSfEqN6Q>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** Effectively a UniScale successor. It does nearly everything better, other than dealing with noise or compression. Use UniScale_Restore or UniScale_Iterp for that.\n\n**Iterations:** 111k\n**batch_size:** 4\n**HR_size:** 112\n**Epoch:** 8\n**Dataset:** Custom Dataset consisting of lossless 4k frames from Metal Arms: Glitch in the System, Just Cause 3, Dirt 3, Forza Horizon 3, Sleeping Dogs, and self-edited photos from SignatureEdits + ATLA DVD images\n**Dataset_size:** 18,909 tiles + 288 ATLA Frames\n**OTF Training** Yes (JPEG artifacts, base_blur, bsrgan_resize)\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** I really don't have a great description for this model set. It just works well on nearly everything (other than Anime sadly). They work best with realistic images. I hope you guys like it 😛\n\nThe Moderate and Soft models are interpolations between UniScaleV2_Sharp, UniScale-Strong, and UniScaleNR-Balanced.\n\nIf your image has compression, the Soft model will work the best. Moderate and Sharp will work on images with compression, but they won't be as clean. \n\nUniScale_Restore or UniScale_Interp (available in the main UniScale folder) will likely work better for images with heavy compression.\n\nComparisons:\nhttps://cdn.discordapp.com/attachments/884239326471393331/884296266857713704/unknown.png\nhttps://cdn.discordapp.com/attachments/884239326471393331/884294468709257216/unknown.png\nhttps://cdn.discordapp.com/attachments/884239326471393331/884298572894441512/unknown.png\nhttps://cdn.discordapp.com/attachments/884239326471393331/884302370136289323/unknown.png",
        "author": "Kim",
        "when": "2021-09-06T05:06:18.293000+00:00",
        "name": "4x-UniScaleV2_Soft/Moderate/Sharp",
        "hasLink": true
    },
    "4x-UniversalUpscalerV2-Neutral": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** UniversalUpscalerV2 (Neutral, Sharp, and Sharper)\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/u/1/folders/15dV3rhMcq_LXg5Vrn7DG6lwo4M6cXt6C\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** General Upscaler\n\n**Iterations:** 115k (Neutral), 101k (Sharp), 103k (Sharper)\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** 981 (Neutral), 861 (Sharp), 880 (Sharper)\n**Dataset:** Realistic images\n**Dataset_size:** 450 (+20)\n**OTF Training:** No\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 29.3, SSIM: 0.79637, LPIPS: 0.053994 (Neutral), PSNR: 29.86, SSIM: 0.80575, LPIPS: 0.053477 (Sharp), PSNR: 30.233, SSIM: 0.80412, LPIPS: 0.054815 (Sharper)\n\n**Description:** A collection of general upscalers that work best on their respective downscaled filters. For each model, I downscaled the HR images by 50% using the Box filter. Afterwards, I used the Box filter for Neutral, Hermite for Sharp, and Gaussian for Sharper to downscale the images again by 25%. To top it off, 20 more images were downscaled using the Point filter for all three models.\n\nhttps://imgsli.com/NDc3ODA/2/1",
        "author": "Sisyphean",
        "when": "2021-04-01T16:03:58.127000+00:00",
        "name": "UniversalUpscalerV2 (Neutral, Sharp, and Sharper)",
        "hasLink": true
    },
    "4x-UniversalUpscalerV2-Sharp": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** UniversalUpscalerV2 (Neutral, Sharp, and Sharper)\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/u/1/folders/15dV3rhMcq_LXg5Vrn7DG6lwo4M6cXt6C\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** General Upscaler\n\n**Iterations:** 115k (Neutral), 101k (Sharp), 103k (Sharper)\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** 981 (Neutral), 861 (Sharp), 880 (Sharper)\n**Dataset:** Realistic images\n**Dataset_size:** 450 (+20)\n**OTF Training:** No\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 29.3, SSIM: 0.79637, LPIPS: 0.053994 (Neutral), PSNR: 29.86, SSIM: 0.80575, LPIPS: 0.053477 (Sharp), PSNR: 30.233, SSIM: 0.80412, LPIPS: 0.054815 (Sharper)\n\n**Description:** A collection of general upscalers that work best on their respective downscaled filters. For each model, I downscaled the HR images by 50% using the Box filter. Afterwards, I used the Box filter for Neutral, Hermite for Sharp, and Gaussian for Sharper to downscale the images again by 25%. To top it off, 20 more images were downscaled using the Point filter for all three models.\n\nhttps://imgsli.com/NDc3ODA/2/1",
        "author": "Sisyphean",
        "when": "2021-04-01T16:03:58.127000+00:00",
        "name": "UniversalUpscalerV2 (Neutral, Sharp, and Sharper)",
        "hasLink": true
    },
    "4x-UniversalUpscalerV2-Sharper": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** UniversalUpscalerV2 (Neutral, Sharp, and Sharper)\n**License:** WTFPL\n**Link:** https://drive.google.com/drive/u/1/folders/15dV3rhMcq_LXg5Vrn7DG6lwo4M6cXt6C\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** General Upscaler\n\n**Iterations:** 115k (Neutral), 101k (Sharp), 103k (Sharper)\n**batch_size:** 4\n**HR_size:** 128\n**Epoch:** 981 (Neutral), 861 (Sharp), 880 (Sharper)\n**Dataset:** Realistic images\n**Dataset_size:** 450 (+20)\n**OTF Training:** No\n**Pretrained_Model_G:** 1xESRGAN\n**Validation Stats:** PSNR: 29.3, SSIM: 0.79637, LPIPS: 0.053994 (Neutral), PSNR: 29.86, SSIM: 0.80575, LPIPS: 0.053477 (Sharp), PSNR: 30.233, SSIM: 0.80412, LPIPS: 0.054815 (Sharper)\n\n**Description:** A collection of general upscalers that work best on their respective downscaled filters. For each model, I downscaled the HR images by 50% using the Box filter. Afterwards, I used the Box filter for Neutral, Hermite for Sharp, and Gaussian for Sharper to downscale the images again by 25%. To top it off, 20 more images were downscaled using the Point filter for all three models.\n\nhttps://imgsli.com/NDc3ODA/2/1",
        "author": "Sisyphean",
        "when": "2021-04-01T16:03:58.127000+00:00",
        "name": "UniversalUpscalerV2 (Neutral, Sharp, and Sharper)",
        "hasLink": true
    },
    "4x-Valar": {
        "content": "**Name:** 4x_Valar_v1\n**License:** CC0\n**Link:** https://mega.nz/file/STI3kIKJ#mr3PqAu35SI9yGHWUdCJsXm2kai1R1SrsYsBafyH-Vo\n**SHA256:** dd4e77b9493afda13d70303f0759776a49b85675c5dd6de2e19f5089dea866ac\n**Model Architecture:** ESRGAN-Plus\n**Scale:** 4\n**Purpose:** General realistic super-resolution.\n**Iterations:** 400k\n**Dataset:** Hand selected 1471 Raw images from the Adobe-MIT 5k dataset, processed using Rawtherapee.\n**Pretrained_Model_G:** 4x_RRDB_ESRGAN\n\n**Description:** Meant as an experiment to test latest techniques implemented on traiNNer, including: AdaTarget, KernelGAN, UNet discriminator, nESRGAN+ arch, noise patches, camera noise, isotropic/anisotropic/sinc blur, frequency separation, contextual loss, mixup, clipL1 pixel loss, AdamP optimizer, etc. The config file is provided on the download link above.\nI encourage everybody to mirror the model, distribute and modify it in anyway you want.",
        "author": "musl",
        "when": "2021-09-17T02:52:37.181000+00:00",
        "name": "4x_Valar_v1",
        "hasLink": true
    },
    "4x-VimeoScale": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 4x_VimeoScale\n**License:** CC-BY-SA\n**Link:** <https://mega.nz/folder/b8MTgYhA#CkKajnw0fPu5BVynl-caOQ>\n**Model Architecture:** SOFVSR-RRDB, 5 Frames\n**Scale:** 4\n**Purpose:** IRL video\n**Iterations:** >100k Pretrain + 200k Unet Training\n**batch_size:** 13\n**HR_size:** 128\n**Epoch:** N/A\n**Dataset:** 200gb+ Combination of Collected Vimeo scenes (CG, realistic/video, movie trailers, and motion graphics) at original quality and Vimeo90k\n**Dataset_size:** 800k+ images, 3,000+ scenes\n**OTF Training** Yes\n**Pretrained_Model_G:** Self-Trained Base for the Unet finalisation with vgg_fea discrim\n**Description:** This model is one of the longest I have trained and tuned, includes some noise training but this is not intended for deblocking/decompression. Combined with https://github.com/JoeyBallentine/Video-Inference 's fp16 mode, this should handle most SD and 720p content at or faster than ESRGAN, with a significant bump in quality.\nExamples: https://imgsli.com/NzUwODU \nhttps://imgsli.com/NzUwODQ\nhttps://imgsli.com/NzUwODc\nhttps://imgsli.com/NzUwODY",
        "author": "Sazoji",
        "when": "2021-10-08T04:03:26.016000+00:00",
        "name": "4x_VimeoScale",
        "hasLink": true
    },
    "4x-VolArt": {
        "content": "**Name:** 4x-VolArt\n**License:** CC BY-NC-SA 4.0\n**Link:** <https://u.pcloud.link/publink/show?code=kZg4XhVZAQShVXJatvXlu2FGNV4GQj8Lr6k7> <https://mega.nz/folder/HNh0hLzD#vUuFhp2ZAeNi_5shZfCC0g>\n**Model Architecture:** ESRGAN\n**Scale:** 4\n**Purpose:** This model upscales Art for the game Volfoss.\n\n**Iterations:** 1.6k/8k (interpolated 50/50)\n**batch_size:** 12\n**HR_size:** 64\n**Epoch:** 370\n**Dataset:** Artwork by Yasushi Nirasawa, provided by <@295426806188736523> \n**Dataset_size:** 526 tiles\n**OTF Training** Yes\n**Pretrained_Model_G:** 4xESRGAN\n\n**Description:** This model upscales artwork for the game Volfoss (2001). The model peaked in quality very quickly.\n\nThe NR model removes most noise, but has the downside of removing transparent portions. Use the main model\n\nExamples:\nhttps://cdn.discordapp.com/attachments/888869052867543060/888869070928248852/unknown.png\n<https://cdn.discordapp.com/attachments/888869052867543060/888869784169619496/unknown.png>",
        "author": "Kim",
        "when": "2021-09-18T19:32:28.856000+00:00",
        "name": "4x-VolArt",
        "hasLink": true
    },
    "4x-WaifuGAN-v3": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n\nScale: 4\nPurpose: Upscaling CG-painted anime with variable outlines.\nDescription: Third attempt at training from a mostly anime dataset sourced from image boards.  Only PNGs used, pics mainly brush strokes and gradients. Texturised images avoided as much as possible. If too generative, tone down by interpolating with a softer model.\nLink: https://1drv.ms/u/s!Aip-EMByJHY20wpGoLuSRzjdqh0T\nIterations: 30000\nbatch_size: 2\nHR_size: 128\nDataset_size: 73\nPretrained_Model_G: Manga109v2.pth",
        "author": "DinJerr",
        "when": "2019-07-12T09:46:41.281000+00:00",
        "name": null,
        "hasLink": true
    },
    "4x-xbrz": {
        "content": "<@&560103931204861954> added xbrz style pixel art upscaler to wiki under Specialized\nhttps://drive.google.com/open?id=1LFd4BqZ2N8p21JzjLX6hGdm7RpVIkH7s direct link",
        "author": "LyonHrt",
        "when": "2019-06-05T22:10:38.839000+00:00",
        "name": null,
        "hasLink": true
    },
    "4x-xbrz-dd": {
        "content": "<@&560103931204861954> xbrz plus dedithering style pixel-art upscaling model to wiki under specialised \nDirect link:\nhttps://drive.google.com/open?id=1h5YufgcS1jbCdqwsw7dye0DvLj3kIfox",
        "author": "LyonHrt",
        "when": "2019-06-17T17:14:02.334000+00:00",
        "name": null,
        "hasLink": true
    },
    "8x-BoyMeBob-Redux": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 8x_BoyMeBob-Redux\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZy6TKXZruJNkiysJ3R2fqxMra9RQQquMczV\n**Model Architecture:** ESRGAN+\n**Scale:** 8\n**Purpose:** Upscaling cartoons\n\n**Iterations:** 200,000\n**batch_size:** I changed it during training but it was set to 4 for most of it\n**HR_size:** 256\n**Epoch:** 33\n**Dataset:** SpongeBob Season 11\n**Dataset_size:** 24,123\n**OTF Training** Yes. Various downscaling types (including heavily anti-aliased ones) and otf quantization.\n**Pretrained_Model_G:** 8xBoyMeBob (unreleased) which used 8xESRGAN\n\n**Description:** Wanted to try this out using the new OTF downscaling types and it works really well. Should theoretically work on most downscaling types. Overall a much better model than my old unreleased 8xBoyMeBob, which is why I'm actually releasing it. I'm interested to see results using it on something other than spongebob, I haven't tested it with anything else, so please try it on some stuff for me 🙂 Oh and ⚠️ **since it's an ESRGAN+ model you have to use my or efonte's fork or iNNfer in order to be able to use it (aka won't work with Cupscale or IEU)** ⚠️",
        "author": "Joey",
        "when": "2021-08-19T16:28:41.525000+00:00",
        "name": "8x_BoyMeBob-Redux",
        "hasLink": true
    },
    "8x-MS-Unpainter": {
        "content": "<@&560103931204861954> \n**Name:** MS Unpainter + MS Unpainter De-Dither\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZPTiBXZPnCIVsFUmrpu4NY1sKfKw8Pyw6rk\n**Model Architecture:** ESRGAN\n**Scale:** 8\n**Purpose:** Low-resolution MS Paint drawings, general pixel art, and general dithered pixel art, all kinds of pixel art\n\n**Iterations:** 195K~\n**Pretrained_Model_G:** A mix of: 8x_glasshopper_ArzenalV1.1, 8x_glasshopper_MS-Unpainter, 8x_NMKD-Sphax + Sphax de-dither, and 8x_NMKD-YanderePixelArt4 + Yandere De-Dither, all interpolated with adjustments using the interpolate function in Cupscale\n**Side Note:** The other stats aren't here, as this model was made from the interpolation function, so the extra info is unknown, and would likely be inaccurate with a guess. Though, I can estimate the rounded up model iteration count is around 195K. The interpolation function was ran multiple times making small changes, so it wouldn't be accurate anyway.\n\n**Description:** MS Unpainter and MS Unpainter De-Dither are models that take Arzenal's qualities and issues, and plays around with it to make a version that works for all types of pixel art. I'd consider this model to work on all kinds of pixel art. The de-dithered version is designed to remove dithering, and smooth out sharp color changing in pixel art. Both of these models try to work best for most pixel art, and I think it does this well.\n*Also, you don't need to add this to the wiki as I already did (idk might be illegal). So I might be getting banned idk? I'll just be hoping I don't*\n\n**Fun Facts:** This pixel art model was designed to fit all kinds of pixel art, to be the most general pixel art model yet. This model and Arzenal are the only 8x pixel art models in the wiki, there are NMKD's pixel art models, but they aren't on the wiki yet. This big model for all kinds of pixel art branched from a small little model called Sphaxdere.",
        "author": "ComputerK",
        "when": "2021-03-24T19:33:56.331000+00:00",
        "name": "MS Unpainter + MS Unpainter De-Dither",
        "hasLink": true
    },
    "8x-MS-Unpainter-De-Dither": {
        "content": "<@&560103931204861954> \n**Name:** MS Unpainter + MS Unpainter De-Dither\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZPTiBXZPnCIVsFUmrpu4NY1sKfKw8Pyw6rk\n**Model Architecture:** ESRGAN\n**Scale:** 8\n**Purpose:** Low-resolution MS Paint drawings, general pixel art, and general dithered pixel art, all kinds of pixel art\n\n**Iterations:** 195K~\n**Pretrained_Model_G:** A mix of: 8x_glasshopper_ArzenalV1.1, 8x_glasshopper_MS-Unpainter, 8x_NMKD-Sphax + Sphax de-dither, and 8x_NMKD-YanderePixelArt4 + Yandere De-Dither, all interpolated with adjustments using the interpolate function in Cupscale\n**Side Note:** The other stats aren't here, as this model was made from the interpolation function, so the extra info is unknown, and would likely be inaccurate with a guess. Though, I can estimate the rounded up model iteration count is around 195K. The interpolation function was ran multiple times making small changes, so it wouldn't be accurate anyway.\n\n**Description:** MS Unpainter and MS Unpainter De-Dither are models that take Arzenal's qualities and issues, and plays around with it to make a version that works for all types of pixel art. I'd consider this model to work on all kinds of pixel art. The de-dithered version is designed to remove dithering, and smooth out sharp color changing in pixel art. Both of these models try to work best for most pixel art, and I think it does this well.\n*Also, you don't need to add this to the wiki as I already did (idk might be illegal). So I might be getting banned idk? I'll just be hoping I don't*\n\n**Fun Facts:** This pixel art model was designed to fit all kinds of pixel art, to be the most general pixel art model yet. This model and Arzenal are the only 8x pixel art models in the wiki, there are NMKD's pixel art models, but they aren't on the wiki yet. This big model for all kinds of pixel art branched from a small little model called Sphaxdere.",
        "author": "ComputerK",
        "when": "2021-03-24T19:33:56.331000+00:00",
        "name": "MS Unpainter + MS Unpainter De-Dither",
        "hasLink": true
    },
    "8x-NMKD-Typescale": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** NMKD Typescale\n**License:** WTFPL\n**Link:** https://u.pcloud.link/publink/show?code=kZHbUHXZPIrt5BhJ0r0emd4U0oD7UV1hj6d7\n**Model Architecture:** ESRGAN\n**Scale:** 8\n**Purpose:** Low-resolution text/typography and symbols\n\n**Iterations:** 175k\n**batch_size:** 3\n**HR_size:** 448 (= 56px LR)\n**Dataset:** Custom - Typography images from Flickr\n**Dataset_size:** 1960 (980 unique)\n**OTF Training** Only rotation/flipping\n**Pretrained_Model_G:** 8xESRGAN\n\n**Description:** A strong 8x upscaler made primarily for text. Trained on both clean images and JPEGs. Also work pretty well for symbols and other illustrations that are mostly made out of lines.",
        "author": "nmkd",
        "when": "2020-11-04T12:56:10.165000+00:00",
        "name": "NMKD Typescale",
        "hasLink": false
    },
    "8x-Sphax-Alpha-NN": {
        "content": "<@&560103931204861954> <@&577839492199874570>\n**Name:** 8x_Sphax-Alpha-NN\n**License:** CC BY-NC-SA 4.0\n**Link:** https://u.pcloud.link/publink/show?code=kZeNFhXZbuw2jGJOGqQnKecb8Moo1ha47W57\n**Model Architecture:** ESRGAN (4 in/out nc)\n**Scale:** 8\n**Purpose:** Pixel art with transparency\n\n**Iterations:** 140k\n**batch_size:** 12\n**HR_size:** 256\n**Epoch:** 854\n**Dataset:** Sphax PureBDcraft Minecraft texture pack (256x) for HR, downscaled with nearest neighbor for LR.\n**Dataset_size:** 2,415\n**OTF Training** Yes (for nearest-neighbor downscaling)\n**Pretrained_Model_G:** 4xFireAlpha\n\n**Description:** I tried to imitate NMKD's sphax model, but with 4 channels (images with alpha). It works pretty similarly, though not as well, since I couldn't use CX loss only. It still works pretty well to capture the sphax aesthetic and upscale your images with it.",
        "author": "Joey",
        "when": "2021-02-05T04:07:59.202000+00:00",
        "name": "8x_Sphax-Alpha-NN",
        "hasLink": true
    },
    "8x-TGHQFace8x": {
        "content": "<@&560103931204861954> <@&577839492199874570> \n**Author:** tg#1425\n**Name:** TGHQFace8x\n**License:** GNU GPL3\n**Link:** https://drive.google.com/open?id=1OyOJIW224hBhb-aTCbuUQb0qzKmE4oH6\n**Model Architecture:** ESRGAN\n**Scale:** 8\n**Purpose:** Face Upscaling\n\n**Iterations:** 500k\n**batch_size:** 8\n**HR_size:** 128\n**Epoch:** 54\n**Dataset:** Flickr Cropped Faces\n**Dataset_size:** 70k full frames\n**OTF Training:** No\n**Pretrained_Model_G:** 8xESRGAN\n\n**Description:** Upscales blurry 128px faces, usefull for enhancing that someone in a small picture.\n**Results:** https://imgsli.com/OTM1NQ (LR vs 500k)",
        "author": "tg",
        "when": "2019-11-27T18:01:47.465000+00:00",
        "name": "TGHQFace8x",
        "hasLink": true
    }
}
