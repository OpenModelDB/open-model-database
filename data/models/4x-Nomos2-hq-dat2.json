{
    "name": "4xNomos2_hq_dat2",
    "author": "helaman",
    "license": "CC-BY-4.0",
    "tags": [
        "general-upscaler",
        "photo"
    ],
    "description": "[Link to Github Release](https://github.com/Phhofm/models/releases/tag/4xNomos2_hq_dat2)\n\n# 4xNomos2_hq_dat2   \nScale: 4   \nArchitecture: [DAT](https://github.com/zhengchen1999/dat)   \nArchitecture Option: [dat2](https://github.com/muslll/neosr/blob/5fba7f162d36052010169e6517dec3b406c569ab/neosr/archs/dat_arch.py#L1111)   \n\nAuthor: Philip Hofmann   \nLicense: CC-BY-0.4   \nPurpose: Upscaler   \nSubject: Photography   \nInput Type: Images   \nRelease Date: 29.08.2024   \n\nDataset: [nomosv2](https://github.com/muslll/neosr/?tab=readme-ov-file#-datasets)   \nDataset Size: 6000   \nOTF (on the fly augmentations): No   \nPretrained Model: DAT_2_x4   \nIterations: 140'000   \nBatch Size: 4   \nPatch Size: 48   \n\nDescription:    \nA dat2 4x upscaling model, similiar to the [4xNomos2_hq_mosr](https://github.com/Phhofm/models/releases/tag/4xNomos2_hq_mosr) model, trained and for usage on non-degraded input to give good quality output.    \n\nI scored 7 validation outputs of each of the 21 checkpoints (10k-210k) of this model training with 68 metrics.    \n[The metric scores can be found in this google sheet](https://docs.google.com/spreadsheets/d/1NL-by7WvZyDMHj5XN8UeDALVSSwH70IKvwV65ATWqrA/edit?usp=sharing).   \nThe corresponding image files for this scoring can be [found here](https://drive.google.com/file/d/1ZTp9fBMeawftNqzg4RN9_zIvHtul5jVc/view?usp=sharing)     \nScreenshot of the google sheet:     \n![|100](https://i.slow.pics/VZJTsrUv.webp)\n\nRelease checkpoint has been selected by looking at the scores, manually inspecting, and then getting responses on discord to this quick visual test, A B or C, which denote different checkpoints: https://slow.pics/c/8Akzj6rR   \n\nCheckpoint B is the one released here, but you can also try out [Checkpoint A](https://github.com/Phhofm/models/releases/download/4xNomos2_hq_dat2/4xNomos2_hq_dat2_150000.pth) or [Checkpoint C](https://github.com/Phhofm/models/releases/download/4xNomos2_hq_dat2/4xNomos2_hq_dat2_10000.pth) if you like them better.\n\n## Model Showcase:\n[Slowpics](https://slow.pics/c/yuue9WpF)",
    "date": "2024-08-29",
    "architecture": "dat",
    "size": null,
    "scale": 4,
    "inputChannels": 3,
    "outputChannels": 3,
    "resources": [
        {
            "platform": "pytorch",
            "type": "pth",
            "size": 140196334,
            "sha256": "1f2be2b4786b5031776aed90e1818564f5576ab1e44da9355c7f3788ea25bdea",
            "urls": [
                "https://github.com/Phhofm/models/releases/download/4xNomos2_hq_dat2/4xNomos2_hq_dat2.pth"
            ]
        },
        {
            "platform": "onnx",
            "type": "onnx",
            "size": 52855546,
            "sha256": "f5b0b05f767ebc4b85431df6743f52995824eb3f965a8b9b97d94e5b896df5ca",
            "urls": [
                "https://github.com/Phhofm/models/releases/download/4xNomos2_hq_dat2/4xNomos2_hq_dat2_fp32.onnx"
            ]
        }
    ],
    "trainingIterations": 140000,
    "trainingBatchSize": 4,
    "trainingOTF": false,
    "dataset": "nomosv2",
    "datasetSize": 6000,
    "pretrainedModelG": "4x-DAT-2",
    "images": [
        {
            "type": "paired",
            "LR": "https://i.slow.pics/oW1f4p1l.webp",
            "SR": "https://i.slow.pics/CRfQ7dU2.webp",
            "thumbnail": "/thumbs/small/4fc250add54644dd5bdb22eb.jpg"
        },
        {
            "type": "paired",
            "LR": "https://i.slow.pics/6Qi5mWHb.webp",
            "SR": "https://i.slow.pics/GQOCnBa6.webp",
            "thumbnail": "/thumbs/small/b2b9d97717f13ac05375fea9.jpg"
        },
        {
            "type": "paired",
            "LR": "https://i.slow.pics/zIGNMeLm.webp",
            "SR": "https://i.slow.pics/qmW0vhkn.webp",
            "thumbnail": "/thumbs/small/6df19e725acc8871eb61177d.jpg"
        },
        {
            "type": "paired",
            "LR": "https://i.slow.pics/a3CuMs15.webp",
            "SR": "https://i.slow.pics/A1PdbkMy.webp",
            "thumbnail": "/thumbs/small/ea6358150b3192c16e2e8a5b.jpg"
        },
        {
            "type": "paired",
            "LR": "https://i.slow.pics/8FgJcKRP.webp",
            "SR": "https://i.slow.pics/4tW09CWE.webp",
            "thumbnail": "/thumbs/small/063b7f98d5f4d1f6e67c982a.jpg"
        },
        {
            "type": "paired",
            "LR": "https://i.slow.pics/SXH01SS2.webp",
            "SR": "https://i.slow.pics/FVjyAFHf.webp",
            "thumbnail": "/thumbs/small/6ac6ae146d55ce14a6c0c0ce.jpg"
        },
        {
            "type": "paired",
            "LR": "https://i.slow.pics/YwG0JUQY.webp",
            "SR": "https://i.slow.pics/ZmyGy2An.webp",
            "thumbnail": "/thumbs/small/608b2e9be3c9085369bfa34f.jpg"
        }
    ],
    "thumbnail": {
        "type": "paired",
        "LR": "/thumbs/a5830a98c896629990c0f8a6.jpg",
        "SR": "/thumbs/772e41c09e5e357a8fa5d9ce.jpg",
        "LRSize": {
            "width": 366,
            "height": 296
        },
        "SRSize": {
            "width": 366,
            "height": 296
        }
    }
}