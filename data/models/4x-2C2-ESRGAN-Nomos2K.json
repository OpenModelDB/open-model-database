{
    "name": "2C2-ESRGAN Nomos2K",
    "author": "joey",
    "license": "MIT",
    "tags": [
        "pretrained"
    ],
    "description": "Technically my previous experiment was the pretrained model, but for all intents and purposes this was trained from scratch. Description: Pretrained model for the new architecture modification I made. You can read more about it in the [GitHub README](https://github.com/joeyballentine/2C2-ESRGAN). Basically it makes smaller ESRGAN models that theoretically can produce the same level of quality.\n\n**NOTE:** THIS WILL NOT WORK IN CUPSCALE or IEU! You have to use chaiNNer or my fork to use it for now.",
    "date": "2022-04-20",
    "architecture": "2c2-esrgan",
    "size": [
        "64nf",
        "23nb"
    ],
    "scale": 4,
    "inputChannels": 3,
    "outputChannels": 3,
    "resources": [
        {
            "platform": "pytorch",
            "type": "pth",
            "size": 39444095,
            "sha256": "7d728ac6e7a673be71b0222a11a0bf159d773a2f8e1330e22b302dead24d4f70",
            "urls": [
                "https://objectstorage.us-phoenix-1.oraclecloud.com/n/ax6ygfvpvzka/b/open-modeldb-files/o/models%2F4x-2C2-ESRGAN-Nomos2K.pth",
                "https://github.com/joeyballentine/2C2-ESRGAN/tree/main/models"
            ]
        }
    ],
    "trainingIterations": 200000,
    "trainingEpochs": 16,
    "trainingBatchSize": 6,
    "trainingHRSize": 128,
    "dataset": "Nomos2K",
    "pretrainedModelG": {
        "description": "Technically my previous experiment was the pretrained model, but for all intents and purposes this was trained from scratch."
    }
}