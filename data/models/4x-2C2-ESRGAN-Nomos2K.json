{
    "name": "2C2-ESRGAN Nomos2K",
    "author": "joey",
    "license": "MIT",
    "tags": [
        "image",
        "general-upscaler"
    ],
    "description": "Category: Universal Models\nPurpose: General Upscaler\n\nTechnically my previous experiment was the pretrained model, but for all intents and purposes this was trained from scratch. Description: Pretrained model for the new architecture modification I made. You can read more about it in the github README. Basically it makes smaller ESRGAN models that theoretically can produce the same level of quality. NOTE: THIS WILL NOT WORK IN CUPSCALE, IEU, OR CHAINNER (yet)! You have to use my fork to use it for now.",
    "date": "2022-04-20",
    "architecture": "2C2-ESRGAN",
    "size": null,
    "scale": 4,
    "inputChannels": null,
    "outputChannels": null,
    "resources": [
        {
            "type": "pth",
            "size": 39444095,
            "sha256": "7d728ac6e7a673be71b0222a11a0bf159d773a2f8e1330e22b302dead24d4f70",
            "urls": [
                "https://objectstorage.us-phoenix-1.oraclecloud.com/n/ax6ygfvpvzka/b/open-modeldb-files/o/models%2F4x-2C2-ESRGAN-Nomos2K.pth",
                "https://github.com/joeyballentine/2C2-ESRGAN/tree/main/models"
            ]
        }
    ],
    "trainingIterations": 200000,
    "trainingEpochs": 16,
    "trainingBatchSize": 6,
    "trainingHRSize": 128,
    "dataset": "Nomos2K",
    "pretrainedModelG": {
        "description": "Technically my previous experiment was the pretrained model, but for all intents and purposes this was trained from scratch."
    }
}