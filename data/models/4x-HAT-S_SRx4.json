{
    "name": "HAT-S_SRx4",
    "author": "xpixelgroup",
    "license": null,
    "tags": [],
    "description": "Official paper pretrain model\n\nPWC PWC PWC PWC\nHAT [Paper Link] Replicate\nActivating More Pixels in Image Super-Resolution Transformer\n\nXiangyu Chen, Xintao Wang, Jiantao Zhou and Chao Dong\nBibTeX\n\n@InProceedings{chen2023hat,\n    author    = {Chen, Xiangyu and Wang, Xintao and Zhou, Jiantao and Qiao, Yu and Dong, Chao},\n    title     = {Activating More Pixels in Image Super-Resolution Transformer},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month     = {June},\n    year      = {2023},\n    pages     = {22367-22377}\n}\n\nGoogle Scholar has unknown bugs for indexing this paper recently, while it can still be cited by the above BibTeX.\nUpdates\n\n    ✅ 2022-05-09: Release the first version of the paper at Arxiv.\n    ✅ 2022-05-20: Release the codes, models and results of HAT.\n    ✅ 2022-08-29: Add a Replicate demo for SRx4.\n    ✅ 2022-09-25: Add the tile mode for inference with limited GPU memory.\n    ✅ 2022-11-24: Upload a GAN-based HAT model for Real-World SR (Real_HAT_GAN_SRx4.pth).\n    ✅ 2023-03-19: Update paper to CVPR version. Small HAT models are added.\n    ✅ 2023-04-05: Upload the HAT-S codes, models and results.\n    (To do) Add the tile mode for Replicate demo.\n    (To do) Update the Replicate demo for Real-World SR.\n    (To do) Upload the training configs for the Real-World GAN-based HAT model.\n    (To do) Add HAT models for Multiple Image Restoration tasks.\n\nOverview\n\nBenchmark results on SRx4 without x2 pretraining. Mulit-Adds are calculated for a 64x64 input.\nModel \tParams(M) \tMulti-Adds(G) \tSet5 \tSet14 \tBSD100 \tUrban100 \tManga109\nSwinIR \t11.9 \t53.6 \t32.92 \t29.09 \t27.92 \t27.45 \t32.03\nHAT-S \t9.6 \t54.9 \t32.92 \t29.15 \t27.97 \t27.87 \t32.35\nHAT \t20.8 \t102.4 \t33.04 \t29.23 \t28.00 \t27.97 \t32.48\nEnvironment\n\n    PyTorch >= 1.7 (Recommend NOT using torch 1.8!!! It would cause abnormal performance.)\n    BasicSR == 1.3.4.9\n\nInstallation\n\npip install -r requirements.txt\npython setup.py develop\n\nHow To Test\n\nWithout implementing the codes, chaiNNer is a nice tool to run our models.\n\nOtherwise,\n\n    Refer to ./options/test for the configuration file of the model to be tested, and prepare the testing data and pretrained model.\n    The pretrained models are available at Google Drive or Baidu Netdisk (access code: qyrl).\n    Then run the follwing codes (taking HAT_SRx4_ImageNet-pretrain.pth as an example):\n\npython hat/test.py -opt options/test/HAT_SRx4_ImageNet-pretrain.yml\n\nThe testing results will be saved in the ./results folder.\n\n    Refer to ./options/test/HAT_SRx4_ImageNet-LR.yml for inference without the ground truth image.\n\nNote that the tile mode is also provided for limited GPU memory when testing. You can modify the specific settings of the tile mode in your custom testing option by referring to ./options/test/HAT_tile_example.yml.\nHow To Train\n\n    Refer to ./options/train for the configuration file of the model to train.\n    Preparation of training data can refer to this page. ImageNet dataset can be downloaded at the official website.\n    The training command is like\n\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=8 --master_port=4321 hat/train.py -opt options/train/train_HAT_SRx2_from_scratch.yml --launcher pytorch\n\n    Note that the default batch size per gpu is 4, which will cost about 20G memory for each GPU.\n\nThe training logs and weights will be saved in the ./experiments folder.\nResults\n\nThe inference results on benchmark datasets are available at Google Drive or Baidu Netdisk (access code: 63p5).\nContact\n\nIf you have any question, please email chxy95@gmail.com or join in the Wechat group of BasicSR to discuss with the authors.",
    "date": "2023-04-05",
    "architecture": "HAT",
    "size": [
        "HAT-S"
    ],
    "scale": 4,
    "inputChannels": 3,
    "outputChannels": 3,
    "resources": [
        {
            "platform": "pytorch",
            "type": "pth",
            "size": 81089561,
            "sha256": "a92f81bd2c0c1aaa371a6e4d6cac69e749fde2e36196885ee47a4a3667542c9a",
            "urls": [
                "https://drive.google.com/file/d/1YvU9PF1XqlP8TVzH7P0bg-YlfP1TKDPC/view?usp=drive_link"
            ]
        }
    ],
    "images": [
        {
            "type": "standalone",
            "url": "https://raw.githubusercontent.com/chxy95/HAT/master/figures/Performance_comparison.png"
        }
    ]
}